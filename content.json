{"meta":{"title":"聘宝研发","subtitle":"一群有追求的人","description":"做有趣的事","author":"聘宝研发","url":"http://blog.pinbot.me"},"pages":[],"posts":[{"title":"用Sphinx快速制作文档","slug":"用Sphinx快速制作文档","date":"2017-06-25T08:16:17.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2017/06/25/用Sphinx快速制作文档/","link":"","permalink":"http://blog.pinbot.me/2017/06/25/用Sphinx快速制作文档/","excerpt":"简介Sphinx 是一种文档工具，它可以令人轻松的撰写出清晰且优美的文档, 由 Georg Brandl 在BSD 许可证下开发. 新版的Python文档就是由Sphinx生成的， 并且它已成为Python项目首选的文档工具,同时它对 C/C++ 项目也有很好的支持; 并计划对其它开发语言添加特殊支持. 本站当然也是使用 Sphinx 生成的，它采用reStructuredText! Sphinx还在继续开发. 下面列出了其良好特性,这些特性在Python官方文档中均有体现: 丰富的输出格式: 支持 HTML (包括 Windows 帮助文档), LaTeX (可以打印PDF版本), manual pages（man 文档）, 纯文本 完备的交叉引用: 语义化的标签,并可以自动化链接函数,类,引文,术语及相似的片段信息 明晰的分层结构: 可以轻松的定义文档树,并自动化链接同级/父级/下级文章 美观的自动索引: 可自动生成美观的模块索引 精确的语法高亮: 基于 Pygments 自动生成语法高亮 开放的扩展: 支持代码块的自动测试,并包含Python模块的自述文档(API docs)等 Sphinx 使用 reStructuredText 作为标记语言, 可以享有 Docutils 为reStructuredText提供的分析，转换等多种工具.","text":"简介Sphinx 是一种文档工具，它可以令人轻松的撰写出清晰且优美的文档, 由 Georg Brandl 在BSD 许可证下开发. 新版的Python文档就是由Sphinx生成的， 并且它已成为Python项目首选的文档工具,同时它对 C/C++ 项目也有很好的支持; 并计划对其它开发语言添加特殊支持. 本站当然也是使用 Sphinx 生成的，它采用reStructuredText! Sphinx还在继续开发. 下面列出了其良好特性,这些特性在Python官方文档中均有体现: 丰富的输出格式: 支持 HTML (包括 Windows 帮助文档), LaTeX (可以打印PDF版本), manual pages（man 文档）, 纯文本 完备的交叉引用: 语义化的标签,并可以自动化链接函数,类,引文,术语及相似的片段信息 明晰的分层结构: 可以轻松的定义文档树,并自动化链接同级/父级/下级文章 美观的自动索引: 可自动生成美观的模块索引 精确的语法高亮: 基于 Pygments 自动生成语法高亮 开放的扩展: 支持代码块的自动测试,并包含Python模块的自述文档(API docs)等 Sphinx 使用 reStructuredText 作为标记语言, 可以享有 Docutils 为reStructuredText提供的分析，转换等多种工具. 安装SphinxSphinx为Python语言的一个第三方库。我们需要在终端中输入下列命令进行安装：1pip install sphinx 创建Sphinx项目创建一个用于存放文档的文件夹，然后在该文件夹路径下运行下列命令快速生成Sphinx项目：1sphinx-quickstart 接下来会让你选择一些配置： 设置文档的根路径（回车，使用默认设置） 12Enter the root path for documentation.&gt; Root path for the documentation [.]: 是否分离source和build目录（输入y,选择分离，方便管理） 1234You have two options for placing the build directory for Sphinx output.Either, you use a directory &quot;_build&quot; within the root path, or you separate&quot;source&quot; and &quot;build&quot; directories within the root path.&gt; Separate source and build directories (y/n) [n]: 设定模板前缀（回车，使用默认选项） 1234Inside the root directory, two more directories will be created; &quot;_templates&quot;for custom HTML templates and &quot;_static&quot; for custom stylesheets and other staticfiles. You can enter another prefix (such as &quot;.&quot;) to replace the underscore.&gt; Name prefix for templates and static dir [_]: 输入项目名称和作者 123The project name will occur in several places in the built documentation.&gt; Project name: Sphinx-test&gt; Author name(s): test 输入项目版本号 1234567Sphinx has the notion of a &quot;version&quot; and a &quot;release&quot; for thesoftware. Each version can have multiple releases. For example, forPython the version is something like 2.5 or 3.0, while the release issomething like 2.5.1 or 3.0a1. If you don&apos;t need this dual structure,just set both to the same value.&gt; Project version []: 1.0.0&gt; Project release [1.0.0]: 文档语言（回车，默认即可） 1234567If the documents are to be written in a language other than English,you can select a language here by its language code. Sphinx will thentranslate text that it generates into that language.For a list of supported codes, seehttp://sphinx-doc.org/config.html#confval-language.&gt; Project language [en]: 设定文档文就按的后缀 123The file name suffix for source files. Commonly, this is either &quot;.txt&quot;or &quot;.rst&quot;. Only files with this suffix are considered documents.&gt; Source file suffix [.rst]: 设定首页名称（回车，选择默认index即可） 12345One document is special in that it is considered the top node of the&quot;contents tree&quot;, that is, it is the root of the hierarchical structureof the documents. Normally, this is &quot;index&quot;, but if your &quot;index&quot;document is a custom template, you can also set this to another filename.&gt; Name of your master document (without suffix) [index]: 根据需要选择是否开启epub输出(一般用不到，回车默认不开启即可) 12Sphinx can also add configuration for epub output:&gt; Do you want to use the epub builder (y/n) [n]: 根据需求选择是否开启相应的Sphinx拓展功能 12345678910111213Please indicate if you want to use one of the following Sphinx extensions:&gt; autodoc: automatically insert docstrings from modules (y/n) [n]: y&gt; doctest: automatically test code snippets in doctest blocks (y/n) [n]: y&gt; intersphinx: link between Sphinx documentation of different projects (y/n) [n]: y&gt; todo: write &quot;todo&quot; entries that can be shown or hidden on build (y/n) [n]: y&gt; coverage: checks for documentation coverage (y/n) [n]: y&gt; imgmath: include math, rendered as PNG or SVG images (y/n) [n]: y&gt; mathjax: include math, rendered in the browser by MathJax (y/n) [n]: yNote: imgmath and mathjax cannot be enabled at the same time.imgmath has been deselected.&gt; ifconfig: conditional inclusion of content based on config values (y/n) [n]: y&gt; viewcode: include links to the source code of documented Python objects (y/n) [n]: y&gt; githubpages: create .nojekyll file to publish the document on GitHub pages (y/n) [n]: n 创建项目 1234567891011121314151617A Makefile and a Windows command file can be generated for you so that youonly have to run e.g. `make html&apos; instead of invoking sphinx-builddirectly.&gt; Create Makefile? (y/n) [y]: y&gt; Create Windows command file? (y/n) [y]: yCreating file ./conf.py.Creating file ./index.rst,.md.Creating file ./Makefile.Creating file ./make.bat.Finished: An initial directory structure has been created.You should now populate your master file ./index.rst,.md and create other documentationsource files. Use the Makefile to build the docs, like so: make builderwhere &quot;builder&quot; is one of the supported builders, e.g. html, latex or linkcheck. 项目创建以后目录结构如下所示:123456789.├── Makefile├── build├── make.bat└── source ├── _static ├── _templates ├── conf.py └── index.rst build:用来存放通过make html生成文档网页文件的目录 source：存放用于生成文档的源文件 conf.py: Sphinx的配置文件 index.rst: 主文档定义文档结构主文档index.rst的主要功能是被转换成欢迎页, 它包含一个目录表（ “table of contents tree”或者 toctree ). Sphinx 主要功能是使用 reStructuredText, 把许多文件组织成一份结构合理的文档. toctree指令初始值如下：12.. toctree:: :maxdepth: 2 你可以在 content 的位置添加文档列表:12345.. toctree:: :maxdepth: 2 tutorial.md ... 注：文档文件放在与index.rst同级目录下。 支持markdown文件、更改文档主题Spinx本身不支持.md文件生成文档，需要我们使用第三方库recommonmark进行转换。首先分别运行下列命令安装recommonmark与sphinx_rtd_theme库。123pip install recommonmarkpip install sphinx_rtd_theme 安装好，在conf.py中修改下列两个配置：12source_suffix = [&apos;.rst&apos;, &apos;.md&apos;, &apos;.MD&apos;]html_theme = &apos;sphinx_rtd_theme&apos; 并新增：1234source_parsers = &#123; &apos;.md&apos;: CommonMarkParser, &apos;.MD&apos;: CommonMarkParser,&#125; 生成文档在Sphinx项目所在的文件夹路径下运行下列命令生成文档：1make html 生成后的文档位于build/html文件夹内，用浏览器打开index.html即可看到生成后的文档。 参考文章 Sphinx 使用手册 使用 sphinx 制作简洁而又美观的文档 使用Sphinx制作说明文档","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"http://blog.pinbot.me/tags/python/"},{"name":"sphinx","slug":"sphinx","permalink":"http://blog.pinbot.me/tags/sphinx/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"Python打包发布一条龙","slug":"Python打包发布一条龙","date":"2016-11-29T14:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/11/29/Python打包发布一条龙/","link":"","permalink":"http://blog.pinbot.me/2016/11/29/Python打包发布一条龙/","excerpt":"今天介绍一下 Python 项目的打包及分发。","text":"今天介绍一下 Python 项目的打包及分发。 相关工具distutilsdistutils 是于2000年发布的 Python 项目创建打包工具。 但现在很少直接使用, 而是使用 setuptools 。 setuptoolssetuptools 于2004年发布, 是 distutils 的增强版本, 包含 setup 和 find_packages 等常见函数。 另外它包含了 easy_install 这个包安装工具, 使用Eggs包来进行安装(扩展名为 .egg ), 如果你不幸用 Windows 做过 Python 开发, 应该对它不陌生。 另外现在在 macOS 的 Python 环境中也附带了这个工具。 常见用法如下: 1$ easy_install redisdict-0.0.1-py2.7.egg Eggs文件实际是一个压缩包, 解压后目录结构大致如下, 具体可以参看 The Internal Structure of Python Eggs¶ 123456789├── EGG-INFO│ ├── PKG-INFO│ ├── SOURCES.txt│ ├── dependency_links.txt│ ├── requires.txt│ ├── top_level.txt│ └── zip-safe├── redisdict│ ├── __init__.py 说到 Eggs 就得谈谈 Wheel, Wheel 是2012年发布的Python包格式, 类似于 Eggs , 其目录结构大致如下: 123456789├── redisdict│ └── __init__.py└── redisdict-0.0.1.dist-info ├── DESCRIPTION.rst ├── METADATA ├── RECORD ├── WHEEL ├── metadata.json └── top_level.txt pippip 是 PyPA(Python Packaging Authority) 推荐、时下最主流的包安装工具, 发布于2013年, pip 支持从常见的VCS系统(例如git)进行安装, 也可以安装Eggs包, Wheel包。 使用 setuptools 打包我们以 RedisDict 这个模块为例, 首先在项目中新建 setup.py 文件, 并引入 setuptools 。之后使用 setuptools.setup 方法对项目进行打包, 方法的参数设定可以参看 setup-args 。 示例代码如下: 1234567891011121314151617181920import setuptoolsimport codecsdef get_requirements(filename): return codecs.open(&apos;requirements/&apos; + filename, encoding=&apos;utf-8&apos;).read().splitlines()setuptools.setup( name=&apos;redisdict&apos;, version=&apos;0.0.1&apos;, packages=setuptools.find_packages(exclude=[&apos;tests&apos;, &apos;tests.*&apos;]), url=&apos;https://github.com/kxrr/redisdict&apos;, license=&apos;MIT&apos;, author=&apos;kxrr&apos;, author_email=&apos;hi@kxrr.us&apos;, description=&apos;A dict-like object using Redis as the backend.&apos;, install_requires=get_requirements(&apos;default.txt&apos;), tests_require=get_requirements(&apos;test.txt&apos;),) 之后, 我们可以使用下面命令将项目打包为压缩文件: 1234$ python setup.py check # 检查$ python setup.py sdist # 打包为 .tar.gz $ python setup.py bdist_egg # 创建 Eggs包$ python setup.py bdist_wheel # 创建 Wheel包 生成的文件均位于 dist 目录下: 1234dist├── redisdict-0.0.1-py2-none-any.whl├── redisdict-0.0.1-py2.7.egg└── redisdict-0.0.1.tar.gz 别外也可以通过 setuptools 将项目打包成其它格式(比如exe), 有兴趣的可以自己看看。 分发发布到自建 PyPI Server只需要通过 scp 命令将打好的包传到我们的 PyPI Server 上去: 1$ scp dist/redisdict-0.0.1-py2.py3-none-any.whl deploy@pypi.pinbot.me:/home/deploy/packages 上传完成后可以使用 pip 进行安装: 1$ pip install redisdict -i http://pypi.pinbot.me/simple/ --trusted-host pypi.pinbot.me 发布到官方 PyPI Server首先到 PyPI 注册一个帐号, 在邮箱内确认。 之后在家目录新建一个 .pypirc 文件, 写入下面内容(注意填入自己的帐号密码): 12345[pypirc]servers = pypi[server-login]username:usernamepassword:password 接下来就可以开始上传了: 12$ python setup.py register # 将包注册到 PyPI$ python setup.py register sdist upload # 上传 运行过后根据提示操作即可发布完成。 Packaging User Guide: https://packaging.python.org/key_projects/Celery setup.py: https://github.com/celery/celery/blob/master/setup.pyWhat was the problem with packaging?: http://pythonhosted.org/distlib/overview.html#what-was-the-problem-with-packagingUploading your project to pypi: https://packaging.python.org/distributing/#uploading-your-project-to-pypi","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"http://blog.pinbot.me/tags/python/"},{"name":"setup.py","slug":"setup-py","permalink":"http://blog.pinbot.me/tags/setup-py/"},{"name":"pypi","slug":"pypi","permalink":"http://blog.pinbot.me/tags/pypi/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"Django入门进阶","slug":"Django入门进阶","date":"2016-11-17T16:58:51.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/11/18/Django入门进阶/","link":"","permalink":"http://blog.pinbot.me/2016/11/18/Django入门进阶/","excerpt":"作者：Adam at 2016-11-18 00:58:51 我们前面讲了快速熟悉Django架构网站的方式，接下来我们来深入了解一下Django框架。它不仅提供了建站的方法，也提供了一整套的从开发简单的网站到一个完整维护迭代的开发模式，下面几个方面非常重要。","text":"作者：Adam at 2016-11-18 00:58:51 我们前面讲了快速熟悉Django架构网站的方式，接下来我们来深入了解一下Django框架。它不仅提供了建站的方法，也提供了一整套的从开发简单的网站到一个完整维护迭代的开发模式，下面几个方面非常重要。 学会调试 单步调试直接在代码中插入下面这一行： 1234#polls/views.pyfrom django.shortcuts import get_object_or_404, renderfrom .models import Questionimport pdb; pdb.set_trace() #这一行 运行程序，就可以在服务器端命令行中一步步的跟踪执行结果。 1234567891011121314151617181920212223242526272829[26/Nov/2016 14:17:49] \"GET /polls/ HTTP/1.1\" 200 72Performing system checks...&gt; /myFirstDjango/mysite/polls/views.py(6)&lt;module&gt;()-&gt; def index(request):(Pdb) l ＃这是调试命令：展示当前运行代码 1 from django.shortcuts import get_object_or_404, render 2 3 from .models import Question 4 import pdb; pdb.set_trace() 5 # Create your views here. 6 -&gt; def index(request): 7 latest_question_list = Question.objects.order_by('-pub_date')[:5] 8 context = &#123; 9 'latest_question_list': latest_question_list, 10 &#125; 11 return render(request, 'polls/index.html', context)(Pdb) n ＃这是调试命令: 下一步&gt; /Users/gzadamlv/myFirstDjango/mysite/polls/views.py(13)&lt;module&gt;()-&gt; def detail(request, question_id):(Pdb) h ＃这是调试命令：获取帮助，查看所有可用命令Documented commands (type help &lt;topic&gt;):========================================EOF bt cont enable jump pp run unta c continue exit l q s untilalias cl d h list quit step upargs clear debug help n r tbreak wb commands disable ignore next restart u whatisbreak condition down j p return unalias where... Shell 调试方式用下面的命令进入命令行 123456(myFirstDjango)➜ mysite git:(master) ✗ python manage.py shellPython 2.7.10 (default, Oct 23 2015, 19:19:21)[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwinType \"help\", \"copyright\", \"credits\" or \"license\" for more information.(InteractiveConsole)&gt;&gt;&gt; 通过在命令行引入程序中使用的模块、函数来跟踪返回结果。1234567&gt;&gt;&gt; django.setup()&gt;&gt;&gt; import mysite&gt;&gt;&gt; from django.shortcuts import get_object_or_404, render&gt;&gt;&gt; from polls.models import Question&gt;&gt;&gt; latest_question_list = Question.objects.order_by('-pub_date')[:5]&gt;&gt;&gt; print(latest_question_list)&lt;QuerySet [&lt;Question: 你会学习python吗？&gt;]&gt; 学会测试修改models.py12from django.utils import timezoneimport datetime 给class Question增加一个方法12def was_published_recently(self): return self.pub_date &gt;= timezone.now() - datetime.timedelta(days=1) 添加测试文件：polls/tests.py1234567891011121314import datetimefrom django.utils import timezonefrom django.test import TestCasefrom .models import Questionclass QuestionMethodTests(TestCase): def test_was_published_recently_with_future_question(self): \"\"\" was_published_recently() should return False for questions whose pub_date is in the future. \"\"\" time = timezone.now() + datetime.timedelta(days=30) future_question = Question(pub_date=time) self.assertIs(future_question.was_published_recently(), False) 运行测试12345678910111213141516(myFirstDjango)➜ mysite git:(master) ✗ python manage.py test pollsCreating test database for alias 'default'...F======================================================================FAIL: test_was_published_recently_with_future_question (polls.tests.QuestionMethodTests)----------------------------------------------------------------------Traceback (most recent call last): File \"/Users/gzadamlv/myFirstDjango/mysite/polls/tests.py\", line 14, in test_was_published_recently_with_future_question self.assertIs(future_question.was_published_recently(), False)AssertionError: True is not False----------------------------------------------------------------------Ran 1 test in 0.002sFAILED (failures=1)Destroying test database for alias 'default'... 修改was_published_recently方法123#return self.pub_date &gt;= timezone.now() - datetime.timedelta(days=1)now = timezone.now()return now - datetime.timedelta(days=1) &lt;= self.pub_date &lt;= now 再运行测试12345678(myFirstDjango)➜ mysite git:(master) ✗ python manage.py test pollsCreating test database for alias 'default'....----------------------------------------------------------------------Ran 1 test in 0.001sOKDestroying test database for alias 'default'... 学会写文档 安装： pip install Sphinx运行 sphinx-quickstart 命令(下面几项要选yes) 123456Please indicate if you want to use one of the following Sphinx extensions:&gt; autodoc: automatically insert docstrings from modules (y/n) [n]: y&gt; doctest: automatically test code snippets in doctest blocks (y/n) [n]: y&gt; intersphinx: link between Sphinx documentation of different projects (y/n) [n]: y&gt; todo: write \"todo\" entries that can be shown or hidden on build (y/n) [n]: y&gt; coverage: checks for documentation coverage (y/n) [n]: y 生成文档： make html打开网页：_build/html/index.html 默认模版不太好看，我们新加一个主题：pip install sphinx_rtd_theme在conf.py最后增加：12import sphinx_rtd_themehtml_theme = 'sphinx_rtd_theme' 再make html，初始化文档生成成功： 但是我们会发现并没有看到程序中的文档，别着急，还差两步。 先用sphinx-apidoc把app目录中python程序的文档内容提取出来，保存到api这个目录。1sphinx-apidoc -o api ./app -f 最后，我们需要编辑index.rst这个文件：123456Contents:.. toctree:: :maxdepth: 2 api/polls.rst #增加这行 再次make html，大功告成～ 这是提取出来的具体文档： 另外：如果make html出现下面错误1sphinx autodoc ImportError: No module named 可以试试这个：1export PYTHONPATH=$(pwd) 别小看上面这三步，其实还有很多细节没能和大家演示，师傅领进门，修行靠个人哦～","categories":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://blog.pinbot.me/tags/Django/"},{"name":"Python","slug":"Python","permalink":"http://blog.pinbot.me/tags/Python/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}]},{"title":"使用R并行方式对数值型数据离散化","slug":"使用R并行方式对数值型数据离散化","date":"2016-11-09T14:48:37.000Z","updated":"2017-06-30T02:42:01.000Z","comments":true,"path":"2016/11/09/使用R并行方式对数值型数据离散化/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/使用R并行方式对数值型数据离散化/","excerpt":"数据的特征按照其取值可以分为连续型和离散型。离散数值属性在数据挖掘的过程中具有重要的作用。比如在信用卡评分模型中，当自变量很多时，并非所有字段对于目标字段来说都是有效的，因此通常的做法是通过计算woe值和iv值(类似于信息增益)来初步挑选通过对目标变量重要的字段，然后建模逻辑回归模型。而这当中就需要对数值型数据离散化。数值型数据离散化通常分为有监督离散化和无监督离散化。考虑到数据建模通常是建立目标字段和其影响因素之间的关系的量化，因此会选择有监督离散化。R语言中用于数值型数据离散化的包discretization。安装和加载如下：12&gt; install.packages(\"discretization\")&gt; library(discretization)","text":"数据的特征按照其取值可以分为连续型和离散型。离散数值属性在数据挖掘的过程中具有重要的作用。比如在信用卡评分模型中，当自变量很多时，并非所有字段对于目标字段来说都是有效的，因此通常的做法是通过计算woe值和iv值(类似于信息增益)来初步挑选通过对目标变量重要的字段，然后建模逻辑回归模型。而这当中就需要对数值型数据离散化。数值型数据离散化通常分为有监督离散化和无监督离散化。考虑到数据建模通常是建立目标字段和其影响因素之间的关系的量化，因此会选择有监督离散化。R语言中用于数值型数据离散化的包discretization。安装和加载如下：12&gt; install.packages(\"discretization\")&gt; library(discretization) 以R自带数据集iris为例，以”Species” 为目标字段，对”Sepal.Length”、”Sepal.Width”、”Petal.Length”、”Petal.Width”四个数值型属性离散化。 12345&gt; lisan_result &lt;- mdlp(iris)&gt; class(lisan_result)[1] \"list\"&gt; names(lisan_result)[1] \"cutp\" \"Disc.data\" 使用mdlp()方法对iris离散化，该方法默认数据框最后一列最后为目标字段。返回结果为列表。”cutp”为各列的分割点向量。”Disc.data”为离散化后的数据框。该方法对于较小的样本量和维度时，程序运行时间还可以接受。但随着数据量的增大，数据维度的增加，程序运行时间会越来越长。因此考虑采用并行的方式对数据进行离散化。介绍R用于离散化的包parallel。12345&gt; install.packages(\"parallel\")&gt; library(parallel)&gt; cores &lt;- detectCores() ##查看本机虚拟核心数cores[1] 4 现在考虑以并行的方式实现离散化方法。考虑设计思路如下：1.将字段10个为一组分别与目标字段组合成数据框，(不足10个时以实际数量字段与目标字段组合)存放在一个列表中。列表的元素即离散字段与目标字段构成的数据框。2.启动M个附属进程，并初始化。M&lt;=本机虚拟核心数。使用parLapply()作用于步骤1中建立的列表数据。此时既有M个附属进程对数据进行离散化。3.将步骤2中的离散化结果合并。4.将上述步骤封装成函数。整理后使得返回结果与mdlp()函数一致。这样方便调用。将上述设计思路写成R代码，如下：输入离散数据、使用核心数，返回结果与使用mdlp()函数相同12345678910111213141516171819202122232425262728293031323334353637383940414243444546parallel_lisan &lt;- function(lisan_data,cores_num)&#123; library(parallel) library(discretization) res &lt;- list() lisan_data_v &lt;- list() cut_point &lt;- list() Disc.data &lt;- data.frame(c(rep(NA,nrow(lisan_data)))) name_num = ncol(lisan_data)-1 ##将原始数据分割成多列，先考虑每组10列。不足的单独分为一组。 group_num = floor(name_num/10) last_group_num = name_num%%10 if( name_num &gt; 10)&#123; ##当原始数据列数多余10列 for(i in 1:group_num)&#123; lw_flag &lt;- lisan_data[,ncol(lisan_data)] lisan_data_v[[i]] &lt;- cbind(lisan_data[,(10*i-9):(10*i)],lw_flag) &#125; lisan_data_v[[group_num+1]] &lt;- lisan_data[,(10*group_num+1):ncol(lisan_data)] &#125;else&#123; lisan_data_v[[group_num+1]] &lt;- lisan_data[,(10*group_num+1):ncol(lisan_data)] &#125; cl &lt;- makeCluster(cores_num) ##初始化核心 results &lt;- parLapply(cl,lisan_data_v,mdlp) ##对列表数据使用mdlp函数并行离散化 for(i in 1:length(results))&#123; for(j in 1:length(results[[i]][[1]]))&#123; cut_point[[(i-1)*10+j]] &lt;- results[[i]][[1]][[j]] &#125; temp &lt;- as.data.frame(results[[i]][[2]]) Disc.data &lt;- cbind(Disc.data,temp[,1:(ncol(temp)-1)]) ##合并离散数据结果 &#125; Disc.data &lt;- Disc.data[,2:ncol(Disc.data)] Disc.data$lw_flag &lt;- lisan_data[,ncol(lisan_data)] names(Disc.data) &lt;- names(lisan_data) stopCluster(cl) res[[\"cutp\"]] &lt;- cut_point res[[\"Disc.data\"]] &lt;- Disc.data return(res) &#125;","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/tags/机器学习/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/categories/机器学习/"}]},{"title":"向量的相似性度量","slug":"向量的相似性度量","date":"2016-11-09T14:48:37.000Z","updated":"2017-06-30T02:41:18.000Z","comments":true,"path":"2016/11/09/向量的相似性度量/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/向量的相似性度量/","excerpt":"author: 贺晓松 在机器学习或是数据挖掘的算法当中，经常需要比较不同样本特征向量的相似性，进而作为下一步的判断依据。比方说最近数据挖掘领域比较火的精准营销、定点广告投放、用户画像等。都需要计算不同数据样例特征向量的相似性，然后根据相似性或分类或排序等。因此，向量的相似性度量可以说是数据挖掘或机器学习领域的一个基础性工具。","text":"author: 贺晓松 在机器学习或是数据挖掘的算法当中，经常需要比较不同样本特征向量的相似性，进而作为下一步的判断依据。比方说最近数据挖掘领域比较火的精准营销、定点广告投放、用户画像等。都需要计算不同数据样例特征向量的相似性，然后根据相似性或分类或排序等。因此，向量的相似性度量可以说是数据挖掘或机器学习领域的一个基础性工具。需要用到相似度度量的方法类型分为：样本点的相似性度量、类与类之间的相似性度量、变量之间的相似性度量。用数量化的方法描述事物之间的相似程度，一个事物通常需要多个维度来刻画。假设一群样本点需要用p个维度去描述，则每个样本点可以看成是p维空间中的一个点。因此，很自然地想到可以用距离来度量样本点之间的相似程度。假定p维空间中的两个点，，则x,y间的距离度量最常用的是闵氏距离： 当q=1,2，或q趋于无穷时可分别得到： （1）曼哈顿距离(绝对值距离)： 从名字就可以猜出这种距离的计算方法了。想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源， 曼哈顿距离也称为城市街区距离(City Block distance)。 （2）欧氏距离： 在闵氏距离中，最常用的是欧式距离。它的主要优点是当坐标轴进行正交旋转时，欧式距离是保持不变的。另外一定要采用相同量纲的变量，然后再计算距离。同时尽可能避免维度间的想关心，相关性会造成信息重叠，会片面强调某些变量的重要性。 （3）切比雪夫距离： 国际象棋玩过么？国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？自己走走试试。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。 （4）夹角余弦距离几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：(2) 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。即：夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。 (5) 马氏距离有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：而其中向量Xi与Xj之间的马氏距离定义为：若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/tags/机器学习/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/categories/机器学习/"}]},{"title":"web客户端存储方式","slug":"web存储方式","date":"2016-11-09T10:30:30.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/11/09/web存储方式/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/web存储方式/","excerpt":"存储方式近来，在使用angular.js建立web应用时，使用到$cacheFactory缓存服务对应用中的一些数据进行缓存，以提高网站性能，减少数据请求量，突然想对web客户端存储方式做一个总结。存储方式： cookie localStorage sessionStorage 我们来一一说明。","text":"存储方式近来，在使用angular.js建立web应用时，使用到$cacheFactory缓存服务对应用中的一些数据进行缓存，以提高网站性能，减少数据请求量，突然想对web客户端存储方式做一个总结。存储方式： cookie localStorage sessionStorage 我们来一一说明。假设有这样一种情况，在某个用例流程中，由A页面跳至B页面，若在A页面中采用JS用变量temp保存了某一变量的值，在B页面的时候，同样需要使用JS来引用temp的变量值，对于JS中的全局变量或者静态变量的生命周期是有限的，当发生页面跳转或者页面关闭的时候，这些变量的值会重新载入，即没有达到保存的效果。这个问题，就可以使用cookie或者localStorage来保存该变量的值。 cookiecookie的使用在前端开发中是很普遍的，基本人尽皆知，简单说明一下，不做太多赘述。cookie是存于用户硬盘的一个文件，这个文件通常对应于一个域名，当浏览器再次访问这个域名时，便使这个cookie可用。因此，cookie可以跨越一个域名下的多个网页。另外，稍微了解一下cookie的结构，简单地说：cookie是以键值对的形式保存的，即key=value的格式。各个cookie之间一般是以“;”分隔。cookie的写入，读取，删除可参考：http://www.jb51.net/article/14566.htm。 localStoragelocalStorage是html5提供的新的存储数据的方法。在localStorage之前，客户端的数据是由cookie保存的。但是cookie不适合大量数据的存储，因为它们由每个对服务器的请求来传递，这使得 cookie 速度很慢而且效率也不高。而localStorage，数据不是由每个服务器请求传递的，而是只有在请求时使用数据。它使在不影响网站性能的情况下存储大量数据成为可能。localStorage有以下几个特点：1.localStorage是一个普通对象，任何对象的操作都适用。2.localStorage对象的属性值只能是字符串。3.localStorage支持的默认空间大小为5M,现代浏览器支持良好。除了IE７及以下不支持外，其他标准浏览器都完全支持。4.localStorage本身带有方法有： 添加键值对：localStorage.setItem(key,value) 获取键值：localStorage.getItem(key) 删除键值对：localStorage.removeItem(key)。 清除所有键值对：localStorage.clear()。 获取localStorage的属性名称（键名称）：localStorage.key(index)。5、没有时间限制的数据存储。不主动清除，localStorage一直存在。 另外，假设有这样一种情况，在某个用例流程中，我们希望数据在页面强制刷新的时候或者关闭的时候，数据自动清空，不污染下次进入页面的数据。在页面服务不中断的时候保存数据。这个时候就可以用sessionStorage。 sessionStoragesessionStorage和localStorage拥有完全一样的特点，唯一的区别是，sessionStorage在页面强制刷新的时候或者关闭的时候，数据自动清空。而localStorage需要主动清除。 既然是$cacheFactory缓存服务引发的对存储方式的思考。当然需要来介绍一下$cacheFactory缓存服务： AngularJs $cacheFactory 缓存服务完全适用于sessionStorage的应用场景。可用于缓存angular模版和其他数据。$cacheFactory存储的数据也可以在不同controller中传递。$cacheFoctory用于生成一个用来存储缓存对象的服务，并且提供对对象的访问。$cacheFactory.Cache一个用于存储和检索数据的缓存对象。主要使用$http和脚本指令来缓存模板和其他数据。该服务有以下方法：put(key,value);在缓存对象中插入一个键值对(key,value)。 get(key);在缓存对象中通过指定key获取对应的值。 romove(key);在缓存对象中通过指定key删除对应的值。 removeAll();删除缓存对象中所有的键值对。 destroy();销毁这个缓存对象。 info();获取缓存对象信息（id，size）。","categories":[],"tags":[],"keywords":[]},{"title":"影响Lucene对文档打分的四种方式","slug":"影响Lucene对文档打分的四种方式","date":"2016-11-09T10:00:04.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/11/09/影响Lucene对文档打分的四种方式/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/影响Lucene对文档打分的四种方式/","excerpt":"影响Lucene对文档打分的四种方式","text":"影响Lucene对文档打分的四种方式 在索引阶段设置Document Boost和Field Boost，存储在(.nrm)文件中如果希望某些文档和某些域比其他的域更重要，如果此文档和此域包含所要查询的词则应该得分较高，则可以在索引阶段设定文档的boost和域的boost值。这些值是在索引阶段就写入索引文件的，存储在标准化因子(.nrm)文件中，一旦设定，除非删除此文档，否则无法改变。如果不进行设定，则Document Boost和Field Boost默认为1。Document Boost及FieldBoost的设定方式如下：12345Document doc = new Document();Field f = new Field(\"contents\", \"hello world\", Field.Store.NO, Field.Index.ANALYZED);f.setBoost(100);doc.add(f);doc.setBoost(100); 两者是如何影响Lucene的文档打分的呢？让我们首先来看一下Lucene的文档打分的公式：score(q,d) = coord(q,d) · queryNorm(q) · ∑( tf(t in d) · idf(t)2 · t.getBoost() · norm(t,d) )t in q Document Boost和Field Boost影响的是norm(t, d)，其公式如下:norm(t,d) = doc.getBoost() · lengthNorm(field) · ∏f.getBoost()field f in d named as t 它包括三个参数： Document boost：此值越大，说明此文档越重要。 Field boost：此域越大，说明此域越重要。 lengthNorm(field) = (1.0 / Math.sqrt(numTerms))：一个域中包含的Term总数越多，也即文档越长，此值越小，文档越短，此值越大。其中第三个参数可以在自己的Similarity中影响打分，下面会论述。 当然，也可以在添加Field的时候，设置Field.Index.ANALYZED_NO_NORMS或Field.Index.NOT_ANALYZED_NO_NORMS，完全不用norm，来节约空间。 根据Lucene的注释，No norms means that index-time field and document boosting and field length normalization are disabled. The benefit is less memory usage as norms take up one byte of RAM per indexed field for every document in the index, during searching. Note that once you index a given field with norms enabled, disabling norms will have no effect. 没有norms意味着索引阶段禁用了文档boost和域的boost及长度标准化。好处在于节省内存，不用在搜索阶段为索引中的每篇文档的每个域都占用一个字节来保存norms信息了。但是对norms信息的禁用是必须全部域都禁用的，一旦有一个域不禁用，则其他禁用的域也会存放默认的norms值。因为为了加快norms的搜索速度，Lucene是根据文档号乘以每篇文档的norms信息所占用的大小来计算偏移量的，中间少一篇文档，偏移量将无法计算。也即norms信息要么都保存，要么都不保存。 下面几个试验可以验证norms信息的作用： 试验一：Document Boost的作用 1234567891011121314151617181920212223242526public void testNormsDocBoost() throws Exception &#123; File indexDir = new File(\"testNormsDocBoost\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); writer.setUseCompoundFile(false); Document doc1 = new Document(); Field f1 = new Field(\"contents\", \"common hello hello\", Field.Store.NO, Field.Index.ANALYZED); doc1.add(f1); doc1.setBoost(100); writer.addDocument(doc1); Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common common hello\", Field.Store.NO, Field.Index.ANALYZED_NO_NORMS); doc2.add(f2); writer.addDocument(doc2); Document doc3 = new Document(); Field f3 = new Field(\"contents\", \"common common common\", Field.Store.NO, Field.Index.ANALYZED_NO_NORMS); doc3.add(f3); writer.addDocument(doc3); writer.close(); IndexReader reader = IndexReader.open(FSDirectory.open(indexDir)); IndexSearcher searcher = new IndexSearcher(reader); TopDocs docs = searcher.search(new TermQuery(new Term(\"contents\", \"common\")), 10); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; &#125; 如果第一篇文档的域f1也为Field.Index.ANALYZED_NO_NORMS的时候，搜索排名如下：123docid : 2 score : 1.2337708 docid : 1 score : 1.0073696 docid : 0 score : 0.71231794 如果第一篇文档的域f1设为Field.Index.ANALYZED，则搜索排名如下：123docid : 0 score : 39.889805 docid : 2 score : 0.6168854 docid : 1 score : 0.5036848 试验二：Field Boost的作用如果我们觉得title要比contents要重要，可以做一下设定。123456789101112131415161718192021222324public void testNormsFieldBoost() throws Exception &#123; File indexDir = new File(\"testNormsFieldBoost\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); writer.setUseCompoundFile(false); Document doc1 = new Document(); Field f1 = new Field(\"title\", \"common hello hello\", Field.Store.NO, Field.Index.ANALYZED); f1.setBoost(100); doc1.add(f1); writer.addDocument(doc1); Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common common hello\", Field.Store.NO, Field.Index.ANALYZED_NO_NORMS); doc2.add(f2); writer.addDocument(doc2); writer.close(); IndexReader reader = IndexReader.open(FSDirectory.open(indexDir)); IndexSearcher searcher = new IndexSearcher(reader); QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"contents\", new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(\"title:common contents:common\"); TopDocs docs = searcher.search(query, 10); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; &#125; 如果第一篇文档的域f1也为Field.Index.ANALYZED_NO_NORMS的时候，搜索排名如下：12docid : 1 score : 0.49999997 docid : 0 score : 0.35355338 如果第一篇文档的域f1设为Field.Index.ANALYZED，则搜索排名如下：12docid : 0 score : 19.79899 docid : 1 score : 0.49999997 试验三：norms中文档长度对打分的影响1234567891011121314151617181920212223public void testNormsLength() throws Exception &#123; File indexDir = new File(\"testNormsLength\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); writer.setUseCompoundFile(false); Document doc1 = new Document(); Field f1 = new Field(\"contents\", \"common hello hello\", Field.Store.NO, Field.Index.ANALYZED_NO_NORMS); doc1.add(f1); writer.addDocument(doc1); Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common common hello hello hello hello\", Field.Store.NO, Field.Index.ANALYZED_NO_NORMS); doc2.add(f2); writer.addDocument(doc2); writer.close(); IndexReader reader = IndexReader.open(FSDirectory.open(indexDir)); IndexSearcher searcher = new IndexSearcher(reader); QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"contents\", new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(\"title:common contents:common\"); TopDocs docs = searcher.search(query, 10); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; &#125; 当norms被禁用的时候，包含两个common的第二篇文档打分较高：12docid : 1 score : 0.13928263 docid : 0 score : 0.09848769 当norms起作用的时候，虽然包含两个common的第二篇文档，由于长度较长，因而打分较低：12docid : 0 score : 0.09848769 docid : 1 score : 0.052230984 试验四：norms信息要么都保存，要么都不保存的特性12345678910111213141516public void testOmitNorms() throws Exception &#123; File indexDir = new File(\"testOmitNorms\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); writer.setUseCompoundFile(false); Document doc1 = new Document(); Field f1 = new Field(\"title\", \"common hello hello\", Field.Store.NO, Field.Index.ANALYZED); doc1.add(f1); writer.addDocument(doc1); for (int i = 0; i &lt; 10000; i++) &#123; Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common common hello hello hello hello\", Field.Store.NO, Field.Index.ANALYZED_NO_NORMS); doc2.add(f2); writer.addDocument(doc2); &#125; writer.close(); &#125; 当我们添加10001篇文档，所有的文档都设为Field.Index.ANALYZED_NO_NORMS的时候，我们看索引文件，发现.nrm文件只有1K，也即其中除了保持一定的格式信息，并无其他数据。当我们把第一篇文档设为Field.Index.ANALYZED，而其他10000篇文档都设为Field.Index.ANALYZED_NO_NORMS的时候，发现.nrm文件又10K，也即所有的文档都存储了norms信息，而非只有第一篇文档。 在搜索语句中，设置Query Boost.在搜索中，我们可以指定，某些词对我们来说更重要，我们可以设置这个词的boost：1common^4 hello 使得包含common的文档比包含hello的文档获得更高的分数。由于在Lucene中，一个Term定义为Field:Term，则也可以影响不同域的打分：1title:common^4 content:common 使得title中包含common的文档比content中包含common的文档获得更高的分数。实例：1234567891011121314151617181920212223public void testQueryBoost() throws Exception &#123; File indexDir = new File(\"TestQueryBoost\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); Document doc1 = new Document(); Field f1 = new Field(\"contents\", \"common1 hello hello\", Field.Store.NO, Field.Index.ANALYZED); doc1.add(f1); writer.addDocument(doc1); Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common2 common2 hello\", Field.Store.NO, Field.Index.ANALYZED); doc2.add(f2); writer.addDocument(doc2); writer.close(); IndexReader reader = IndexReader.open(FSDirectory.open(indexDir)); IndexSearcher searcher = new IndexSearcher(reader); QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"contents\", new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(\"common1 common2\"); TopDocs docs = searcher.search(query, 10); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; &#125; 根据tf/idf，包含两个common2的第二篇文档打分较高：12docid : 1 score : 0.24999999 docid : 0 score : 0.17677669 如果我们输入的查询语句为：”common1^100 common2”，则第一篇文档打分较高：12docid : 0 score : 0.2499875 docid : 1 score : 0.0035353568 那Query Boost是如何影响文档打分的呢？根据Lucene的打分计算公式：score(q,d) = coord(q,d) · queryNorm(q) · ∑( tf(t in d) · idf(t)2 · t.getBoost() · norm(t,d) )t in q注：在queryNorm的部分，也有q.getBoost()的部分，但是对query向量的归一化(见向量空间模型与Lucene的打分机制[http://forfuture1978.javaeye.com/blog/588721])。 继承并实现自己的SimilaritySimilariy是计算Lucene打分的最主要的类，实现其中的很多借口可以干预打分的过程。(1) float computeNorm(String field, FieldInvertState state)(2) float lengthNorm(String fieldName, int numTokens)(3) float queryNorm(float sumOfSquaredWeights)(4) float tf(float freq)(5) float idf(int docFreq, int numDocs)(6) float coord(int overlap, int maxOverlap)(7) float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length) 它们分别影响Lucene打分计算的如下部分：score(q,d) = (6)coord(q,d) · (3)queryNorm(q) · ∑( (4)tf(t in d) · (5)idf(t)2 · t.getBoost() · (1)norm(t,d) )t in qnorm(t,d) = doc.getBoost() · (2)lengthNorm(field) · ∏f.getBoost()field f in d named as t 下面逐个进行解释：(1) float computeNorm(String field, FieldInvertState state)影响标准化因子的计算，如上述，他主要包含了三部分：文档boost，域boost，以及文档长度归一化。此函数一般按照上面norm(t, d)的公式进行计算。(2) float lengthNorm(String fieldName, int numTokens)主要计算文档长度的归一化，默认是1.0 / Math.sqrt(numTerms)。因为在索引中，不同的文档长度不一样，很显然，对于任意一个term，在长的文档中的tf要大的多，因而分数也越高，这样对小的文档不公平，举一个极端的例子，在一篇1000万个词的鸿篇巨著中，”lucene”这个词出现了11次，而在一篇12个词的短小文档中，”lucene”这个词出现了10次，如果不考虑长度在内，当然鸿篇巨著应该分数更高，然而显然这篇小文档才是真正关注”lucene”的。因而在此处是要除以文档的长度，从而减少因文档长度带来的打分不公。然而现在这个公式是偏向于首先返回短小的文档的，这样在实际应用中使得搜索结果也很难看。于是在实践中，要根据项目的需要，根据搜索的领域，改写lengthNorm的计算公式。比如我想做一个经济学论文的搜索系统，经过一定时间的调研，发现大多数的经济学论文的长度在8000到10000词，因而lengthNorm的公式应该是一个倒抛物线型的，8000到10000词的论文分数最高，更短或更长的分数都应该偏低，方能够返回给用户最好的数据。(3) float queryNorm(float sumOfSquaredWeights)这是按照向量空间模型，对query向量的归一化。此值并不影响排序，而仅仅使得不同的query之间的分数可以比较。(4) float tf(float freq)freq是指在一篇文档中包含的某个词的数目。tf是根据此数目给出的分数，默认为Math.sqrt(freq)。也即此项并不是随着包含的数目的增多而线性增加的。(5) float idf(int docFreq, int numDocs)idf是根据包含某个词的文档数以及总文档数计算出的分数，默认为(Math.log(numDocs/(double)(docFreq+1)) + 1.0)。由于此项计算涉及到总文档数和包含此词的文档数，因而需要全局的文档数信息，这给跨索引搜索造成麻烦。从下面的例子我们可以看出，用MultiSearcher来一起搜索两个索引和分别用IndexSearcher来搜索两个索引所得出的分数是有很大差异的。究其原因是MultiSearcher的docFreq(Term term)函数计算了包含两个索引中包含此词的总文档数，而IndexSearcher仅仅计算了每个索引中包含此词的文档数。当两个索引包含的文档总数是有很大不同的时候，分数是无法比较的。1234567891011121314151617181920212223242526272829public void testMultiIndex() throws Exception&#123; MultiIndexSimilarity sim = new MultiIndexSimilarity(); File indexDir01 = new File(\"TestMultiIndex/TestMultiIndex01\"); File indexDir02 = new File(\"TestMultiIndex/TestMultiIndex02\"); IndexReader reader01 = IndexReader.open(FSDirectory.open(indexDir01)); IndexReader reader02 = IndexReader.open(FSDirectory.open(indexDir02)); IndexSearcher searcher01 = new IndexSearcher(reader01); searcher01.setSimilarity(sim); IndexSearcher searcher02 = new IndexSearcher(reader02); searcher02.setSimilarity(sim); MultiSearcher multiseacher = new MultiSearcher(searcher01, searcher02); multiseacher.setSimilarity(sim); QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"contents\", new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(\"common\"); TopDocs docs = searcher01.search(query, 10); System.out.println(\"----------------------------------------------\"); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; System.out.println(\"----------------------------------------------\"); docs = searcher02.search(query, 10); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; System.out.println(\"----------------------------------------------\"); docs = multiseacher.search(query, 20); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; 12345678910111213141516171819202122232425262728293031结果为：------------------------------- docid : 0 score : 0.49317428 docid : 1 score : 0.49317428 docid : 2 score : 0.49317428 docid : 3 score : 0.49317428 docid : 4 score : 0.49317428 docid : 5 score : 0.49317428 docid : 6 score : 0.49317428 docid : 7 score : 0.49317428 ------------------------------- docid : 0 score : 0.45709616 docid : 1 score : 0.45709616 docid : 2 score : 0.45709616 docid : 3 score : 0.45709616 docid : 4 score : 0.45709616 ------------------------------- docid : 0 score : 0.5175894 docid : 1 score : 0.5175894 docid : 2 score : 0.5175894 docid : 3 score : 0.5175894 docid : 4 score : 0.5175894 docid : 5 score : 0.5175894 docid : 6 score : 0.5175894 docid : 7 score : 0.5175894 docid : 8 score : 0.5175894 docid : 9 score : 0.5175894 docid : 10 score : 0.5175894 docid : 11 score : 0.5175894 docid : 12 score : 0.5175894 (6) float coord(int overlap, int maxOverlap)一次搜索可能包含多个搜索词，而一篇文档中也可能包含多个搜索词，此项表示，当一篇文档中包含的搜索词越多，则此文档则打分越高。123456789101112131415161718192021222324252627282930public void TestCoord() throws Exception &#123; MySimilarity sim = new MySimilarity(); File indexDir = new File(\"TestCoord\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new StandardAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED); Document doc1 = new Document(); Field f1 = new Field(\"contents\", \"common hello world\", Field.Store.NO, Field.Index.ANALYZED); doc1.add(f1); writer.addDocument(doc1); Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common common common\", Field.Store.NO, Field.Index.ANALYZED); doc2.add(f2); writer.addDocument(doc2); for(int i = 0; i &lt; 10; i++)&#123; Document doc3 = new Document(); Field f3 = new Field(\"contents\", \"world\", Field.Store.NO, Field.Index.ANALYZED); doc3.add(f3); writer.addDocument(doc3); &#125; writer.close(); IndexReader reader = IndexReader.open(FSDirectory.open(indexDir)); IndexSearcher searcher = new IndexSearcher(reader); searcher.setSimilarity(sim); QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"contents\", new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(\"common world\"); TopDocs docs = searcher.search(query, 2); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125; &#125; 12345678class MySimilarity extends Similarity &#123; @Override public float coord(int overlap, int maxOverlap) &#123; return 1; &#125;&#125; 如上面的实例，当coord返回1，不起作用的时候，文档一虽然包含了两个搜索词common和world，但由于world的所在的文档数太多，而文档二包含common的次数比较多，因而文档二分数较高：12docid : 1 score : 1.9059997 docid : 0 score : 1.2936771 而当coord起作用的时候，文档一由于包含了两个搜索词而分数较高：12345678class MySimilarity extends Similarity &#123; @Override public float coord(int overlap, int maxOverlap) &#123; return overlap / (float)maxOverlap; &#125;&#125; 12docid : 0 score : 1.2936771 docid : 1 score : 0.95299983 (7) float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length)由于Lucene引入了payload，因而可以存储一些自己的信息，用户可以根据自己存储的信息，来影响Lucene的打分。payload的定义我们知道，索引是以倒排表形式存储的，对于每一个词，都保存了包含这个词的一个链表，当然为了加快查询速度，此链表多用跳跃表进行存储。Payload信息就是存储在倒排表中的，同文档号一起存放，多用于存储与每篇文档相关的一些信息。当然这部分信息也可以存储域里(stored Field)，两者从功能上基本是一样的，然而当要存储的信息很多的时候，存放在倒排表里，利用跳跃表，有利于大大提高搜索速度。Payload的存储方式如下图：由payload的定义，我们可以看出，payload可以存储一些不但与文档相关，而且与查询词也相关的信息。比如某篇文档的某个词有特殊性，则可以在这个词的这个文档的position信息后存储payload信息，使得当搜索这个词的时候，这篇文档获得较高的分数。要利用payload来影响查询需要做到以下几点，下面举例用标记的词在payload中存储1，否则存储0：首先要实现自己的Analyzer从而在Token中放入payload信息：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class BoldAnalyzer extends Analyzer &#123; @Override public TokenStream tokenStream(String fieldName, Reader reader) &#123; TokenStream result = new WhitespaceTokenizer(reader); result = new BoldFilter(result); return result; &#125;&#125;class BoldFilter extends TokenFilter &#123; public static int IS_NOT_BOLD = 0; public static int IS_BOLD = 1; private TermAttribute termAtt; private PayloadAttribute payloadAtt; protected BoldFilter(TokenStream input) &#123; super(input); termAtt = addAttribute(TermAttribute.class); payloadAtt = addAttribute(PayloadAttribute.class); &#125; @Override public boolean incrementToken() throws IOException &#123; if (input.incrementToken()) &#123; final char[] buffer = termAtt.termBuffer(); final int length = termAtt.termLength(); String tokenstring = new String(buffer, 0, length); if (tokenstring.startsWith(\"&lt;b&gt;\") &amp;&amp; tokenstring.endsWith(\"&lt;/b&gt;\")) &#123; tokenstring = tokenstring.replace(\"&lt;b&gt;\", \"\"); tokenstring = tokenstring.replace(\"&lt;/b&gt;\", \"\"); termAtt.setTermBuffer(tokenstring); payloadAtt.setPayload(new Payload(int2bytes(IS_BOLD))); &#125; else &#123; payloadAtt.setPayload(new Payload(int2bytes(IS_NOT_BOLD))); &#125; return true; &#125; else return false; &#125; public static int bytes2int(byte[] b) &#123; int mask = 0xff; int temp = 0; int res = 0; for (int i = 0; i &lt; 4; i++) &#123; res &lt;&lt;= 8; temp = b[i] &amp; mask; res |= temp; &#125; return res; &#125; public static byte[] int2bytes(int num) &#123; byte[] b = new byte[4]; for (int i = 0; i &lt; 4; i++) &#123; b[i] = (byte) (num &gt;&gt;&gt; (24 - i * 8)); &#125; return b; &#125;&#125; 然后，实现自己的Similarity，从payload中读出信息，根据信息来打分。12345678910111213class PayloadSimilarity extends DefaultSimilarity &#123; @Override public float scorePayload(int docId, String fieldName, int start, int end, byte[] payload, int offset, int length) &#123; int isbold = BoldFilter.bytes2int(payload); if(isbold == BoldFilter.IS_BOLD)&#123; System.out.println(\"It is a bold char.\"); &#125; else &#123; System.out.println(\"It is not a bold char.\"); &#125; return 1; &#125;&#125; 最后，查询的时候，一定要用PayloadXXXQuery(在此用PayloadTermQuery，在Lucene 2.4.1中，用BoostingTermQuery)，否则scorePayload不起作用。1234567891011121314151617181920212223public void testPayloadScore() throws Exception &#123; PayloadSimilarity sim = new PayloadSimilarity(); File indexDir = new File(\"TestPayloadScore\"); IndexWriter writer = new IndexWriter(FSDirectory.open(indexDir), new BoldAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED); Document doc1 = new Document(); Field f1 = new Field(\"contents\", \"common hello world\", Field.Store.NO, Field.Index.ANALYZED); doc1.add(f1); writer.addDocument(doc1); Document doc2 = new Document(); Field f2 = new Field(\"contents\", \"common &lt;b&gt;hello&lt;/b&gt; world\", Field.Store.NO, Field.Index.ANALYZED); doc2.add(f2); writer.addDocument(doc2); writer.close(); IndexReader reader = IndexReader.open(FSDirectory.open(indexDir)); IndexSearcher searcher = new IndexSearcher(reader); searcher.setSimilarity(sim); PayloadTermQuery query = new PayloadTermQuery(new Term(\"contents\", \"hello\"), new MaxPayloadFunction()); TopDocs docs = searcher.search(query, 10); for (ScoreDoc doc : docs.scoreDocs) &#123; System.out.println(\"docid : \" + doc.doc + \" score : \" + doc.score); &#125;&#125; 如果scorePayload函数始终是返回1，则结果如下，不起作用。1234It is not a bold char.It is a bold char.docid : 0 score : 0.2101998docid : 1 score : 0.2101998 如果scorePayload函数如下：1234567891011121314class PayloadSimilarity extends DefaultSimilarity &#123; @Override public float scorePayload(int docId, String fieldName, int start, int end, byte[] payload, int offset, int length) &#123; int isbold = BoldFilter.bytes2int(payload); if(isbold == BoldFilter.IS_BOLD)&#123; System.out.println(\"It is a bold char.\"); return 10; &#125; else &#123; System.out.println(\"It is not a bold char.\"); return 1; &#125; &#125;&#125; 则结果如下，同样是包含hello，包含加粗的文档获得较高分：1234It is not a bold char.It is a bold char.docid : 1 score : 2.101998docid : 0 score : 0.2101998 继承并实现自己的collector以上各种方法，已经把Lucene score计算公式的所有变量都涉及了，如果这还不能满足您的要求，还可以继承实现自己的collector。在Lucene 2.4中，HitCollector有个函数public abstract void collect(int doc, float score)，用来收集搜索的结果。其中TopDocCollector的实现如下：1234567891011121314public void collect(int doc, float score) &#123; if (score &gt; 0.0f) &#123; totalHits++; if (reusableSD == null) &#123; reusableSD = new ScoreDoc(doc, score); &#125; else if (score &gt;= reusableSD.score) &#123; reusableSD.doc = doc; reusableSD.score = score; &#125; else &#123; return; &#125; reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD); &#125;&#125; 此函数将docid和score插入一个PriorityQueue中，使得得分最高的文档先返回。我们可以继承HitCollector，并在此函数中对score进行修改，然后再插入PriorityQueue，或者插入自己的数据结构。比如我们在另外的地方存储docid和文档创建时间的对应，我们希望当文档时间是一天之内的分数最高，一周之内的分数其次，一个月之外的分数很低。我们可以这样修改：1234567891011121314151617181920212223242526272829303132333435363738394041public static long milisecondsOneDay = 24L * 3600L * 1000L;public static long millisecondsOneWeek = 7L * 24L * 3600L * 1000L;public static long millisecondsOneMonth = 30L * 24L * 3600L * 1000L;public void collect(int doc, float score) &#123; if (score &gt; 0.0f) &#123; long time = getTimeByDocId(doc); if(time &lt; milisecondsOneDay) &#123; score = score * 1.0; &#125; else if (time &lt; millisecondsOneWeek)&#123; score = score * 0.8; &#125; else if (time &lt; millisecondsOneMonth) &#123; score = score * 0.3; &#125; else &#123; score = score * 0.1; &#125; totalHits++; if (reusableSD == null) &#123; reusableSD = new ScoreDoc(doc, score); &#125; else if (score &gt;= reusableSD.score) &#123; reusableSD.doc = doc; reusableSD.score = score; &#125; else &#123; return; &#125; reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD); &#125;&#125; 在Lucene 3.0中，Collector接口为void collect(int doc)，TopScoreDocCollector实现如下：12345678910public void collect(int doc) throws IOException &#123; float score = scorer.score(); totalHits++; if (score &lt;= pqTop.score) &#123; return; &#125; pqTop.doc = doc + docBase; pqTop.score = score; pqTop = pq.updateTop();&#125; 同样可以用上面的方式影响其打分。","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/categories/搜索引擎/"}],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/tags/搜索引擎/"},{"name":"apache lucene","slug":"apache-lucene","permalink":"http://blog.pinbot.me/tags/apache-lucene/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/categories/搜索引擎/"}]},{"title":"django自定义storage","slug":"django自定义storage","date":"2016-11-09T07:48:37.000Z","updated":"2017-06-30T02:44:25.000Z","comments":true,"path":"2016/11/09/django自定义storage/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/django自定义storage/","excerpt":"最近遇到了这样的一个问题，由于某些原因，需要把静态文件放到cdn上，之前使用的是django默认的storage（FileSystemStorage）。于是这里需要自定义storage。第一次写storage，过程中遇到一些坑，记录下来。","text":"最近遇到了这样的一个问题，由于某些原因，需要把静态文件放到cdn上，之前使用的是django默认的storage（FileSystemStorage）。于是这里需要自定义storage。第一次写storage，过程中遇到一些坑，记录下来。 什么是storage其实这个玩意要我说明白，好像有点难，于是我就按我的方法说，如果有错，谢谢指正！首先，我们的web中使用了许多的模版文件，静态文件，如js,css，图片这些，我们在配置服务器的时候，让nginx对请求进行分发，将动态请求分发给uWSGI，将静态文件交由nginx处理，这里nginx将从文件系统中读取静态资源，这里的文件系统就是我们当前的storage。但是这里我们是想用cdn，于是我们这里的静态资源不在从服务器上加载，而是从我们的cdn服务提供商那里加载，这里我们又有一个问题了，cdn服务提供商怎么给出正确的资源，于是我们就需要自己来写一个storage，将原来的文件系统更换为cdn提供商的空间。 storage的结构和重写这里就需要参考django的官方文档了(原版,翻译版)，由于我的外语水平不是很好，做的时候为了图效率，就没有花时间去琢磨英文官方文档，所以这里给一个翻译的文档地址，当然，如果你能很快看明白官方的文档，那去读原版是更好的选择。首先，我们看文件系统的基类，其源码位于django/core/files/storage.py文件下。这里我就不贴它的代码了，如果需要查看结构，请自行到该文件下查看！从文档中，我们可以了解的一些东西，这个类的子类需要不带参数实例化，于是我们需要在settings中加入自定义的参数。1234567891011STATICFILES_STORAGE = 'project.storage.CustomStorage'CUSTOM_STORAGE_OPTIONS = &#123; 'AccessKeyId': 'your_accesskeyid', 'AccessKeySecret': 'your_accesskeyidsecret', 'endpoint': 'oss-cn-hangzhou.aliyuncs.com', 'oss_url': 'http://oss-cn-hangzhou.aliyuncs.com', 'bucketname': 'your_bucketname',&#125;COMPRESS_STORAGE = 'project.storage.CustomCompressorFileStorage'STATIC_URL = 'your_cdn_prefix_address' 参数说明： 这里我们指定了STATICFILES_STORAGE为我们自定义的CustomStorage,如果不指定，那么系统将会使用默认的FileSystemStorage CUSTOM_STORAGE_OPTIONS是一个你自定义的Storage初始化参数，这里我用字典来初始化，当然，你可以使用你觉得合理的任何数据类型。 COMPRESS_STORAGE是我们指定的压缩文件的存放位置，与STATICFILES_STORAGE同理。 STATIC_URL这个就是静态文件的路由前缀，例如你的文件系统中路径是’aa/bb.js’，你的cdn地址是’static.cdn.com’，这里就使用cdn地址作为你的STATIC_URL 从文档中，我们可以知道，自定义的storage，必须实现_open,_save两个方法，我们参考源码可以知道，这两个方法分别被save和open两个方法调用，而这两个方法的作用分别是‘打开文件，读取内容’、‘将文件保存到指定的位置’，由此，我们需要自己定义的存储就在这里来实现。由于这里我采用的是阿里云的oss，所以认证的过程，我们放在构造函数中完成，本着D.R.Y的原则，为了让多个自定义的storage可以使用，我们将它放在外部，只在构造函数里来使用它。123456789101112131415def authticate(option): auth = oss2.Auth( option.get('AccessKeyId'), option.get('AccessKeySecret') ) service = oss2.Service( auth, option.get('endpoint') ) bucket = oss2.Bucket( auth, option.get('oss_url'), option.get('bucketname') )return (auth, service, bucket) 认证过后，就可以重写save过程了。123456def _save(self, name, content): self.bucket.put_object( name, content ) return name 这里我们使用的是阿里云的oss，它的save就这么简单。文档可以直接google阿里云oss！这里就不贴出来了！哈哈，这里是不是很简单！其实理解了它的各个方法，真的很简单…………继续，由于open方法是打开本地文件系统的文件，我们就不重写它了。其他的方法。文档中说到，delete()，exists()，listdir()，size()，url() 这几个方法都需要被覆写，不然就会抛出NotImplementedError异常。这里我们通过源码可以解释一下，这些玩意在干嘛。 delete方法：顾名思义，就是删除，此方法被调用时，从storage中删除文件 exists方法：额，还是顾名思义，就是判断是否存在该文件，返回布尔值 listdir方法：返回文件列表 size方法：返回文件大小 url方法：这个方法需要提一下，我在之前重写的时候，直接pass了，所以，我在打开xadmin时，就会一直报错，于是我就找了很久的原因，我在这个函数中下了断点，最后发现，这个函数是必须自己重新写的（如果你的文件是静态文件，可以通过url访问的话）。因为如果不重写它，返回的是一个None，于是该文件就没有url可以访问，在某些需要判断的地方，也会报错！ 最后不同的云服务提供商的上传方式可能不一样，但是原理都是一样的，重写save方法，改为上传到云端，重写需要使用的方法。最后collectstatic,compress即可。","categories":[{"name":"django","slug":"django","permalink":"http://blog.pinbot.me/categories/django/"}],"tags":[{"name":"django","slug":"django","permalink":"http://blog.pinbot.me/tags/django/"},{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/tags/技术/"}],"keywords":[{"name":"django","slug":"django","permalink":"http://blog.pinbot.me/categories/django/"}]},{"title":"MySQL性能优化","slug":"mysql性能优化","date":"2016-11-09T07:48:37.000Z","updated":"2017-06-30T02:43:20.000Z","comments":true,"path":"2016/11/09/mysql性能优化/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/mysql性能优化/","excerpt":"作者：liudong at 2016-11-09 15:48:37 ##1、为查询优化你的查询大多数的MySQL服务器都开启了查询缓存。这是提高性最有效的方法之一，而且这是被MySQL的数据库引擎处理的。当有很多相同的查询被执行了多次的时候，这些查询结果会被放到一个缓存中，这样，后续的相同的查询就不用操作表而直接访问缓存结果了。 123456// 查询缓存不开启$r = mysql_query(&quot;SELECT username FROM user WHERE signup_date &gt;= CURDATE()&quot;);// 开启查询缓存$today = date(&quot;Y-m-d&quot;);$r = mysql_query(&quot;SELECT username FROM user WHERE signup_date &gt;= &apos;$today&apos;&quot;);","text":"作者：liudong at 2016-11-09 15:48:37 ##1、为查询优化你的查询大多数的MySQL服务器都开启了查询缓存。这是提高性最有效的方法之一，而且这是被MySQL的数据库引擎处理的。当有很多相同的查询被执行了多次的时候，这些查询结果会被放到一个缓存中，这样，后续的相同的查询就不用操作表而直接访问缓存结果了。 123456// 查询缓存不开启$r = mysql_query(&quot;SELECT username FROM user WHERE signup_date &gt;= CURDATE()&quot;);// 开启查询缓存$today = date(&quot;Y-m-d&quot;);$r = mysql_query(&quot;SELECT username FROM user WHERE signup_date &gt;= &apos;$today&apos;&quot;); 区别： CURDATE() ，MySQL的查询缓存对这个函数不起作用。所以，像 NOW() 和 RAND() 或是其它的诸如此类的SQL函数都不会开启查询缓存，因为这些函数的返回是会不定的易变的。所以，你所需要的就是用一个变量来代替MySQL的函数，从而开启缓存。 ##2、EXPLAIN 你的SELECT查询使用EXPLAIN关键字可以让你知道MySQL是如何处理你的SQL语句的。 有表关联的查询，如下列：123select username, group_namefrom users ujoins groups g on (u.group_id = g.id) 发现查询缓慢，然后在group_id字段上增加索引，则会加快查询 ##3、当只要一行数据时使用LIMIT 1当你查询表的有些时候，你已经知道结果只会有一条结果，单因为你可能需要去fetch游标，或是你也许会去检查返回的记录数。在这种情况下，加上LIMIT 1 可以增加性能。这样一样， MySQL数据库引擎会在找到一条数据后停止搜索，而不是继续往后查找下一条符合记录的数据。下面的示例，只是为了找一下是否有“成都”的用户，很明显，后面的会比前面的更有效率。（请注意，第一条中是Select *，第二条是Select 1）1234567891011// 没有效率的：$r = mysql_query(&quot;SELECT * FROM user WHERE country = &apos;China&apos;&quot;);if (mysql_num_rows($r) &gt; 0) &#123; // ...&#125;// 有效率的：$r = mysql_query(&quot;SELECT 1 FROM user WHERE country = &apos;China&apos; LIMIT 1&quot;);if (mysql_num_rows($r) &gt; 0) &#123;// ...&#125; ##4、为搜索字段建索引索引并不一定就是给主键或是唯一的字段。如果在你的表中，有某个字段你总要会经常用来做搜索，那么，请为其建立索引吧。12345678910111213141516171819添加PRIMARY KEY（主键索引）mysql&gt;ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` )添加UNIQUE(唯一索引)mysql&gt;ALTER TABLE `table_name` ADD UNIQUE (`column`)添加INDEX(普通索引)mysql&gt;ALTER TABLE `table_name` ADD INDEX index_name ( `column` )添加FULLTEXT(全文索引)mysql&gt;ALTER TABLE `table_name` ADD FULLTEXT (`column`)添加多列索引mysql&gt;ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) ##5、在Join表的时候使用相当类型的列，并将其索引如果你的应用程序有很多JOIN查询，你应该确认两个表中Join的字段是被建过索引的。这样，MySQL内部会启动为你优化Join的SQL语句的机制。而且，这些被用来Join的字段，应该是相同的类型的。例如：如果你要把DECIMAL字段和一个INT字段JOIN在一起，MYSQL就无法使用他们的索引。对于那些STRING类型，还需要有相同的字符集才行（两个表的字符集有可能不一样） ##6、千万不要ORDER BY RAND() ##7、避免SELECT *从数据库里读出越多的数据，那么查询就会变得越慢。并且，如果你的数据库服务器和WEB服务器是两台独立的服务器的话，这还会增加网络传输的负载。123456789// 不推荐$r = mysql_query(&quot;SELECT * FROM user WHERE user_id = 1&quot;);$d = mysql_fetch_assoc($r);echo &quot;Welcome &#123;$d[&apos;username&apos;]&#125;&quot;;// 推荐$r = mysql_query(&quot;SELECT username FROM user WHERE user_id = 1&quot;);$d = mysql_fetch_assoc($r);echo &quot;Welcome &#123;$d[&apos;username&apos;]&#125;&quot;; ##8、永远为两张表设置一个ID为数据库里的每张表都设置一个ID作为其主键，而最好的是一个INT型（推荐使用UNSIGNED），并设置上自动增长的AUTO INCREMENT标志。就算是你 users 表有一个主键叫 “email”的字段，你也别让它成为主键。使用 VARCHAR 类型来当主键会使用得性能下降。另外，在你的程序中，你应该使用表的ID来构造你的数据结构。而且，在MySQL数据引擎下，还有一些操作需要使用主键，在这些情况下，主键的性能和设置变得非常重要，比如，集群，分区 ##9、使用 ENUM 而不是 VARCHAR ？ENUM 类型是非常快和紧凑的。在实际上，其保存的是 TINYINT，但其外表上显示为字符串。这样一来，用这个字段来做一些选项列表变得相当的完美。 如果你有一个字段，比如“性别”，“国家”，“民族”，“状态”或“部门”，你知道这些字段的取值是有限而且固定的，那么，你应该使用 ENUM 而不是 VARCHAR。 ##10、从 PROCEDURE ANALYSE() 取得建议 ？PROCEDURE ANALYSE() 会让 MySQL 帮你去分析你的字段和其实际的数据，并会给你一些有用的建议。只有表中有实际的数据，这些建议才会变得有用，因为要做一些大的决定是需要有数据作为基础的。 例如，如果你创建了一个 INT 字段作为你的主键，然而并没有太多的数据，那么，PROCEDURE ANALYSE()会建议你把这个字段的类型改成 MEDIUMINT 。或是你使用了一个 VARCHAR 字段，因为数据不多，你可能会得到一个让你把它改成 ENUM 的建议。这些建议，都是可能因为数据不够多，所以决策做得就不够准。 ##11、尽可能的使用 NOT NULL摘自MySQL官方文档“NULL columns require additional space in the row to record whether their values are NULL. For MyISAM tables, each NULL column takes one bit extra, rounded up to the nearest byte.” ##12、把IP地址存成 UNSIGNED INT很多使用者都会创建一个 VARCHAR(15) 字段来存放字符串形式的IP而不是整形的IP。如果你用整形来存放，只需要4个字节，并且你可以有定长的字段。而且，这会为你带来查询上的优势，尤其是当你需要使用这样的WHERE条件：IP between ip1 and ip2。 ##13、拆分大的 DELETE 或 INSERT 语句如果有一个大的处理，你定你一定把其拆分，使用 LIMIT 条件是一个好的方法。下面是一个示例：123456789while (1) &#123;//每次只做1000条mysql_query(&quot;DELETE FROM logs WHERE log_date &lt;= &apos;2009-11-01&apos; LIMIT 1000&quot;);if (mysql_affected_rows() == 0) &#123; // 没得可删了，退出！ break;&#125;// 每次都要休息一会儿usleep(50000); ##14、当查询较慢的时候，可用Join来改写一下该查询来进行优化123456789101112131415161718192021222324mysql&gt; select sql_no_cache * from guang_deal_outs where deal_id in (select id from guang_deals where id = 100017151) ; Empty set (18.87 sec)mysql&gt; select sql_no_cache a.* from guang_deal_outs a inner join guang_deals b on a.deal_id = b.id where b.id = 100017151; Empty set (0.01 sec)原因mysql&gt; desc select sql_no_cache * from guang_deal_outs where deal_id in (select id from guang_deals where id = 100017151) ;+----+--------------------+-----------------+-------+---------------+---------+---------+-------+----------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+-----------------+-------+---------------+--------- +---------+-------+----------+-------------+| 1 | PRIMARY | guang_deal_outs | ALL | NULL | NULL | NULL | NULL | 18633779 | Using where || 2 | DEPENDENT SUBQUERY | guang_deals | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+--------------------+-----------------+-------+---------------+--------- +---------+-------+----------+-------------+2 rows in set (0.04 sec)mysql&gt; desc select sql_no_cache a.* from guang_deal_outs a inner join guang_deals b on a.deal_id = b.id where b.id = 100017151;+----+-------------+-------+-------+---------------------- +----------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------------- +----------------------+---------+-------+------+-------------+| 1 | SIMPLE | b | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index || 1 | SIMPLE | a | ref | idx_guang_dlout_dlid | idx_guang_dlout_dlid | 4 | const | 1 | |+----+-------------+-------+-------+---------------------- +----------------------+---------+-------+------+-------------+ 2 rows in set (0.05 sec) ##15、子查询时用exists而不是用in1234不推荐inselect * from guang_deal_outs where deal_id in (select id from guang_deals where id = 100017151);推荐existsselect * from guang_deal_outs where exists (select * from guang_deals where id = 100017151);","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://blog.pinbot.me/tags/运维/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"Vue.js基础入门","slug":"Vue.js基础入门","date":"2016-11-09T06:08:08.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/11/09/Vue.js基础入门/","link":"","permalink":"http://blog.pinbot.me/2016/11/09/Vue.js基础入门/","excerpt":"作者: 李纯利 Vue.js是一套构建用户界面的渐进式框架,Vue的核心库只关注视图层，并且非常容易学习，非常容易与其它库或已有项目整合;Vue.js 的目标是通过尽可能简单的 API 实现响应的数据绑定和组合的视图组件。","text":"作者: 李纯利 Vue.js是一套构建用户界面的渐进式框架,Vue的核心库只关注视图层，并且非常容易学习，非常容易与其它库或已有项目整合;Vue.js 的目标是通过尽可能简单的 API 实现响应的数据绑定和组合的视图组件。 一、模板语法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 //数据绑定最常见的形式:&#123;&#123; &#125;&#125;的文本插入. //HTML &lt;div id='app'&gt; &lt;!-- 文本 --&gt; &lt;!-- v-once指令只能执行一次性地插值 --&gt; &lt;div v-once&gt;&#123;&#123;message | capitalize&#125;&#125;&lt;/div&gt; &lt;!-- 纯HTML --&gt; &lt;div v-html=\"htmlStr\"&gt;&lt;/div&gt; &lt;!-- 属性 --&gt; &lt;div v-bind:class=\"vueJs\"&gt;&lt;/div&gt; &lt;!-- 使用JavaScript表达式 --&gt; &lt;div v-bind:title=\"num + 1\"&gt;&lt;/div&gt; &lt;!-- 修饰符 --&gt; &lt;form v-on:submit.prevent=\"onSubmit\"&gt;&lt;/form&gt; &lt;!-- 缩写 --&gt; &lt;!-- 完整语法 --&gt; &lt;div v-bind:class=\"vueJs\"&gt;&lt;/div&gt; &lt;div v-on:click=\"greet\"&gt;&lt;/div&gt; &lt;!-- 缩写 --&gt; &lt;div :class=\"vueJs\"&gt;&lt;/div&gt; &lt;div @click=\"greet\"&gt;&lt;/div&gt; &lt;/div&gt; //js var app = new Vue(&#123; el: '#app', data: &#123; message: 'Hello Vue', htmlStr: '&lt;span&gt;HTML&lt;/span&gt;', vueJs: 'vue-div', num: 2 &#125;, filters: &#123; capitalize: function (value) &#123; if (!value) return '' value = value.toString() return value..split(''); &#125; &#125; methods: &#123; onSubmit:function(event)&#123; alert('hello!') &#125;, greet: function()&#123; alert('缩写.') &#125; &#125; &#125;)&lt;/script&gt; v-前缀在模板中是作为一个标示 Vue 特殊属性的明显标识。二.计算属性 123456789101112131415&lt;div id='app'&gt; &lt;div&gt;&#123;&#123;message&#125;&#125;&lt;/div&gt;&lt;/div&gt;var app = new Vue(&#123; el: '#app', data: &#123; message: 'HelloVue' &#125;, reverseMessage: &#123; greet: function()&#123; return this.message.split('').reverse().join(''); &#125; &#125;&#125;) 三.class与style绑定 语法:v-bind:class, v-bind:style 12345678910111213141516&lt;div class=\"static\" v-bind:class=\"&#123; active: isActive &#125;\"&gt;&lt;/div&gt;&lt;div v-bind:class=\"[activeClass, errorClass]\"&gt;&lt;div v-bind:style=\"&#123; color: activeColor, fontSize: fontSize + 'px' &#125;\"&gt;&lt;/div&gt;var app = new Vue(&#123; el: '#app', data: &#123; activeClass: 'active', errorClass: 'errors', activeColor: 'red', fontSize: 30 &#125;&#125;) 这里也可以绑定返回对象的计算属性。123456789101112&lt;div v-bind:class=\"classObject\"&gt;&lt;/div&gt;var app = new Vue(&#123; el: '#app', data: &#123; classObject: &#123; active: true, 'text-danger': false &#125; &#125;&#125;) 四.条件渲染语法:v-show,v-if,v-elsev-else必须紧跟在v-if,v-show后面,不然就不能被识别.v-if与v-show的区别v-show始终保持在Dom中,用简单的display来切换的.v-if有更高的切换消耗,如果频繁的切换还是使用v-show比较好. 1234567&lt;h1 v-if=\"ok\"&gt;Yes&lt;/h1&gt;&lt;h1 v-else&gt;No&lt;/h1&gt;&lt;h1 v-show=\"ok\"&gt;Yes&lt;/h1&gt;&lt;template v-if=\"ok\"&gt; &lt;h1&gt;1&lt;/h1&gt;&lt;/template&gt; 注意 v-show 不支持 &lt;template&gt; 语法.五.列表渲染语法: v-for123&lt;div v-for=\"item of items\"&gt;&lt;/div&gt;&lt;div v-for=\"item in items\"&gt;&lt;/div&gt; 如同 v-if 模板，你也可以用带有 v-for 的 &lt;template&gt; 标签来渲染多个元素块.在自定义组件里，你可以像任何普通元素一样用 v-for.12&lt;my-component v-for=\"item in items\"&gt;&lt;/my-component&gt; 六.事件处理器语法:v-on:事件 事件修饰符:.submit提交事件不再重载页面.stop阻止单击事件冒泡.prevent修饰符可以串联.capture添加事件侦听器时使用事件捕获模式.self只当事件在该元素本身（而不是子元素）触发时触发回调 按键修饰符:全部的按键别名：.enter,.tab,.delete (捕获 “删除” 和 “退格” 键),.esc,.space,.up,.down,.left,.right. 可以通过全局 config.keyCodes 对象自定义按键修饰符别名：123// 可以使用 v-on:keyup.fVue.config.keyCodes.f = 113 七.表单控件绑定v-model 指令在表单控件元素上创建双向数据绑定. 修饰符:.lazy在默认情况下， v-model 在 input 事件中同步输入框的值与数据，但你可以添加一个修饰符 lazy ，从而转变为在 change 事件中同步..number只能输入number类型的值..trim去掉首尾空格","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"Vue.js","slug":"Vue-js","permalink":"http://blog.pinbot.me/tags/Vue-js/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"使用python实现图片转字符画","slug":"使用python实现图片转字符画","date":"2016-11-07T12:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/11/07/使用python实现图片转字符画/","link":"","permalink":"http://blog.pinbot.me/2016/11/07/使用python实现图片转字符画/","excerpt":"可能不少人都见过这张图…… 这是一张多帧合成的gif图片, 每帧图片由ascii字符组成。 今天要讲的是如何将一张静态图片转为字符, 关于多帧合成gif的方法这里不涉及。","text":"可能不少人都见过这张图…… 这是一张多帧合成的gif图片, 每帧图片由ascii字符组成。 今天要讲的是如何将一张静态图片转为字符, 关于多帧合成gif的方法这里不涉及。 灰度首先介绍一下灰度的概念。 灰度（Gray scale）数字图像是每个像素只有一个采样颜色的图像。这类图像通常显示为从最暗黑色到最亮的白色的灰度，尽管理论上这个采样可以任何颜色的不同深浅，甚至可以是不同亮度上的不同颜色。灰度图像与黑白图像不同，在计算机图像领域中黑白图像只有黑白两种颜色，灰度图像在黑色与白色之间还有许多级的颜色深度。 如下图所示, 左边的第一幅是原始的彩色照片, 右边的为灰度图片。 灰度图片可以用黑白的颜色深度来表示一张图片, 这样我们可以在程序中用较大块的字符(比如M或N)来表示深色的像素, 用小块的字符(比如.或:)表示浅色。 我们设置一个列表: 12CHARS = list(\"\"\"MNHQ$OC?7&gt;!:-;. \"\"\")CHARS_N = len(CHARS) 从左住右, 表示的颜色深度依次递减。 RGB-&gt;灰度-&gt;字符那么如何将彩色图片转为灰度呢？ 常见的公式是: 1Gray = 0.30 * R + 0.59 * G + 0.11 * B 不同的人眼对RGB颜色的感知并不相同，所以转换时候给予不同的权重。这个原理也普遍应用于计算机图像处理系统。 这里我们使用rgb2char实现这个转换: 123456def rgb2char(r, g, b, alpha=256): if alpha == 0: return ' ' gray = float(0.30 * r + 0.59 * g + 0.11 * b) index = int(round(gray / 256 * (CHARS_N - 1))) return CHARS[index] 函数有一个alpha参数指的是图像的透明度, 为0是则表示该像素为透明像素, 我们将其转换为一个空格。 转换实现每个像素的转换函数后, 需要对整个图片的每个像素进行处理, 这里我们使用python官方的PIL库。 123456789101112131415161718192021from PIL import Imagedef get_image(path): im = Image.open(path) im = im.resize((HEIGHT, WIDTH), Image.NEAREST) return im def image2text(im): text = [] for i in range(HEIGHT): for j in range(WIDTH): text.append(rgb2char(*im.getpixel((j, i)))) text.append('\\n') return ''.join(text)def convert(path): image = get_image(path) print(image2text(image)) 至些, 基本的转换就完成了。完整的代码可以在这里看到。 另外如果要处理复杂图片的转换需要做许多优化, 大家可以自行研究。","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"http://blog.pinbot.me/tags/python/"},{"name":"ascii","slug":"ascii","permalink":"http://blog.pinbot.me/tags/ascii/"},{"name":"image","slug":"image","permalink":"http://blog.pinbot.me/tags/image/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"Django快速入门","slug":"Django快速入门","date":"2016-10-29T04:53:26.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/10/29/Django快速入门/","link":"","permalink":"http://blog.pinbot.me/2016/10/29/Django快速入门/","excerpt":"作者：Adam at 2016-10-29 12:53:26(注：本教程来源于官方最新Django教程) 作为一名前端开发，了解后端技术很有必要。作为一名web程序员，掌握web开发全栈技能，成为未来发展的必然趋势。既然聘宝的研发以Python开发为主，我们有必要先学习一下Django这个开发框架。","text":"作者：Adam at 2016-10-29 12:53:26(注：本教程来源于官方最新Django教程) 作为一名前端开发，了解后端技术很有必要。作为一名web程序员，掌握web开发全栈技能，成为未来发展的必然趋势。既然聘宝的研发以Python开发为主，我们有必要先学习一下Django这个开发框架。 首先，为了快速进入学习，我们假设你已经安装好聘宝研发的Python+Docker开发环境。接着我们建立虚拟开发环境。 123456➜ ~ mkdir myFirstDjango➜ ~ cd myFirstDjango➜ myFirstDjango mkvirtualenv myFirstDjangoNew python executable in myFirstDjango/bin/pythonInstalling setuptools, pip, wheel...done.(myFirstDjango)➜ myFirstDjango 安装Django 12345678(myFirstDjango)➜ myFirstDjango pip install djangoYou are using pip version 7.1.0, however version 8.1.2 is available.You should consider upgrading via the 'pip install --upgrade pip' command.Collecting django Downloading http://pypi.doubanio.com/packages/8a/09/46f790104abca7eb93786139d3adde9366b1afd59a77b583a1f310dc8cbd/Django-1.10.2-py2.py3-none-any.whl (6.8MB) 100% |████████████████████████████████| 6.8MB 1.5MB/sInstalling collected packages: djangoSuccessfully installed django-1.10.2 创建一个项目mysite1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950(myFirstDjango)➜ django-admin startproject mysite项目文件列表------------------mysite/ 项目名称 manage.py 命令行工具 mysite/ 代码目录 __init__.py 模块化申明文件，一般为空 settings.py 配置文件 urls.py 路由配置文件 wsgi.py 服务器文件启动本地开发环境：------------------(myFirstDjango)➜ myFirstDjango cd mysite &amp;&amp; python manage.py runserverPerforming system checks...System check identified no issues (0 silenced).You have 13 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): admin, auth, contenttypes, sessions.Run 'python manage.py migrate' to apply them.October 29, 2016 - 05:47:37Django version 1.10.2, using settings 'mysite.settings'Starting development server at http://127.0.0.1:8000/Quit the server with CONTROL-C.启动前需要用 migrate 做些数据库迁移修复工作：------------------(myFirstDjango)➜ mysite python manage.py migrateOperations to perform: Apply all migrations: admin, auth, contenttypes, sessionsRunning migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying sessions.0001_initial... OK再次启动开发环境：------------------(myFirstDjango)➜ python manage.py runserver 0.0.0.0:8000 浏览器访问： http://127.0.0.1:8000/ 第一步完成了，我们还需要澄清一个细节，注意看这两个命令： django-admin startproject 和 python manage.py startapp polls 这里面的project和app是有区别的： app可以看作是一个完成功能模块，而project可以看作成一个网站，由多个功能模块app组成。关键是模块app可以被多个project直接使用，这点非常重要，DRY万岁。 我们运行python manage.py startapp polls，生成一个新的投票模块polls。可以看到mysite目录下多了一个polls目录。123456789101112mysite/ manage.py mysite/ polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py 接着，我们需要补充路由文件urls.py123456#urls.pyfrom django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^$', views.index, name='index'),] 同样在site目录下也需要补充一个： 1234567#urls.pyfrom django.conf.urls import include, urlfrom django.contrib import adminurlpatterns = [ url(r'^polls/', include('polls.urls')), #包含polls模块的路由 url(r'^admin/', admin.site.urls),] 同时在mysite/settings.py修改：123456789INSTALLED_APPS = [ 'polls.apps.PollsConfig', #引入安装的模块 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles',] 最后增加模版文件mysite/polls/templates/polls/index.html12345678&#123;% if latest_question_list %&#125; &lt;ul&gt;&#123;% for question in latest_question_list %&#125;&lt;li&gt;&lt;a href=\"/polls/&#123;&#123; question.id &#125;&#125;/\"&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; &#123;% else %&#125;&lt;p&gt;暂无投票。&lt;/p&gt; &#123;% endif %&#125; 在mysite目录下，再次启动开发环境：1(myFirstDjango)➜ python manage.py runserver 0.0.0.0:8000 访问http://0.0.0.0:8000/polls/可以看到：1暂无投票。 对，还没有投票内容。我们还需要建立Model制定数据结构，添加投票数据，然后从数据库获取投票数据。 为了讲解方便我们先使用sqlite作为默认的数据库存储数据：12345678#mysite/settings.pyDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', #数据库文件保存的位置 'NAME': os.path.join(BASE_DIR, 'db.sqlite3') &#125;&#125; 首先，编辑polls/models.py，制定Model数据： 1234567891011121314151617from __future__ import unicode_literalsfrom django.db import modelsfrom django.utils.encoding import python_2_unicode_compatible@python_2_unicode_compatibleclass Question(models.Model): question_text = models.CharField(max_length=200) pub_date = models.DateTimeField('date published') def __str__(self): return self.question_text@python_2_unicode_compatibleclass Choice(models.Model): question = models.ForeignKey(Question, on_delete=models.CASCADE) choice_text = models.CharField(max_length=200) votes = models.IntegerField(default=0) def __str__(self): return self.choice_text 然后利用makemigrations工具生成数据库迁移文件polls/migrations/0001_initial.py123456(myFirstDjango)➜ mysite python manage.py makemigrations pollsMigrations for 'polls': polls/migrations/0001_initial.py: - Create model Choice - Create model Question - Add field question to choice 接着执行数据库迁移操作，这里应该包含建表的操作，这样我们就可以通过管理工具添加投票数据了。1234567891011121314151617181920(myFirstDjango)➜ mysite python manage.py sqlmigrate polls 0001BEGIN;---- Create model Choice--CREATE TABLE \"polls_choice\" (\"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT, \"choice_text\" varchar(200) NOT NULL, \"votes\" integer NOT NULL);---- Create model Question--CREATE TABLE \"polls_question\" (\"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT, \"question_text\" varchar(200) NOT NULL, \"pub_date\" datetime NOT NULL);---- Add field question to choice--ALTER TABLE \"polls_choice\" RENAME TO \"polls_choice__old\";CREATE TABLE \"polls_choice\" (\"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT, \"choice_text\" varchar(200) NOT NULL, \"votes\" integer NOT NULL, \"question_id\" integer NOT NULL REFERENCES \"polls_question\" (\"id\"));INSERT INTO \"polls_choice\" (\"choice_text\", \"votes\", \"id\", \"question_id\") SELECT \"choice_text\", \"votes\", \"id\", NULL FROM \"polls_choice__old\";DROP TABLE \"polls_choice__old\";CREATE INDEX \"polls_choice_7aa0f6ee\" ON \"polls_choice\" (\"question_id\");COMMIT;(myFirstDjango)➜ mysite 完成后可以检查下是否迁移有错误发生：12(myFirstDjango)➜ mysite python manage.py checkSystem check identified no issues (0 silenced). 或者直接使用migrate命令执行所有未执行的迁移操作。12345(myFirstDjango)➜ mysite python manage.py migrateOperations to perform: Apply all migrations: admin, auth, contenttypes, polls, sessionsRunning migrations: Applying polls.0001_initial... OK 然后我们生成后台管理界面：123456(myFirstDjango)➜ mysite python manage.py createsuperuserUsername (leave blank to use 'super'): adminEmail address: yourname@gmail.comPassword:Password (again):Superuser created successfully. 在polls/admin.py里面注册可以管理的Model:123from .models import Question, Choiceadmin.site.register(Question)admin.site.register(Choice) 然后访问http://0.0.0.0:8000/admin/ 添加完投票内容后， 修改一下polls的路由：123456app_name = 'polls'urlpatterns = [ url(r'^$', views.index, name='index'), # the 'name' value as called by the &#123;% url %&#125; template tag url(r'^(?P&lt;question_id&gt;[0-9]+)/$', views.detail, name='detail'),] 修改投票主页模版：1&lt;li&gt;&lt;a href=\"&#123;% url 'polls:detail' question.id %&#125;\"&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt; 并且增加投票详细页的模版detail.html：12345&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&lt;ul&gt;&#123;% for choice in question.choice_set.all %&#125;&lt;li&gt;&#123;&#123; choice.choice_text &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt; 访问http://0.0.0.0:8000/polls/1/，最简单的Django投票样例就完成了。1234你会学习python吗？* 会* 不会* 不知道 这是个非常简单的MVC架构，熟悉Angular的同学应该很快就能理解Django的做法，怎么样，Python也不难吧～","categories":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://blog.pinbot.me/tags/Django/"},{"name":"Python","slug":"Python","permalink":"http://blog.pinbot.me/tags/Python/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}]},{"title":"Docker学习之路","slug":"Docker学习之路","date":"2016-10-10T12:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/10/10/Docker学习之路/","link":"","permalink":"http://blog.pinbot.me/2016/10/10/Docker学习之路/","excerpt":"作者：liudong Docker简介Docker是什么？Docker 是一个开源项目,Go 语言实现,遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。Docker 的基础是 Linux 容器（LXC）等技术。 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。","text":"作者：liudong Docker简介Docker是什么？Docker 是一个开源项目,Go 语言实现,遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。Docker 的基础是 Linux 容器（LXC）等技术。 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。 为什么要使用 Docker？首先，Docker 容器的启动可以在秒级实现，这相比传统的虚拟机方式要快得多。 其次，Docker 对系统资源的利用率很高，一台主机上可以同时运行数千个 Docker 容器。容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。 更快速的交付和部署开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。 更高效的虚拟化Docker 容器的运行不需要额外的 hypervisor 支持，它是内核级的虚拟化，因此可以实现更高的性能和效率。 更轻松的迁移和扩展Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 更简单的管理使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。 对比传统虚拟机总结 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为 MB 一般为 GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般几十个 Docker能做什么？Docker可以解决虚拟机能够解决的问题，同时也能够解决虚拟机由于资源要求过高而无法解决的问题。Docker能处理的事情包括：隔离应用依赖创建应用镜像并进行复制创建容易分发的即启即用的应用允许实例简单、快速地扩展测试应用并随后销毁它们 Docker背后的想法是创建软件程序可移植的轻量容器，让其可以在任何安装了Docker的机器上运行，而不用关心底层操作系统基本概念 Docker 镜像 （Image）镜像原理：Docker的镜像类似虚拟机的快照，但更轻量，非常非常轻量。Docker 使用 Union FS 将这些不同的层结合到一个镜像中去。通常 Union FS 有两个用途, 一方面可以实现不借助 LVM、RAID 将多个 disk 挂到同一个目录下,另一个更常用的就是将一个只读的分支和一个可写的分支联合在一起，Live CD 正是基于此方法可以允许在镜像不变的基础上允许用户在其上进行一些写操作；创建Docker镜像有几种方式，多数是在一个现有镜像基础上创建新镜像，因为几乎你需要的任何东西都有了公共镜像，包括所有主流Linux发行版，你应该不会找不到你需要的镜像。不过，就算你想从头构建一个镜像，也有好几种方法。要创建一个镜像，你可以拿一个镜像，对它进行修改来创建它的子镜像。实现前述目的的方式有两种：在一个文件中指定一个基础镜像及需要完成的修改；或通过“运行”一个镜像，对其进行修改并提交。不同方式各有优点，不过一般会使用文件来指定所做的变化。Docker 镜像（Image）就是一个只读的模板，可以用来创建 Docker 容器。 简单命令 （Ubuntu系统）安装Docker12$ sudo apt-get update$ wget -qO- https://get.docker.com/ | sh 注：系统会提示你输入sudo密码，输入完成之后，就会下载脚本并且安装Docker及依赖包。 Docker命令工具需要root权限才能工作。你可以将你的用户放入docker组来避免每次都要使用sudo。1$ sudo docker pull ubuntu:latest 列出docker镜像1$ sudo docker images 上传镜像1$ sudo docker push ouruser/sinatra 保存镜像1$ sudo docker save -o ubuntu_14.04.tar ubuntu:14.04 加载镜像1$ sudo docker load --input ubuntu_14.04.tar # 或者 sudo docker load &lt; ubuntu_14.04.tar 删除镜像12sudo docker rmi training/sinatra注：在删除镜像之前要先用 docker rm 删掉依赖于这个镜像的所有容器. 清理所有未打过标签的本地镜像1$ sudo docker rmi $(docker images -q -f &quot;dangling=true&quot;) #sudo docker rmi $(docker images --quiet --filter &quot;dangling=true&quot;) Dockerfile创建镜像12345678910111213$ vim Dockerfile# This is a commentFROM ubuntu:14.04MAINTAINER Docker Newbee &lt;newbee@docker.com&gt;RUN apt-get -qq updateRUN apt-get -qqy install ruby ruby-devRUN gem install sinatra$ sudo docker build -t=&quot;ouruser/sinatra:v2&quot; .注：其中 -t 标记来添加 tag，指定新的镜像的用户信息。 “.” 是 Dockerfile 所在的路径（当前目录），也可以替换为一个具体的 Dockerfile 的路径。$ sudo docker run -t -i ouruser/sinatra:v2 /bin/bash 从本地文件系统导入1$ sudo cat ubuntu-14.04-x86_64-minimal.tar.gz |docker import - ubuntu:14.04 Docker 容器（Container）Docker 利用容器（Container）来运行应用。 容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 *注：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。 新建并后台启动容器12345$ sudo docker run -tid ubuntu /bin/bash注：-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开，-d 让容器进入后台运行.$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb548b2d4a537 ubuntu &quot;/bin/bash&quot; 11 seconds ago Up 10 seconds zen_engelbart docker run 来创建容器时，Docker 在后台运行的标准操作包括：检查本地是否存在指定的镜像，不存在就从公有仓库下载利用镜像创建并启动一个容器分配一个文件系统，并在只读的镜像层外面挂载一层可读写层从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去从地址池配置一个 ip 地址给容器执行用户指定的应用程序执行完毕后容器被终止 查看docker容器1sudo docker ps 启动已经停止的容器1$ sudo docker start ubuntu:14.04 docker自带命令进入容器12$ sudo docker attach zen_engelbart #zen_engelbart是容器名当退出容器后，容器会关闭$ docker exec -it zen_engelbart /bin/bash #进入已经开启的容器，退出后容器能继续运行 第三方工具进入容器123$ cd /tmp; curl https://www.kernel.org/pub/linux/utils/util-linux/v2.24/util-linux-2.24.tar.gz | tar -zxf-; cd util-linux-2.24;$ ./configure --without-ncurses$ make nsenter &amp;&amp; sudo cp nsenter /usr/local/bin 容器的第一个进程的 PID，可以通过下面的命令获取12$ PID=$(docker inspect --format &quot;&#123;&#123; .State.Pid &#125;&#125;&quot; &lt;container&gt;)$ nsenter --target $PID --mount --uts --ipc --net --pid 实例演示123456789$ sudo docker run -idt ubuntu243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES243c32535da7 ubuntu:latest &quot;/bin/bash&quot; 18 seconds ago Up 17 seconds nostalgic_hypatia$ PID=$(docker-pid 243c32535da7)10981$ sudo nsenter --target 10981 --mount --uts --ipc --net --pidroot@243c32535da7:/# 123456简单的方法是：下载 .bashrc_docker，并将内容放到 .bashrc 中。$ wget -P ~ https://github.com/yeasy/docker_practice/raw/master/_local/.bashrc_docker;$ echo &quot;[ -f ~/.bashrc_docker ] &amp;&amp; . ~/.bashrc_docker&quot; &gt;&gt; ~/.bashrc; source ~/.bashrc$ echo $(docker-pid &lt;container&gt;)$ docker-enter &lt;container&gt; ls 获取容器日志1$ sudo docker logs ubuntu:14.04 导出容器1234$ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9c365aaa875f mysql &quot;docker-entrypoint.sh&quot; 9 days ago Exited 8 minutes ago 0.0.0.0:3308-&gt;3306/tcp mysql_3308$ sudo docker export 9c365aaa875f &gt; mysql.tar 导入容器快照12$ cat mysql.tar | sudo docker import - test/mysql:5.6$sudo docker import http://example.com/exampleimage.tgz example/imagerepo Docker 仓库（Repository）仓库（Repository）是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。*注：Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。 Dockerfile使用指令 指令的一般格式为 INSTRUCTION arguments，指令包括 FROM、MAINTAINER、RUN 等。 FROM格式为 FROM 或FROM :。 第一条指令必须为 FROM 指令。并且，如果在同一个Dockerfile中创建多个镜像时，可以使用多个 FROM 指令（每个镜像一次）。 MAINTAINER格式为 MAINTAINER ，指定维护者信息。 RUN格式为 RUN 或 RUN [“executable”, “param1”, “param2”]。 前者将在 shell 终端中运行命令，即 /bin/sh -c；后者则使用 exec 执行。指定使用其它终端可以通过第二种方式实现，例如 RUN [“/bin/bash”, “-c”, “echo hello”]。 每条 RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用 \\ 来换行。 CMD支持三种格式 CMD [“executable”,”param1”,”param2”] 使用 exec 执行，推荐方式；CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用；CMD [“param1”,”param2”] 提供给 ENTRYPOINT 的默认参数；指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。 如果用户启动容器时候指定了运行的命令，则会覆盖掉 CMD 指定的命令。 EXPOSE格式为 EXPOSE […]。 告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。 ENV格式为 ENV 。 指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。 例如 ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATHADD 格式为 ADD 。 该命令将复制指定的 到容器中的 。 其中 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录）。 COPY格式为 COPY 。 复制本地主机的 （为 Dockerfile 所在目录的相对路径）到容器中的 。 当使用本地目录为源目录时，推荐使用 COPY。 ENTRYPOINT两种格式： ENTRYPOINT [“executable”, “param1”, “param2”]ENTRYPOINT command param1 param2（shell中执行）。配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。 每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。 VOLUME格式为 VOLUME [“/data”]。 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。 USER格式为 USER daemon。 指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。 当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如：RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres。要临时获取管理员权限可以使用 gosu，而不推荐 sudo。 WORKDIR格式为 WORKDIR /path/to/workdir。 为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。 可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如 WORKDIR /aWORKDIR bWORKDIR cRUN pwd则最终路径为 /a/b/c。 ONBUILD格式为 ONBUILD [INSTRUCTION]。 配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。 例如，Dockerfile 使用如下的内容创建了镜像 image-A。 […]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build –dir /app/src[…]如果基于 image-A 创建新的镜像时，新的Dockerfile中使用 FROM image-A指定基础镜像时，会自动执行 ONBUILD 指令内容，等价于在后面添加了两条指令。 FROM image-A1234#Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src注：使用 ONBUILD 指令的镜像，推荐在标签中注明 实例演示(dockerfile创建镜像，运行Django+uwsgi+nginx+supervisor)启动mysql容器12sudo docker run -d -e MYSQL_ROOT_PASSWORD=pinbot@123 --name mysql_3308 -v /data/mysql/data:/var/lib/mysql -p 3308:3306 mysql注：用mysql镜像后台启动容器，并设置root用户初始密码为谁pinbot123，挂载本地目录/data/mysql/data到容器mysql_3308 的/var/lib/mysql目录，映射本地3308端口到容器的3306端口 用Dockerfile创建镜像12sudo docker build -t talentbi:1.0 .注：根据Dockerfile创建镜像，并命名为talentbi:1.0； 后台启动容器12sudo docker run -d -p 8001:8080 -v /home/bigdata/github/TalentMiner/:/home/bigdata/github/TalentMiner --name talentbi1.0 talentbi:1.0注：用talentbi:1.0镜像启动容器并后台运行，映射本地端口8001到容器内8080端口，挂载本地目录等 进入容器1sudo docker exec -ti talentbi1.1 /bin/bash 端口映射1sudo iptables -t nat -A DOCKER -p tcp --dport 8080 -j DNAT --to-destination 172.17.0.3:8080 查看iptables列表1sudo iptables -t nat -nL","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://blog.pinbot.me/tags/运维/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"JavaScript之设计模式(单例模式,构造函数模式)","slug":"JavaScript之设计模型(单例模式,构造函数模式)","date":"2016-10-10T06:08:08.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/10/10/JavaScript之设计模型(单例模式,构造函数模式)/","link":"","permalink":"http://blog.pinbot.me/2016/10/10/JavaScript之设计模型(单例模式,构造函数模式)/","excerpt":"作者: 李纯利 一、设计模式之单例模式单例模式就是保证一个类只有一个实例，实现的方法一般是先判断实例存在与否，如果存在直接返回，如果不存在就创建了再返回，这就确保了一个类只有一个实例对象。","text":"作者: 李纯利 一、设计模式之单例模式单例模式就是保证一个类只有一个实例，实现的方法一般是先判断实例存在与否，如果存在直接返回，如果不存在就创建了再返回，这就确保了一个类只有一个实例对象。下面我们来看一个单例的最佳实践： 1234567891011121314151617181920212223242526272829var SingletonTester = (function () &#123; //参数：传递给单例的一个参数集合 function Singleton(args) &#123; //设置args变量为接收的参数或者为空（如果没有提供的话） var args = args || &#123;&#125;; //设置name参数 this.name = 'SingletonTester'; //设置pointX的值 this.pointX = args.pointX || 6; //从接收的参数里获取，或者设置为默认值 //设置pointY的值 this.pointY = args.pointY || 10; &#125; //实例容器 var instance; var _static = &#123; name: 'SingletonTester', //获取实例的方法 //返回Singleton的实例 getInstance: function (args) &#123; if (instance === undefined) &#123; instance = new Singleton(args); &#125; return instance; &#125; &#125;; return _static;&#125;)();var singletonTest = SingletonTester.getInstance(&#123; pointX: 5 &#125;);console.log(singletonTest.pointX); // 输出 5 二、设计模式之构造函数模式介绍：构造函数用于创建特定类型的对象——不仅声明了使用的对象，构造函数还可以接受参数以便第一次创建对象的时候设置对象的成员值。你可以自定义自己的构造函数，然后在里面声明自定义类型对象的属性或方法。 基本用法：JavaScript没有类的概念，但是有特殊的构造函数。通过new关键字来调用定义的否早函数，你可以告诉JavaScript你要创建一个新对象并且新对象的成员声明都是构造函数里定义的。在构造函数内部，this关键字引用的是新创建的对象。基本用法如下： 1234567891011121314function Car(model, year, names) &#123; this.model = model; this.year = year; this.names = names; this.output= function () &#123; return this.model + \"喜欢\" + this.names; &#125;;&#125;var tom= new Car(\"大叔\", 30, '萝莉');var dudu= new Car(\"欧巴\", 24, '御姐');console.log(tom.output());console.log(dudu.output()); 上面是一个非常简单的构造函数模式，但是使用继承就很麻烦了，而且output()在每次创建对象的时候都重新定义了，最好的方法是让所有Car类型的实例都共享这个output()方法，这样如果有大批量的实例的话，就会节约很多内存。解决方法如下：12345678910function Car(model, year, names) &#123; this.model = model; this.year = year; this.names = names; this.output= formatCar;&#125;function formatCar() &#123; return this.model + \"喜欢\" + this.names;&#125; 这个方法虽然可用,但是我们还有更好的办法哟! 构造函数与原型JavaScript里函数有个原型属性叫prototype，当调用构造函数创建对象的时候，所有该构造函数原型的属性在新创建对象上都可用.下面来看一下上面扩展的代码: 123456789101112131415161718function Car(model, year, names) &#123; this.model = model; this.year = year; this.names = names;&#125;/*注意：这里我们使用了Object.prototype.方法名，而不是Object.prototype主要是用来避免重写定义原型prototype对象*/Car.prototype.output= function () &#123; return this.model + \"喜欢\" + this.names;&#125;;var tom = new Car(\"大叔\", 33, '萝莉');var dudu = new Car(\"欧巴\", 25, '御姐');console.log(tom.output());console.log(dudu.output()); 这里，output()单实例可以在所有Car对象实例里共享使用。另外：我们推荐构造函数以大写字母开头，以便区分普通的函数。 强制使用new如果不是new来创建对象,直接用在全局调用函数的话,this指向的是全局对象window,下面来验证一下: 1234//作为函数调用var tom = Car(\"大叔\", 30, '萝莉');console.log(typeof tom); // \"undefined\"console.log(window.output()); // \"大叔喜欢萝莉\" 这个时候的tom是undefined,而window.output()会正确输出结果,而如果使用new关键字则没有这个问题,验证如下: 1234//使用new 关键字var tom = new Car(\"大叔\", 30, '萝莉');console.log(typeof tom); // \"object\"console.log(tom.output()); // \"大叔喜欢萝莉\" 上述的例子展示了不使用new的问题，那么我们有没有办法让构造函数强制使用new关键字呢，答案是肯定的，上代码：12345678910111213141516171819function Car(model, year, names) &#123; if (!(this instanceof Car)) &#123; return new Car(model, year, names); &#125; this.model = model; this.year = year; this.names = names; this.output = function () &#123; return this.model + \"喜欢\" + this.names; &#125;&#125;var tom = new Car(\"大叔\", 32, '萝莉');var dudu = Car(\"欧巴\", 25, '御姐');console.log(typeof tom); // \"object\"console.log(tom.output()); // \"大叔喜欢萝莉\"console.log(typeof dudu); // \"object\"console.log(dudu.output()); // \"欧巴喜欢御姐\" 通过判断this的instanceof是不是Car来决定返回new Car还是继续执行代码，如果使用的是new关键字，则(this instanceof Car)为真，会继续执行下面的参数赋值，如果没有用new，(this instanceof Car)就为假，就会重新new一个实例返回。","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://blog.pinbot.me/tags/JavaScript/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"react.js components的生命周期","slug":"react.js components的生命周期","date":"2016-10-09T16:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/10/10/react.js components的生命周期/","link":"","permalink":"http://blog.pinbot.me/2016/10/10/react.js components的生命周期/","excerpt":"react.js components的生命周期React提供了和以往不一样的方式来看待视图，它以组件开发为基础。组件是React的核心概念，React 允许将代码封装成组件（component），然后像插入普通 HTML 标签一样，在网页中插入这个组件。React.createClass 方法就用于生成一个组件类。对React应用而言，你需要分割你的页面，使其成为一个个的组件。也就是说，你的应用是由这些组件组合而成的。你可以通过分割组件的方式去开发复杂的页面或某个功能区块，组件是可以被复用的。 之前我们简单介绍了react的由来、特点、应用场景。以及，jsx语法糖，使用React.createClass生成自定义标签，插入节点，添加css样式，这些都是react的基础，接下来，我们继续react compenents的生命周期。","text":"react.js components的生命周期React提供了和以往不一样的方式来看待视图，它以组件开发为基础。组件是React的核心概念，React 允许将代码封装成组件（component），然后像插入普通 HTML 标签一样，在网页中插入这个组件。React.createClass 方法就用于生成一个组件类。对React应用而言，你需要分割你的页面，使其成为一个个的组件。也就是说，你的应用是由这些组件组合而成的。你可以通过分割组件的方式去开发复杂的页面或某个功能区块，组件是可以被复用的。 之前我们简单介绍了react的由来、特点、应用场景。以及，jsx语法糖，使用React.createClass生成自定义标签，插入节点，添加css样式，这些都是react的基础，接下来，我们继续react compenents的生命周期。组件的生命周期分成三个状态： Mounting：已插入真实 DOM，即Initial Render Updating：正在被重新渲染，即Props与State改变 Unmounting：已移出真实 DOM，即Component Unmount React 为每个状态都提供了两种处理函数，will 函数在进入状态之前调用，did 函数在进入状态之后调用，三种状态共计五种处理函数。 componentWillMount() componentDidMount() componentWillUpdate(object nextProps, object nextState) componentDidUpdate(object prevProps, object prevState) componentWillUnmount()此外，React 还提供两种特殊状态的处理函数。 componentWillReceiveProps(object nextProps)：已加载组件收到新的参数时调用 shouldComponentUpdate(object nextProps, object nextState)：组件判断是否重新渲染时调用 Mounting阶段： componentWillMount—render—componentDidMountUpdating阶段： componentWillReceiveProps—shouldCOmponentUpdate—componentWillUpdate—render—componentDidUpdateUnmounting阶段： coponentWillUnmount 一个完整的React Component的写法应该如下： var myComponent = React.createClass({ // The object returned by this method sets the initial value of this.state getInitialState: function(){ return {}; }, // The object returned by this method sets the initial value of this.props // If a complex object is returned, it is shared among all component instances getDefaultProps: function(){ return {}; }, // Returns the jsx markup for a component // Inspects this.state and this.props create the markup // Should never update this.state or this.props render: function(){ return (&lt;div&gt;&lt;/div&gt;); }, // An array of objects each of which can augment the lifecycle methods mixins: [], // Functions that can be invoked on the component without creating instances statics: { aStaticFunction: function(){} }, // -- Lifecycle Methods -- // Invoked once before first render componentWillMount: function(){ // Calling setState here does not cause a re-render }, // Invoked once after the first render componentDidMount: function(){ // You now have access to this.getDOMNode() }, // Invoked whenever there is a prop change // Called BEFORE render componentWillReceiveProps: function(nextProps){ // Not called for the initial render // Previous props can be accessed by this.props // Calling setState here does not trigger an an additional re-render }, // Determines if the render method should run in the subsequent step // Called BEFORE a render // Not called for the initial render shouldComponentUpdate: function(nextProps, nextState){ // If you want the render method to execute in the next step // return true, else return false return true; }, // Called IMMEDIATELY BEFORE a render componentWillUpdate: function(nextProps, nextState){ // You cannot use this.setState() in this method }, // Called IMMEDIATELY AFTER a render componentDidUpdate: function(prevProps, prevState){ }, // Called IMMEDIATELY before a component is unmounted componentWillUnmount: function(){ } });","categories":[{"name":"react","slug":"react","permalink":"http://blog.pinbot.me/categories/react/"}],"tags":[{"name":"react","slug":"react","permalink":"http://blog.pinbot.me/tags/react/"}],"keywords":[{"name":"react","slug":"react","permalink":"http://blog.pinbot.me/categories/react/"}]},{"title":"NotImplemented","slug":"NotImplemented","date":"2016-10-09T08:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/10/09/NotImplemented/","link":"","permalink":"http://blog.pinbot.me/2016/10/09/NotImplemented/","excerpt":"在创建基类时常常会用到raise NotImplementedError这个语句, 但在写下这条语句时IDE可能会补全一个NotImplemented出来, NotImplemented是什么?","text":"在创建基类时常常会用到raise NotImplementedError这个语句, 但在写下这条语句时IDE可能会补全一个NotImplemented出来, NotImplemented是什么? NotImplemented是什么首先NotImplemented并不是一种异常, 而是Built-in的一种类型: 12&gt;&gt;&gt; type(NotImplemented)&lt;type 'NotImplementedType'&gt; 官方文档中是这么描述的: Special value which can be returned by the “rich comparison” special methods (__eq__(), __lt__(), and friends), to indicate that the comparison is not implemented with respect to the other type. NotImplemented的具体应用根据文档描述, NotImplemented常用在object.__eq__这样的比较方法中。 在下面的例子中, 比较Pants和Socks对象时, 首先会调用Pants的__eq__方法, 返回的是NotImplemented则转而调用Socks的__eq__方法。 使用NotImplemented而不是抛出异常, 给了其它对象扩展的机会。 12345678910111213141516171819class Entity(object): def __init__(self, size): self.size = sizeclass Pants(Entity): def __eq__(self, other): return NotImplementedclass Socks(Entity): def __eq__(self, other): if not isinstance(other, self.__class__): return False return self.size == other.sizeif __name__ == '__main__': print Pants(5) == Socks(5) 文末留一个小问题: 123class Foo(object): def __lt__(self, other): return NotImplemented Foo() &lt; Foo()有输出吗? 如果有, 是什么？","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.pinbot.me/tags/Python/"},{"name":"NotImplemented","slug":"NotImplemented","permalink":"http://blog.pinbot.me/tags/NotImplemented/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"Nodejs ORM框架Sequelizejs快速入门","slug":"Nodejs-ORM框架Sequelizejs快速入门","date":"2016-09-12T15:10:10.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/09/12/Nodejs-ORM框架Sequelizejs快速入门/","link":"","permalink":"http://blog.pinbot.me/2016/09/12/Nodejs-ORM框架Sequelizejs快速入门/","excerpt":"作者: Adam 什么是ORM？简单的讲就是对SQL查询语句的封装，让我们可以用OOP的方式操作数据库，优雅的生成安全、可维护的SQL代码。直观上，是一种Model和SQL的映射关系。","text":"作者: Adam 什么是ORM？简单的讲就是对SQL查询语句的封装，让我们可以用OOP的方式操作数据库，优雅的生成安全、可维护的SQL代码。直观上，是一种Model和SQL的映射关系。 12345678910111213141516const User = sequelize.define('user', &#123; id: &#123; type: Sequelize.INTEGER, allowNull: false, autoIncrement: true, primaryKey: true &#125;, email: &#123; type: Sequelize.STRING, allowNull: false, validate: &#123; isEmail: true &#125;, unique: true &#125;&#125;) 1234567CREATE TABLE IF NOT EXISTS `users` ( `id` INTEGER PRIMARY KEY AUTOINCREMENT, `email` VARCHAR(255) NOT NULL UNIQUE `createdAt` DATETIME NOT NULL, `updatedAt` DATETIME NOT NULL, UNIQUE (email)); 那么什么是Sequelize？Sequelize是一款基于Nodejs功能强大的异步ORM框架。同时支持PostgreSQL, MySQL, SQLite and MSSQL多种数据库，很适合作为Nodejs后端数据库的存储接口，为快速开发Nodejs应用奠定扎实、安全的基础。 既然Nodejs的强项在于异步，没有理由不找一个强大的支持异步的数据库框架，与之配合，双剑合并～ 123456789101112131415161718192021222324252627282930313233343536//引入框架var Sequelize = require('sequelize');//初始化链接（支持连接池）var sequelize = new Sequelize('database', 'username', 'password', &#123; host: 'localhost', dialect: 'mysql'|'sqlite'|'postgres'|'mssql', pool: &#123; max: 5, min: 0, idle: 10000 &#125;, // SQLite only storage: 'path/to/database.sqlite'&#125;);//定义数据模型var User = sequelize.define('user', &#123; username: Sequelize.STRING, birthday: Sequelize.DATE&#125;);//初始化数据sequelize.sync().then(function() &#123; return User.create(&#123; username: 'janedoe', birthday: new Date(1980, 6, 20) &#125;);&#125;).then(function(jane) &#123; //获取数据 console.log(jane.get(&#123; plain: true &#125;));&#125;).catch(function (err) &#123; //异常捕获 console.log('Unable to connect to the database:', err);&#125;); Sequelize有哪些特色？ 强大的模型定义，支持虚拟类型。Javascript虽然被很多人诟病杂乱无章法，但是函数即对象这个特色，可以说是我的最爱，非常灵活强大。 123456789101112131415var Foo = sequelize.define('foo', &#123; firstname: Sequelize.STRING, lastname: Sequelize.STRING&#125;, &#123; getterMethods : &#123; fullName : function() &#123; return this.firstname + ' ' + this.lastname &#125; &#125;, setterMethods : &#123; fullName : function(value) &#123; var names = value.split(' '); this.setDataValue('firstname', names.slice(0, -1).join(' ')); this.setDataValue('lastname', names.slice(-1).join(' ')); &#125;, &#125;&#125;); 支持完善的数据验证，减轻前后端的验证压力。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647var ValidateMe = sequelize.define('foo', &#123; foo: &#123; type: Sequelize.STRING, validate: &#123; is: [\"^[a-z]+$\",'i'], // 全匹配字母 is: /^[a-z]+$/i, // 全匹配字母，用规则表达式写法 not: [\"[a-z]\",'i'], // 不能包含字母 isEmail: true, // 检查邮件格式 isUrl: true, // 是否是合法网址 isIP: true, // 是否是合法IP地址 isIPv4: true, // 是否是合法IPv4地址 isIPv6: true, // 是否是合法IPv6地址 isAlpha: true, // 是否是字母 isAlphanumeric: true, // 是否是数字和字母 isNumeric: true, // 只允许数字 isInt: true, // 只允许整数 isFloat: true, // 是否是浮点数 isDecimal: true, // 是否是十进制书 isLowercase: true, // 是否是小写 isUppercase: true, // 是否大写 notNull: true, // 不允许为null isNull: true, // 是否是null notEmpty: true, // 不允许为空 equals: 'specific value', // 等于某些值 contains: 'foo', // 包含某些字符 notIn: [['foo', 'bar']], // 不在列表中 isIn: [['foo', 'bar']], // 在列表中 notContains: 'bar', // 不包含 len: [2,10], // 长度范围 isUUID: 4, // 是否是合法 uuids isDate: true, // 是否是有效日期 isAfter: \"2011-11-05\", // 是否晚于某个日期 isBefore: \"2011-11-05\", // 是否早于某个日期 max: 23, // 最大值 min: 23, // 最小值 isArray: true, // 是否是数组 isCreditCard: true, // 是否是有效信用卡号 // 自定义规则 isEven: function(value) &#123; if(parseInt(value) % 2 != 0) &#123; throw new Error('请输入偶数!') &#125; &#125; &#125; &#125;&#125;); Sequelize的查询非常全面和灵活 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273Project.findAll(&#123; //复杂过滤，可嵌套 where: &#123; id: &#123; $and: &#123;a: 5&#125; // AND (a = 5) $or: [&#123;a: 5&#125;, &#123;a: 6&#125;] // (a = 5 OR a = 6) $gt: 6, // id &gt; 6 $gte: 6, // id &gt;= 6 $lt: 10, // id &lt; 10 $lte: 10, // id &lt;= 10 $ne: 20, // id != 20 $between: [6, 10], // BETWEEN 6 AND 10 $notBetween: [11, 15], // NOT BETWEEN 11 AND 15 $in: [1, 2], // IN [1, 2] $notIn: [1, 2], // NOT IN [1, 2] $like: '%hat', // LIKE '%hat' $notLike: '%hat' // NOT LIKE '%hat' $iLike: '%hat' // ILIKE '%hat' (case insensitive) (PG only) $notILike: '%hat' // NOT ILIKE '%hat' (PG only) $overlap: [1, 2] // &amp;&amp; [1, 2] (PG array overlap operator) $contains: [1, 2] // @&gt; [1, 2] (PG array contains operator) $contained: [1, 2] // &lt;@ [1, 2] (PG array contained by operator) $any: [2,3] // ANY ARRAY[2, 3]::INTEGER (PG only) &#125;, status: &#123; $not: false, // status NOT FALSE &#125; &#125;&#125;)Project.all()Project.findByIdProject.findByOneProject.findOrCreateProject.findAndCountAllProject.count()Project.max()//CRUDProject.create()Project.save()Project.update()Project.destroy()//批量User.bulkCreate([])//排序something.findOne(&#123; order: [ 'name', // 返回 `name` 'username DESC', // 返回 `username DESC` ['username', 'DESC'], // 返回 `username` DESC sequelize.fn('max', sequelize.col('age')), // 返回 max(`age`) [sequelize.fn('max', sequelize.col('age')), 'DESC'], // 返回 max(`age`) DESC [sequelize.fn('otherfunction', sequelize.col('col1'), 12, 'lalala'), 'DESC'], // 返回 otherfunction(`col1`, 12, 'lalala') DESC [sequelize.fn('otherfunction', sequelize.fn('awesomefunction', sequelize.col('col'))), 'DESC'] // 返回 otherfunction(awesomefunction(`col`)) DESC, 有可能是无限循环 [&#123; raw: 'otherfunction(awesomefunction(`col`))' &#125;, 'DESC'] // 也可以这样写 ]&#125;)// 分页查询Project.findAll(&#123; limit: 10 &#125;)Project.findAll(&#123; offset: 8 &#125;)Project.findAll(&#123; offset: 5, limit: 5 &#125;) 关联查询 include 支持嵌套，这可能是ORM里面最难的部分。 12345678910111213141516171819202122232425262728293031323334353637383940var User = sequelize.define('user', &#123; name: Sequelize.STRING &#125;) , Task = sequelize.define('task', &#123; name: Sequelize.STRING &#125;) , Tool = sequelize.define('tool', &#123; name: Sequelize.STRING &#125;)Task.belongsTo(User) // 增加外键属性 UserId 到 TaskUser.hasMany(Task) // 给 Task 增加外键属性 userIdUser.hasMany(Tool, &#123; as: 'Instruments' &#125;) // 给 Task 增加自定义外键属性 InstrumentsIdTask.findAll(&#123; include: [ User ] &#125;)User.findAll(&#123; include: [&#123; model: Tool, as: 'Instruments', where: &#123; name: &#123; $like: '%ooth%' &#125; &#125;&#125;] &#125;)User.findAll(&#123; include: ['Instruments'] &#125;)var User = this.sequelize.define('user', &#123;/* attributes */&#125;, &#123;underscored: true&#125;) , Company = this.sequelize.define('company', &#123; uuid: &#123; type: Sequelize.UUID, primaryKey: true &#125; &#125;);User.belongsTo(Company); // 增加 company_uuid 外键属性到 userUser.belongsTo(UserRole, &#123;as: 'role'&#125;);// 自定义外键属性 roleId 到 user 而不是 userRoleIdUser.belongsTo(Company, &#123;foreignKey: 'fk_companyname', targetKey: 'name'&#125;); // 增加自定义外键属性 fk_companyname 到 UserPerson.hasOne(Person, &#123;as: 'Father', foreignKey: 'DadId'&#125;)// Person 增加外键属性 DadIdCoach.hasOne(Team) // `coachId` 作为 Team 的外键属性Project.hasMany(User, &#123;as: 'Workers'&#125;)// 给 User 增加外键属性 projectId ／ project_idProject.belongsToMany(User, &#123;through: 'UserProject'&#125;);User.belongsToMany(Project, &#123;through: 'UserProject'&#125;);// 创建新的模型: UserProject 包含外键属性：projectId 和 userId Sequelize还有完善的迁移同步数据方案,migrate so easy。 12345678910//$ sequelize db:migrate //用命令直接生成模版脚本，接下来的还是写jsmodule.exports = &#123; up: function(queryInterface, Sequelize) &#123; // 需要修改数据库的操作 &#125;, down: function(queryInterface, Sequelize) &#123; // 取消修改的操作 &#125;&#125; 好的，快餐吃到这里，希望大家喜欢nodejs，能够快速开发Node App～","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"ORM","slug":"ORM","permalink":"http://blog.pinbot.me/tags/ORM/"},{"name":"Nodejs","slug":"Nodejs","permalink":"http://blog.pinbot.me/tags/Nodejs/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"OpenVPN服务部署使用","slug":"OpenVPN服务部署使用","date":"2016-09-08T12:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/09/08/OpenVPN服务部署使用/","link":"","permalink":"http://blog.pinbot.me/2016/09/08/OpenVPN服务部署使用/","excerpt":"作者：liudong at 2016-09-08 20:00:00 OpenVPN服务部署使用 1 服务端部署（Ubuntu）1.1 安装OpenVPN 所需插件1234$ sudo apt-get install openssl$ sudo apt-get install libssl-dev$ sudo apt-get install libpam0g-dev$ sudo apt-get install liblzo2-dev","text":"作者：liudong at 2016-09-08 20:00:00 OpenVPN服务部署使用 1 服务端部署（Ubuntu）1.1 安装OpenVPN 所需插件1234$ sudo apt-get install openssl$ sudo apt-get install libssl-dev$ sudo apt-get install libpam0g-dev$ sudo apt-get install liblzo2-dev 1.2 安装OpenVPN注:以下安装方式任选一种,推荐apt-get方式安装 1.2.1 apt-get安装OpenVPN123$apt-get install openvpn$cd /etc/openvpn$mkdir conf log 1.2.2 源码安装OpenVPN（建议使用2.2.2版本）12345$ wget http://swupdate.openvpn.org/community/releases/openvpn-2.2.2.tar.gz $ tar -zxvf openvpn-2.2.2.tar.gz $ mkdir /data/openvpn &amp;&amp; cd openvpn-2.2.2 $ ./configure --enable-password-save --prefix=/etc/openvpn $ make &amp;&amp; sudo make install 注：–enable-password-save该选项是避免手工输入客户端密码；–prefix选项是真正的安装路径 1.3 开启内核转发并配置源地址路由12$ echo &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward $ iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE 1.4 服务端配置1.4.1 生成密钥123456789$ cd openvpn-2.2.2/easy-rsa/2.0 $ source ./vars # 在此之前，可以修改vars文件对国家省份等修改;配置dh的位数(默认是1024，可以改成export KEY_SIZE=2048)和下文生成的dh2048.pem相对应$ ./clean-all $ ./build-ca $ ./build-key-server server # 产生服务器证书，此处的server是文件名参数，可以任意修改。$ ./build-key-pass client1 # 生成客户端key，pass表示需要输入一个密码作为客户端启动时的凭证； ./build-key则不需要输入密码$ ./build-dh # 产生Diffie Hellman参数至此一个客户端所需的证书已经完毕，都在easy-rsa/2.0/keys文件夹下面，其中ca.crt server.crt server.csr server.key dh1024.pem是服务端所需证书文件，ca.crt ca.key client1.crt client1.csr client1.key是客户端所需证书文件。注：可以继续使用./build-key产生更多客户端证书,一个客户端证书只能同时用于一个客户端连接。 1.4.2 服务端配目录及文件123$ cd openvpn &amp;&amp; mkdir conf # openvpn就是第2步中openvpn的安装目录 $ cp openvpn-2.2.2/sample-config-files/server.conf conf/$ cp openvpn-2.2.2/easy-rsa/2.0/keys/&#123;ca.crt,server.crt,server.csr,server.key,dh1024.pem&#125; conf/ # 拷贝openvpn-2.2.2/easy-rsa/2.0/keys/下的相关证书文件到openvpn/conf/目录下，注意:2048位的key则是dh2048.pem; 1024位的key则是dh1024.pem 1.4.3 服务端配置文件参数指定123456789101112131415161718192021$ vim conf/server.confdev tapproto tcpport 1194ca /path/to/openvpn/conf/ca.crtcert /path/to/openvpn/conf/server.crtkey /path/to/openvpn/conf/server.keydh /path/to/openvpn/conf/dh1024.pemuser nobodygroup nogroupserver 10.8.0.0 255.255.255.0 # 分配给clinet的ip段second time periodkeepalive 10 120 # 每10秒ping一次，120秒内客户端没有动作就断开连接persist-keypersist-tunverb 4log-append /path/to/openvpn/log/openvpn.logstatus /path/to/openvpn/log/openvpn-status.logclient-to-clientcrl-verify /path/to/openvpn/conf/crl.pem # 客户端证书连接限制comp-lzo 1.5 启动OpenVPN服务端1sudo /path/to/openvpn/sbin/openvpn --config /path/to/openvpn/conf/server.conf --daemon 1.6 检查验证12$ ifconfig|grep inet|grep 10.8.0.1 inet 10.8.0.1 netmask 0xffffff00 broadcast 10.8.0.255 注：得到IP为：10.8.0.1 则说明VPN服务端配置成功 1.7 设置OpenVPN服务端开机启动123$ vim /etc/rc.localiptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE/path/to/openvpn/sbin/openvpn --config /path/to/openvpn/conf/server.conf --daemon 2 OpenVPN 客户端部署(MAC系统)2.1 安装OpenVPN 所需插件1$ sudo brew install openssl 2.2 安装OpenVPN注:以下安装方式任选一种,推荐Brew方式安装 2.2.1 Brew 安装12345直接brew安装(推荐)$brew install openvpn$cd /usr/local/Cellar/openvpn/2.3.11_1$mkdir conf log$ln -s /usr/local/Cellar/openvpn/2.3.11_1/sbin/openvpn /usr/local/bin/openvpn 2.2.2 源码安装（建议使用2.2.2版本）12345$ wget http://swupdate.openvpn.org/community/releases/openvpn-2.2.2.tar.gz $ tar -zxvf openvpn-2.2.2.tar.gz $ mkdir /data/openvpn &amp;&amp; cd openvpn-2.2.2 $ ./configure --enable-password-save --prefix=/etc/openvpn $ make &amp;&amp; sudo make install 注：–enable-password-save该选项是避免手工输入客户端密码；–prefix选项是真正的安装路径 2.3 服务端生成客户端所需密钥(客户端部署可忽略此步骤)2.3.1 服务端连接1234服务端所在机器：xxx.xxx.xxx.xxx $ssh root@xxx.xxx.xxx.xxx #连接方式$cd /media/openvpn/ 服务端所在路径$cd /media/openvpn-2.2.2/easy-rsa/2.0 生成密钥所需路径 2.3.2 生成密钥12$ source ./vars $ ./build-key-pass client-A #此处设置密码为：openvpn123 注：生成客户端key，pass表示需要输入一个密码作为客户端启动时的凭证； ./build-key则不需要输入密码1$ ./build-dh 2.3.3设置客户端密钥验证信息12$ vim /media/openvpn/conf/ccd/client-A ifconfig-push 10.8.0.119 255.255.255.0 注：此处的验证信息文件名需要和生成密钥时输入的名字保持一致;10.8.0.119 指客户端被虚拟出来的IP 2.4 客户端配置2.4.1 拷贝密钥到客户端12$scp root@xxx.xxx.xxx.xxx:/media/openvpn-2.2.2/easy-rsa/2.0/keys/&#123;ca.crt,ca.key,client-A.crt,client-A.csr,client-A.key&#125; /usr/local/Cellar/openvpn/2.3.11_1/conf/注:密钥可以由维护人员发放,联系刘东; 2.4.2 配置客户端密码文件12$ vim /usr/local/Cellar/openvpn/2.3.11_1/conf/password.txt openvpn123 注:客户端密码文件和服务端生成密钥时输入的密码一致 2.4.3 客户端配置文件1234567891011121314151617181920212223$ vim /usr/local/Cellar/openvpn/2.3.11_1/conf/client.conf client dev tap proto tcp remote xxx.xxx.xxx.xxx 1194 #指定服务端外网IP及端口nobind user nobody group nogroup ca /usr/local/Cellar/openvpn/2.3.11_1/conf/ca.crt cert /usr/local/Cellar/openvpn/2.3.11_1/conf/client-A.crt key /usr/local/Cellar/openvpn/2.3.11_1/conf/client-A.key ping 15 ping-restart 45 ping-timer-rem persist-key persist-tun ns-cert-type server comp-lzo verb 4 log-append /usr/local/Cellar/openvpn/2.3.11_1/log/openvpn.log status /usr/local/Cellar/openvpn/2.3.11_1/log/openvpn-status.log tcp-queue-limit 4096 # 256 bcast-buffers 4096 2.5 启动客户端2.5.1 命令行启动OpenVPN1$ sudo /usr/local/bin/openvpn --config /usr/local/Cellar/openvpn/2.3.11_1/conf/client.conf --askpass /usr/local/Cellar/openvpn/2.3.11_1/conf/password.txt --daemon 2.5.2 GUI启动OpenVPN 下载Tunnelblick客户端直接官网下载: https://tunnelblick.net/downloads.html 安装Tunnelblick客户端Tunnelblick具体安装使用流程见：Mac系统Tunnelblick下载以及安装流程 2.6 检查验证12$ ifconfig|grep inet|grep 10.8.0.119 inet 10.8.0.119 netmask 0xffffff00 broadcast 10.8.0.255 注：得到IP为：10.8.0.x 则说明VPN客户端配置成功1$ ping 10.8.0.1 #检查是否能ping通内网等机器 2.7 服务加入开机自启动12vim /etc/rc.local/usr/local/bin/openvpn --config /usr/local/Cellar/openvpn/2.3.11_1/conf/client.conf --askpass /usr/local/Cellar/openvpn/2.3.11_1/conf/password.txt --daemon 3 OpenVPN 客户端部署(Ubuntu系统)3.1 安装OpenVPN 所需插件1234$ sudo apt-get install openssl$ sudo apt-get install libssl-dev$ sudo apt-get install libpam0g-dev$ sudo apt-get install liblzo2-dev 3.2 安装OpenVPN注:以下安装方式任选一种,推荐apt-get方式安装 3.2.1 apt-get安装OpenVPN123$apt-get install openvpn$cd /etc/openvpn$mkdir conf log 3.2.2 源码安装OpenVPN（建议使用2.2.2版本）12345$ wget http://swupdate.openvpn.org/community/releases/openvpn-2.2.2.tar.gz $ tar -zxvf openvpn-2.2.2.tar.gz $ mkdir /data/openvpn &amp;&amp; cd openvpn-2.2.2 $ ./configure --enable-password-save --prefix=/etc/openvpn $ make &amp;&amp; sudo make install 注：–enable-password-save该选项是避免手工输入客户端密码；–prefix选项是真正的安装路径 3.3 服务端生成客户端所需密钥(客户端部署可忽略此步骤)3.3.1 服务端连接1234服务端所在机器：xxx.xxx.xxx.xxx $ssh root@xxx.xxx.xxx.xxx #连接方式$cd /media/openvpn/ 服务端所在路径$cd /media/openvpn-2.2.2/easy-rsa/2.0 生成密钥所需路径 3.3.2 生成密钥12$ source ./vars $ ./build-key-pass client-B #此处设置密码为：openvpn123 注：生成客户端key，pass表示需要输入一个密码作为客户端启动时的凭证； ./build-key则不需要输入密码1$ ./build-dh 3.3.3设置客户端密钥验证信息12$ vim /media/openvpn/conf/ccd/client-B ifconfig-push 10.8.0.120 255.255.255.0 注：此处的验证信息文件名需要和生成密钥时输入的名字保持一致;10.8.0.120 指客户端被虚拟出来的IP 3.4 客户端配置3.4.1 拷贝密钥到客户端12$scp root@xxx.xxx.xxx.xxx:/media/openvpn-2.2.2/easy-rsa/2.0/keys/&#123;ca.crt,ca.key,client-B.crt,client-B.csr,client-B.key&#125; /etc/openvpn/conf注:密钥可以由维护人员发放,联系刘东; 3.4.2 配置客户端密码文件12$ vim /etc/openvpn/conf/password.txt openvpn123 注:客户端密码文件和服务端生成密钥时输入的密码一致 3.4.3 客户端配置文件12345678910111213141516171819202122232425262728$ vim /etc/openvpn/conf/client.conf client dev tap proto tcp remote xxx.xxx.xxx.xxx 1194 #指定服务端外网IP及端口nobind user nobody group nogroup ca /etc/openvpn/conf/ca.crt cert /etc/openvpn/conf/client-B.crt key /etc/openvpn/conf/client-B.key ping 15 ping-restart 45 ping-timer-rem persist-key persist-tun ns-cert-type server comp-lzo verb 4 log-append /etc/openvpn/log/openvpn.log status /etc/openvpn/log/openvpn-status.log tcp-queue-limit 4096 # 256 bcast-buffers 4096 3.5 启动客户端1$ sudo openvpn --config /etc/openvpn/conf/client.conf --askpass /etc/openvpn/conf/password.txt --daemon 3.6 检查验证12$ ifconfig|grep inet|grep 10.8.0.120 inet 10.8.0.120 netmask 0xffffff00 broadcast 10.8.0.255 注：得到IP为：10.8.0.x 则说明VPN客户端配置成功1$ ping 10.8.0.1 #检查是否能ping通内网等机器 3.7 服务加入开机自启动12vim /etc/rc.localopenvpn --config /etc/openvpn/conf/client.conf --askpass /etc/openvpn/conf/password.txt --daemon","categories":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://blog.pinbot.me/tags/运维/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}]},{"title":"从程序员到架构师的成长之路","slug":"从程序员到架构师的成长之路","date":"2016-09-07T06:08:21.000Z","updated":"2016-09-27T17:35:55.000Z","comments":true,"path":"2016/09/07/从程序员到架构师的成长之路/","link":"","permalink":"http://blog.pinbot.me/2016/09/07/从程序员到架构师的成长之路/","excerpt":"作者: likaiguo 从程序员到架构师的成长之路 课程大纲 程序员的技术发展道路和职业规划 提升代码质量和开发效率的方法 什么是适合业务发展的好架构? 架构师日常工作,享受什么样的苦与乐?","text":"作者: likaiguo 从程序员到架构师的成长之路 课程大纲 程序员的技术发展道路和职业规划 提升代码质量和开发效率的方法 什么是适合业务发展的好架构? 架构师日常工作,享受什么样的苦与乐? 讲师简介李开国 对系统架构设计有深入理解 专注机器学习和自然语言处理 某互联网创业公司技术总监 前腾讯QQ离线数据挖掘工程师 OneAPM公开课:《推荐系统架构演进》讲师 趋势科技Big Data创意程序大赛中国区亚军,国际季军 中科大硕士研究生 weixin: likaiguoweibo : http://weibo.com/likaiguo 课程大纲 程序员的技术发展道路和职业规划 提升代码质量和开发效率的方法 什么是适合业务发展的好架构? 架构师日常工作,享受什么样的苦与乐? 一.程序员的技术发展道路和职业规划 明确入行的目的 冲着“收入高”这一点 对技术充满热爱 有改变世界的冲动 技术发展规划 软件业人才结构 程序员是技术相关的职业生涯一个不错的开始，不论你以后是要做CTO还是总监等等，只要你还做着技术大家庭中的一员，那现在的技术沉淀，都将是你未来的基石。 主要技术类岗位 选择合适的工具 语言只不过是一个工具，“与其分散进攻，不如全力一击”万变不离其宗 面向过程 面向对象 函数式编程 明确发展方向做啥??? 职业规划最关键的一点: 你的梦想(理想)是什么? O(∩_∩)O哈哈~ 角色发展路线 不仅仅是coder抽取《软技能：代码之外的生存之道》 程序员来源专业: 科班计算机/软件工程类专业; 自动化,通信,信息科学类计算机相关专业; 生物相关理科专业; 文科类专业; 大致分为以下几类: 专业科班相关专业人员半路出家基于兴趣 职位跳转图谱:软件工程师 职位跳转图谱:架构师 职业通道的路线一览 可能与有些公司的职位不符，毕竟公司不一样，规模和起名的习惯可能不一样，但是大体上是这么个路子。不要太拘泥于职位名称。 程序员职业发展路径程序员工作两三年后，基本上都会考虑自己的未来发展方向。发展的路径主要有以下三种： 程序员–系统分析员–架构师–技术经理(Team Leader)–&gt;(技术总监)–CTO； 程序员–&gt;高级工程师–&gt;资深工程师–&gt;技术专家–&gt;CTO; 程序员–项目组长–项目经理–项目总监–CTO； 程序员–&gt;产品经理–&gt;产品总监–&gt;CTO; 现在还多了另一条路，创业（创业合伙人）。现在技术创业的越来越多，大有流行趋势。 （虽然都是CTO，主要的关注点和方向上有些不一样） 不管是项目经理还是技术经理与产品经理，都要求要熟悉业务，业务是需求的来源，没有不谈业务的技术，所以不管你从哪个方向发展，都要求对业务熟悉。 产品经理要求对业务最熟悉，项目经理次之，技术经理排最后。对于程序员来说，刚开始工作的前几年可以埋头扎到技术里面，一般这个时间在2-3年的时间，然后就应该多关注业务了。 分享8年开发经验，浅谈程序员职业规划程序员的职业发展道路 从技术向业务过渡 从程序员向技术管理发展 单方面向技术发展 浅谈程序猿的职业规划，看你如何决定自己的未来吧 程序员职业发展路径开发工程师：这个大家是最熟悉的，这个角色主要负责系统中某个模块或某个功能的设计与编码，有时候还会有数据库设计的工作等等。 研发经理：主要负责项目的技术选型，技术难题的攻克，技术人员的招聘，团队成员的技术培训与熏陶等一系列与技术相关的工作。 项目经理：主要负责项目进度的规划、跟进、落实、交付以及与客户的沟通等任务，是一个项目的监督者与管理者。 小组讨论:IT行业是吃青春饭??? 程序员工作只能做到35岁吗？之后的路是怎么走的呢? 软件工程师年龄分布 高级软件工程师年龄分布 系统架构师年龄分布 IT行业是吃青春饭???关于年龄的传说: 你看的是五年前的文章吧，现在的主流说法是40岁。五年前是35岁，我大学那会儿是30岁。时代是不断发展的。 我二十二的时候，他们说程序员只能干到25 。我二十五的时候，他们说程序员只能干到27 。我三十的时候，他们说程序员只能干到 35 。我现在三十七了。我觉得再干三十年毫无压力。 编程不是青春饭，技术才是硬通货。写程序可以说像盖房子，又不能说就是盖房子。第一，盖房子要绝对的体力，人年纪越大，越吃力。写程序不一样，体力只是一部分，最重要的是智慧，同样的一个模块，你去看senior和刚毕业的小豆包们，绝对不一样。第二，程序员，与其说软件工程师，是要求有完整的思维的，同样是计算机毕业的软件系的同学，北大青鸟和MIT的肯定不一样，所以知识和思维才是软件工程的核心~第三，任何行业都会是优胜劣汰的~时间只是催化剂 成功无所谓年纪如果你仍有斗志，上天就只能让你失踪于海难，让你出车祸，让你死于滑翔伞事故，让你得阿兹海默氏症，或者诸如此类的方式，才能无耻的战胜你。 年纪大了、有家庭了、有小孩了，放不开手脚了。这个「现实」可不仅是程序员需要面对的，是所有人都需要面对的。 心态 真的太重要。 成功无所谓年纪: 你周围的人 国内外牛人各大公司的技术专家?计算机语言的设计者? 二.提升代码质量和开发效率的方法 第一原则: DRYDon’t repeat youself!!! 1.代码质量保证代码质量最简单的方法: 这个代码会给其他人review! 团队代码规范 代码review 单元测试与集成测试 功能测试与性能测试 规范的需求和设计文档 拥抱开源 阅读github上star或fork数高的代码 向开源社区提交代码 遵循开源社区的代码规范 celery对贡献者要求 Community Code of Conduct(社区行为准则) Be considerate(为别人着想) -&gt; 首先为人写程序,其次为机器 Be respectful(尊重) Be collaborative(合作) When you disagree, consult others(异见时询问他人) When you’re unsure, ask for help(不确定时,寻求帮助) Step down considerately(稳定的交接任务) 找到合适的导师(尤达)向周围的人求教 没有天才 &lt;极客与团队&gt; 最直接的方式找认识的人,特别是团队中. 找到合适导师: 书籍《代码整洁之道》《代码大全》《重构 改善既有代码的设计》 计划一年要读书的数量 StackOverflow讨论帖:哪本最具影响力的书，是每个程序员都应该读的 59本 [^books] 找到合适导师: 公开课/MOOCMOOC（Massive Open Online Courses） 国外平台:The Best MOOC Provider: A Review of Coursera, Udacity and Edx coursera https://www.coursera.org edx https://www.edx.org/ 优达学城 (Udacity) https://www.udacity.com/ 国内平台 网易公开课 http://open.163.com/ 慕课网 http://www.imooc.com/ 麦子学院 http://www.maiziedu.com/ 北京大学的公开课平台 http://mooc.pku.edu.cn/ 学堂在线 https://www.xuetangx.com/ 推荐: 在Coursera，随时都是学习的好时候–微软亚洲研究院副院长 张峥 文档 产品设计文档 软件设计文档 测试用例文档 项目部署文档 为什么需要有正式的文档? 重构: 改善既有代码的设计重构：在不改变软件可观察行为的前提下改善其内部设计 为何重构： 重构改进软件设计,提高软件质量 重构软件更容易理解,提升可读性 重构帮助找到bug,减少错误 重构提高编程速度,阻止系统腐烂变质 书: 《重构 改善既有代码的设计》 重构: 代码的坏味道 重复代码 过长函数 过大的类 过长参数列 发散式变化：类经常因为不同的原因在不同的方向上发生变化 霰弹式修改：每遇到某种变化，你都必须在许多不同的类内做出许多小修改 依恋情结：一个类的动作过分依赖其他类 数据泥团：不同地方的相同数据字段 基本类型偏执 Switch 惊悚现身：考虑用多态代替 switch 重构: 代码的坏味道 平行继承体系：为某个类增加一个子类的时候，也必须为另一个类相应增加一个子类冗赘类 夸夸其谈未来性：某个抽象类其实没啥太大作用 令人迷惑的暂时字段 过度耦合的消息链 中间人：某个类接口有一半的函数都委托给其他类 狎昵关系：两个类过于亲密 异曲同工的类：两个函数做同一件事，却有着不同的签名 不完美的库类 纯稚的数据类：单纯的数据容器 被拒绝的遗赠：子类复用超类的行为，却又不愿意支持超类的接口 过多的注释：当你感觉需要撰写注释时，请先尝试重构，试着让所有注释都变得多余 构筑测试体系测试 确保所有测试都完全自动化，让它们检查自己的测试结果 一套测试就是一个强大的 bug 侦测器，能够大大缩减查找 bug 所需要的时间 频繁地运行测试。每次编译请把测试也考虑进去——每天至少执行每个测试一次 每当你收到 bug 报告，请先写一个单元测试来暴露 bug 编写未臻完善的测试并实际运行，好过对完美测试的无尽等待 考虑可能出错的边界条件，把测试火力集中在那儿 当事情被认为应该会出错时，别忘了检查是否抛出了预期的异常 不要因为测试无法捕捉所有 bug 就不写测试，因为测试的确可以捕捉到大多数 bug 浅谈代码覆盖率有赞分层自动化测试实践敏捷开发中高质量 Java 代码开发实践写出高质量代码的10个Tips 2.开发效率效率 就是工作量除以时间。提高效率需要从这两方面着手，一是增大工作量，二是缩短工作时间。 【如何提升工作效率】1、列出具体行动和细分目标，把待办清单画成流程图；2、给每项清单任务附上优先度；3、定时轮换任务调动积极性，花1小时在重要任务上，然后换着做一项容易而优先度较低的任务；4、保持对重要任务的关注度，正在做一件事，却不时想着另外一个事，请把那件事记下来，忙完后再去做。 问:程序员上班有什么提高效率技巧？ 断网!O(∩_∩)O哈哈~(Just a joke!) 其他回答打开音乐播放器，戴上耳机，少刷sns；有条件拔掉网线，无条件关掉浏览器\\QQ，手机静音，暂时无视所有产品经理和设计师。 一切源于专注 专注并且避免重复. DRY(Don’t repeat youself!!!)原则. 首先你得设置一个小目标. O(∩_∩)O! 如何专注? 方法: 番茄工作法 设置有优先级的任务列表(Todo List) 工具: 番茄土豆 关掉微信,QQ,邮件提醒;统一的时间,集中回复; 设置可量化的目标 代码行数 文档数量 commit,bugfix数量 代码覆盖率 nose,CI 设置Deadline 美国的”自然科学基金委”(NSF)发现最近申请各种资金的科学项目提案太多了，审都审不过来，怎么办呢？结果他们发现最好的解决办法就是。。。不设置申请的截止日期 (deadline)，总的提案数量自然减半； O(∩_∩)O!!! 保持对deadline的敬畏 一鼓作气,再而衰,三而竭 我们要多线程么?为什么多任务并行一般都很糟? 不停的上下文切换带来过多消耗 什么任务适合 进行多线程/多进程 并行 例如: 跑步时候听听音乐;写程序时候听舒展的音乐 写作时候看电视??? 例子: 开车时候换挡/巡航模式 函数堆栈 好记性不如烂笔头书写 记云笔记，特别是费好大劲整理出来的资料。轻而易举就能拿到的资料没必要花时间记，利用搜索引擎即可。 桌上放一本白色草稿纸和笔，你随时需要利用图表捋顺思路。 思维导图: 捋顺思路后不妨花点时间整理成思维导图，下次看5秒的效果相当于花5分钟重整思路。 markdown让书写更美好 语法简洁 专注于内容 不去担心样式 纯文本便于版本管理 markdown使用举例: 文档书写: 强大的插件支持 书写博客: hexo等 PPT: landslide 论文: pandoc+latex 责任心 对自己负责: 对自己的承诺负责 自驱力: 自我驱动 外部问责 找到你的Time Killer找到最大的时间杀手 找到最花费时间的地方; 找到自己的节奏: 什么时间段工作效率最高; 分配好工作内和工作外; 跟踪你的时间; 是不是在: SNS 查看碎片信息: weibo,weixin,qq 看电视上花费高昂? 形成习惯行为 惯例 习惯 成就我们的恰恰是那些不断重复做的事情.因此,优秀不是一种行为,而是一种习惯. – 亚里士多德 要培养工作习惯，并且要让其他人理解尊重你的工作习惯，你知道总被人打断的碎片化编程时间会杀了你 了解习惯 习惯: 暗示,惯例,奖励 找出坏习惯 打开电脑后,查看email,各种QQ,微信,刷网页 形成新习惯 集中处理碎片任务 不加班，晚上早点睡觉，保持作息规律 工作时间设立小的计划 快速解决coding中遇到的问题 官方文档 Google(如果是技术问题避开百度吧!) Github Stackoverflow 源码 对新技术充满热情 愿意使用新东西(慎重用于生产) 爱折腾 不要重复造轮子 社会化编程的趋势越来越明显 研究和学习成熟的库 拥抱开源社区 敢于造新轮子并分享/开源它我们并不总是满意其他人的封装和开源的工具包.慎重思考发现并不那么美好的时候,要敢于动手造一个新的了; 找不到的轮子 不合适的轮子 不完美的轮子 例如: gitchangelog mkdocs flask_admin markdown_images http://img.pinbot.me:8088/ 小组讨论:阿里员工用脚本抢中秋节月饼,你怎么看??? 自动化一切Automate everything!!! 非常具有工程师思维. 只是没有做好边界条件测试; 自动化一切 各种自动化工具: fabrickk,ansible,docker等自动化测试 工程师天生是追求效率的有人说认为程序员花大量的时间做自动化的工具，还不如人肉的效率高，比如，写自动化的脚本花5个小时，而重复做这件事200次只花3个小时。有这样的理解的人根本不懂工程。一方面，这个工具可以共享重用，更多的人可以从中受益，这次我花5个小时开发这个工具，下次只用1小时改一下就可以用在别的地方，这是着眼于未来而不是眼下的成本。更重要的是，这是一种文化，一种提高效率的文化，他会鼓励和激发出更多的这样的事情发生。 人类之所以比别的动物聪明就是会使用和发明工具.而古语也有云：“工欲善其事，必先利其器”，看看美军的装备你就知道战争工具的好坏有多重要了，一个公司的强大之处在执行力，而执行力的强大之处在于你有什么样的支持工具。这些，已经不是工程师文化，而是人类发展的文化。 月饼事件-新篇章,leon lee最后回应 快捷键熟练使用快捷键，不单能提高操作之间的切换速度。更重要的是它能时刻提醒你，你的软件还有这样那样的功能（尤其是IDE上的功能）。 不要让手指离开键盘编辑器之神: vim我用到的vim模式演示 vim 所有编辑器: haroopad,Sublime,eclipse,pycharm 浏览器: vimium 命令行: oh-my-zsh vi插件 其他各种快捷键F2,F5-8,F12,C,V… 工欲善其事必先利其器效率提升工具集推荐硬件层面 跑的更快的设备(Mac经验谈,SSD) 宽度合适的屏幕,多配一块屏幕辅助，省得来回切换窗口。 键盘代码书写 合适的IDE vim/emacs/sublime 编辑器 纯文本的威力 文档书写:markdown协作工具 团队协作工具tower 代码版本管理git 持续集成Jenkinsvim模式无处不在,各种插件 浏览量网页: chrome的vimium插件 代码编辑器: 启用vim模式(eclipse,sublime) 书写文档: vim模式 命令行: oh-my-zsh,vim跳转 减少无效沟通 减少无效会议 用有效的非即时团队沟通软件如Tower、 Trello等,建立任务清单 无法快速用即时通讯软件完成的采用当面沟通或电话 没有银弹No sliver bullet!!! 不存在一个神奇的方法或技术“银弹”，实现数量级以上的程序员的工作效率的提升。 《人月神话》 三.什么是适合业务发展的好架构? 架构设计是由需求驱动，而非模型驱动。软件需求 功能需求 质量属性（非功能需求） 设计约束 不管是高层次的架构设计也好，还是最简单的功能实现也罢，对需求的把握都是至关重要的。需求才是我们付出所有努力想要达到的目的，脱离了需求，就是“答非所问”。同样，大多的反复和变更都是因为对需求的把握不够精准，因此我们要给予需求足够的重视。 一线架构师阅读体会-需求之于架构 唯一不变的就是变化本身,把握好需求 架构是不断演进的架构，平台不是买来的，也不是用一个开源就能获得的，也不是设计出来，而是长期演化才能落地生根的。 架构需要验证在系统真正地投入生产使用之前，再好的架构都只是假设，产品越晚被使用者使用，失败的成本和风险就越高，而小步行进，通过MVP快速实验，获取客户反馈，迭代演化产品，能有效地减少失败的成本和风险。 什么是软件架构 什么是架构 架构的种类 功能架构 技术架构 服务器架构 企业架构 网络架构 数据库架构 …. 软件架构定义 架构重要么? 软件架构好处 让团队跟随清晰的愿景和路线图 技术领导力和更好的协调 与人交流的刺激因素: 以便于回答与重要决策,非功能需求、限制和其他横切关注点相关的问题 识别和减轻风险的框架 方法和标准的一致性,随之而来的结构良好的代码库 正在构建的产品的坚实基础 与不同的听众,以不同层次的抽象来交流解决方案的结构 基本概念篇 解析软件架构概念 软件架构是应用程序与系统架构的结合 即从代码结构到将代码部署到生产环境,与一个软件系统重要元素相关的所有东西都是软件架构 应用程序架构 应用程序架构讨论的是软件设计低级别切面,通常只考虑单一的技术栈(如:java,.net,python) 结构单元以软件为基础 系统架构 更大规模的应用程序架构 端到端软件系统在较高层次的整体结构.组件和服务更高层次的抽象. 结构单元是各种软硬件,从编程语言框架到服务器和基础设施 架构设计基础 各种经典的设计模式(GoF) &lt;设计原本&gt; 设计的基本原则 Don’t Repeat Yourself (DRY) Keep It Simple, Stupid (KISS) Program to an interface, not an implementation设计模式中最根本的哲学，注重接口，而不是实现，依赖接口，而不是实现。接口是抽象是稳定的，实现则是多种多样的。 Command-Query Separation (CQS) – 命令-查询分离原则 You Ain’t Gonna Need It (YAGNI)- 只考虑和设计必须的功能，避免过度设计。 Law of Demeter – 迪米特法则 - “最少知识原则” 面向对象的S.O.L.I.D 原则 Single Responsibility Principle (SRP) – 职责单一原则 Open/Closed Principle (OCP) – 开闭原则 Liskov substitution principle (LSP) – 里氏代换原则 Interface Segregation Principle (ISP) – 接口隔离原则 Dependency Inversion Principle (DIP) – 依赖倒置原则 设计的基本原则 Common Closure Principle（CCP）– 共同封闭原则一个包中所有的类应该对同一种类型的变化关闭。一个变化影响一个包，便影响了包中所有的类。一个更简短的说法是：一起修改的类，应该组合在一起（同一个包里）。如果必须修改应用程序里的代码，我们希望所有的修改都发生在一个包里（修改关闭），而不是遍布在很多包里。 Common Reuse Principle (CRP) – 共同重用原则CRP原则帮助我们决定哪些类应该被放到同一个包里。 Hollywood Principle – 好莱坞原则“don’t call us, we’ll call you.”意思是，好莱坞的经纪人们不希望你去联系他们，而是他们会在需要的时候来联系你。也就是说，所有的组件都是被动的，所有的组件初始化和调用都由容器负责。组件处在一个容器当中，由容器负责管理。好莱坞原则就是IoC（Inversion of Control）或DI（Dependency Injection ）的基础原则。这个原则很像依赖倒置原则，依赖接口，而不是实例， 设计的基本原则 High Cohesion &amp; Low/Loose coupling &amp; – 高内聚， 低耦合UNIX操作系统设计的经典原则，把模块间的耦合降到最低，而努力让一个模块做到精益求精。内聚：一个模块内各个元素彼此结合的紧密程度耦合：一个软件结构内不同模块之间互连程度的度量 Convention over Configuration（CoC）– 惯例优于配置原则 Separation of Concerns (SoC) – 关注点分离 Design by Contract (DbC) – 契约式设计 Acyclic Dependencies Principle (ADP) – 无环依赖原则 一些软件设计的原则 最小可用产品(MVP)理念做出最小可用产品(Minimum Viable Product， MVP)，尽快丢给用户试用，快速获取客户反馈，在此基础上不断迭代和演化架构和产品。 过度工程（Over Engineering）的问题讲产品架构和用户之间没有形成有效的反馈闭环，架构师想的和客户想的不在一个方向上，通过最小可用产品，快速迭代反馈的策略，可以避免这种问题。 架构模式 分层架构(n层架构) SOLID原则的通用架构 事件驱动架构:一种流行的分布式异步架构模式 用于小规模或者大规模的应用程序 可以与 调停者拓扑（Mediator Topology） 或者 代理者拓扑（Broker Topology） 一起使用 微内核架构(插件架构) 核心系统和插件模块 微服务架构 核心概念是具备高可伸缩性、易于部署和交付的独立部署单元（Separately Deployable Units） 最重要的概念是包含业务逻辑和处理流程的服务组件（Service Component） 架构原则和模式 如何呈现设计的架构? 可视化软件 画有效的草图 模式设计工具(UML,工具:staruml) UML的5视图方法:4+1视图始终是架构师界最通用的东西，寻找一种向世界妥协的方式。 职责划分（逻辑视图） 程序单元组织（开发视图） 控制流组织（运行视图） 物理节点安排（物理视图） 持久化设计（数据视图） 一线架构师实践指南（二）软件架构师书籍 架构可视化:一图胜千文,图文并茂建模工具对比 建模工具 利 弊 UML 善于表达静态与动态结构 不善于表达概念、约束与行为 文字 不善于表达概念、约束与行为 善于表达静态与动态结构 (伪)代码 好的代码有很强表达能力 太细、难以反映意图、不便于非程度员阅读(非通用语言) 以文字为主体，配合以图形（可以用UML）；图形不要太大、太细；不但要表达是什么，而且要表达为什么。 不画图的专家不是好的架构师【UML 建模】UML建模语言入门-视图,事物,关系,通用机制 恰如其分的预先设计 方法学 瀑布模型: 大型预先设计,推崇写代码前每件事情都经过讨论和评审 敏捷开发: 充分自由度,快速行动,拥抱变化,反馈和交付价值. 演化架构和浮现式设计 恰如其分很难具体量化 过少设计 过分设计 为设计设置语境 最关键是明确自己的需求 为软件生成轻量的文档 代码不会讲完整的故事 软件文档即指南 语境 功能性概览 质量属性 约束 原则 软件架构 外部接口 代码: 呈现底层细节,解释工作原理 文档化的代码 支持自动化生产部分文档 数据文档 数据字典 数据模型 物理架构 服务器架构 网络架构 部署文档 架构实例剖析包括聘宝平台的Web端系统架构、推荐系统架构、分布式存储/计算系统、底层服务器架构 聘宝系统架构演进路线 其中一些的问题 人员架构: 应对需求与团队规模 两个人 5个人 20个人 服务器架构: 2台服务器: 1+1 10台服务器: 5+5 30台: 10+20 60台+: 15+35 架构演进 web端系统: MVC模式–&gt;前端分离 web端架构 微内核架构: 核心系统 和 插件模块 web端系统与推荐系统解耦 RPC架构: 消息队列 华为内部如何实施微服务架构？华为内部如何实施微服务架构？ 架构之道-规划、简化和演化规划还是演化 好的架构是设计出来的 好的性能,好的质量主要源于好的设计,而不是依赖测试 架构设计的质量直接影响演化的难以程度 联想高级架构师分享：架构之道-规划、简化和演化 缺少规划难以演化单靠演化，即使能使架构越来越优化，也可能需要很长的周期，而对于产品或者项目，时间这个约束条件往往是苛刻的。迭代是有条件的。建议：在有规划的基础上进行演化。我们无法得到普适的架构，但可以得到确定领域的通用架构，可以在特定领域通过演化使应用架构逐步优化，逐步与业务架构想适应，提高匹配度。 四.架构师日常工作,享受什么样的苦与乐? ##软件架构师的职责 架构师的基本分类 根据职能: 前端架构师,后端架构师,算法架构师,分布式架构师,运维架构师 其承担的责任 确保概念的完整性，合理的切分工作，制定接口 架构师是最重要的，以确保概念的完整性，合理的切分工作，制定接口。行政领导应当尊重架构师的权威。 软件架构师的职责架构师需要参与项目开发的全部过程，包括需求分析、架构设计、系统实现、集成、测试和部署各个阶段，负责在整个项目中对技术活动和技术说明进行指导和协调。 小组讨论:架构师需要写代码么??? 尽可能要写,找到合理的平衡架构师要尽可能写代码，做测试，纸上谈兵式做架构而后丢给团队的作法非常不靠谱（除非是已经非常清晰成熟的领域） 软件工程师角色和职责区别简单地将写程序的工程师分成三类: 第一，写程序的人 （Coder、Employee、Worker）这种类型的人单纯的只是为了工作、功课、任务而写程序，虽然职务名称叫做工程师，但是写程序对他们来说只是获取成绩、金钱的工具，写程序对他们来说枯燥无味，但为了生活，他们继续产出他们的程序码。 第二，有目标而写程序的人 （Hacker、Doer、Entrepreneur）这种类型的人并不是因为热爱「程序」本身而开始写程序，他们写程序是为了要达成某些目的。 第三，热爱程序本身的人 （Architect、Theorists、Change Maker、Geek）这类工程师喜欢程序本身，他们欣赏程序设计的架构、可扩充性、可被测试性。他们喜欢最新的科技，并且会主动的去接触、试用它们。他们喜欢写有架构、能够被别人重复使用的套件 （Library）。 在我们的环境中，有太多的 Coder、也有许多从 Coder 变成的 Hacker（他们的差别只在有没有目标，还有去实作的毅力），但比较少真正愿意奉献、热爱程序的 Architect。 三种软件工程师——编码员、程序师和架构师 coder vs hacker 架构师的必备技能 架构师的必备技能 项目技能 技术技能 想象力技能 1.一个好的架构师首先是一个合格的工程师；2.具有抽象的思维能力，能把业务抽象在抽象；3.了解技术前沿知识，并深知其优劣；4.沟通；5.权衡取舍，能够在设计系统时综合考虑；6.业务精良，同时具有多领域知识，因为有时候业务是相通的; 进度评估人月 换算 &lt;人月神话&gt; “测试的时间至少需要整个开发流程时间的一半”。 团队协作外科手术队伍: 专业 用好tower进行任务分配和进度管理 用好版本管理工具(git) 用好github,进行代码review 一个人每天200行有效代码 vs 20个人 * 150行/每天 团队文化建设你不是一个人在战斗. 谦逊、尊重、信任-HRT(Honest,Respect,Trust)原则. 团队技术分享 拥抱开源社区 极客与团队 &lt;极客与团队-软件工程师的团队生存秘笈&gt; 与人打交道 架构师与研发团队 关系技能: 架构师和各个业务需求方 商务技能 必要的会议 项目产品需求评审 每日研发内部站立晨会 研发内部关键模块技术评审 除此之外还有一些非研发团队沟通会议 架构师必知软件架构师应该知道的97件事（极致总结） 工具集讲义中提到的各个工具集 用途 推荐工具 你喜欢的 版本管理 git 番茄工作法 番茄工作法 …. … 架构师的每日工作流程案例敏捷开发总的流程如下： 需求规划和分期 需求评审 需求讲解 方案评审 每日晨会 性能测试 CodeReview Demo 测试阶段10.线上Bug修改流程 程序员简易成长指南:学习 如何更高效地学习？ 做一个全流程的demo,即使不理解也要做完 体系化的学习。抱着厚书硬啃了一遍，突然有种豁然开朗的感觉. 做笔记,画思维导图 再去看一些文章 带着问题学习更有效率 架构师应不应该写代码？ 应该 在代码和和其他工作之间平衡 程序员简易成长指南:职责从菜鸟码农到架构师 架构师职责 在代码中第一时间发现可能存在的问题，向其他人提出警告， 或是给予其他人改进的意见， 必要的时候或是给其他人演示一下正确的姿势。 保持大局观需要适度参与“核心模块”开发总的来说，架构师和程序员在某些方面上有点像产品经理和用户的关系，大部分程序员并不会主动告诉你他们想要什么、哪里需要优化，甚至自己也不知道这些。想要做出好的产品，捷径之一就是跟用户做同样的事情。 程序员简易成长指南:沟通 实践：开会是个技术活吗？ 是 大多数的会议都是在毫无意义的交流中浪费时间 这并不是会议才有的问题 大多数时候，沟通的核心不是你说了什么，而是你想要让对方了解什么、让他做什么。 良好的沟通能在工作中显著提升效率，但很多人忽略了这个事情。 程序员简易成长指南:沟通 恰到好处的进行沟通的原则 确保各方对背景的理解一致，比如开会之前先简单通过邮件交流一下，对新加入会议的人花个30秒钟做个前情提要，或者在讨论过程中让对方说一下他的理解。 去掉对方不能/不需要理解的内容，比如跟产品说“这个队列在高并发下因为锁的实现有问题导致CPU性能瓶颈”不如改成“我们发现了性能问题，持续10分钟了，10万用户收不到运营发的无节操广告，大概5分钟后扩容解决”。 确保在对方失去注意力前尽快说出重点， 不要说没有意义的内容浪费其他人的时间，比如”这需求做不了“或者”这里不可能出bug“，没有人想听到这些废话。 程序员简易成长指南:沟通 还有更好的办法吗？成为技术专家/架构师之后的工作可以说是痛并快乐着，会有很多人找你咨询问题，另一方面，会有太多人找你咨询问题。甚至有一段时间每天的工作就是解答问题，小到工具使用中到疑难bug，大到架构设计，从早上到晚上基本都是在给各种各样的小伙伴提供咨询服务。 简化到三个问题: “他们要你解决什么问题？” “你解决的是什么问题？“ ”还有更好的办法吗？“ 现在第三句已经很少问到了。 程序员简易成长指南:门槛 成为架构师最困难的门槛是？ 知易行难。架构师虽然听起来很高大上，但本质上 仍然是工程师，不是科学家，也不是忽悠人的江湖骗子。学习再多，也需要 实践落地。设计架构方案更多的是在做一些抽象和权衡：把复杂的需求抽象成简单的模型，从功能、性能、可用性、研发成本等等方面规划如何构建一个系统，这些内容需要更多的实践练习。 没有实战平台。没有工作在类似平台天天需要接触架构设计的地方，而很多公司没有架构方面的工作可供练级，于是就想办法从理论上下功夫，这类人的特征非常明显：在信息不足，甚至不了解实际场景的情况下就开始做架构设计，这种所谓的架构往往理解比较肤浅，经不住推敲。 需要经验和磨砺。每次招人之后我们都会做一些针对新人的架构方面的培训，课程材料基本上包括了系统架构相关的主要方面，但是学完这些材料之后就能成为独当一面的架构师了吗？并没有。相反，这仅仅是开始，新人真正做了实际生产的系统之后才算是正式入门：面对压力时才会懂得权衡，走过弯路之后才会寻找捷径。 程序员简易成长指南从菜鸟码农到架构师 1）大部分烂代码并不是架构师的设计问题；2）想要做出好的产品，捷径之一就是跟用户做同样的事情；3）大多数的会议都是在毫无意义的交流中浪费时间；4）程序员之间的差距或许比人和猴子之间的差距还大 参考书籍人月神话程序员的职业素养作为公司的架构师，一直致力于如何更好的设计架构，如何优化项目架构，如何提高开发效率和质量，却很少让团队成员理解和明白，为何要这样做。下一个小目标，让团队每个人都理解设计。程序员修炼之道：从小工到专家代码整洁之道代码大全软件架构师的12项修炼软件架构设计：程序员向架构师转型必备程序员必读之软件架构:告诉你怎么像架构师一样思考极客与团队重构 改善既有代码的设计深入理解计算机系统（原书第2版） [Computer Systems]编程珠玑（续 修订版） 参考文章StackOverflow讨论帖:哪本最具影响力的书，是每个程序员都应该读的 59本物理量纲失效了-论《人月神话》程序员简易成长指南：从菜鸟码农到架构师秦迪，微博平台及大数据技术专家. 爱折腾，喜欢研究从内核到前端的所有方向，近几年重点关注大规模系统的架构设计和性能优化，重度代码洁癖：以code review己任，重度工具控：有现成工具的问题就用工具解决，没有工具能解决的问题就写个工具解决。业余时间喜欢偶尔换个语言写代码放松一下。程序员如何才能晋升为优秀的高薪架构师？&lt;重构 改善既有代码的设计&gt;读书笔记软件架构师不等同于资深程序员高质量的工程代码为什么难写不是实现了业务需求就结束了呢，其实远没有，这其实只是写代码的开始，除了正向的逻辑实现外，任何一个点的异常的分支逻辑怎么处理才是工程化的代码中更难处理的部分，这个问题在单机式的系统中会相对还好处理，在分布式的环境会变得非常的复杂异常分支逻辑处理好后，通常还需要增加必要的日志信息，以便在出问题时方便排查吃掉重要的异常信息不抛出这种行为在写代码中是非常可耻的对于高质量的工程代码而言，其实实现业务逻辑只是其中占比很小的一部分，甚至花的时间是相对最少的一部分;好的工程代码，说难也难，说不难也不难，均体现在“工程”二字之上。除了代码之外，想想其他被冠以“工程”二字的，如：大厦、桥梁、船舶、水电站等等等等，高质量“工程”都有共性：安全、易用、可维护、美观… 综合多个维度，缺一不可。 系统设计案例支付宝系统架构（内部架构图） 微信联系我 THANK YOU","categories":[{"name":"UML","slug":"UML","permalink":"http://blog.pinbot.me/categories/UML/"}],"tags":[{"name":"面向对象","slug":"面向对象","permalink":"http://blog.pinbot.me/tags/面向对象/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.pinbot.me/tags/设计模式/"}],"keywords":[{"name":"UML","slug":"UML","permalink":"http://blog.pinbot.me/categories/UML/"}]},{"title":"react入门介绍","slug":"react入门介绍","date":"2016-09-06T10:30:30.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/09/06/react入门介绍/","link":"","permalink":"http://blog.pinbot.me/2016/09/06/react入门介绍/","excerpt":"react.js介绍react.js的提出react.js的首次提出是在2014年Facebook的f8大会上。顺便科普一下f8大会，f8大会是由Facebook组织的年度的技术峰会，之所以叫f8，就是看大家在8小时以内能做出哪些有意思的东西。react.js称为颠覆式前端UI开发框架。目前基于html的前端开发变得越来越复杂，传统的开发方式基于来自服务器和来自用户输入的交互数据,动态反应到复杂界面的时候，代码量变得越来越大，难以维护。比如，前端开发框架jquey，每次数据更新，必须手动把数据更新渲染到ui界面上,代码量极大。基于此，google推出的angular.js的双向数据绑定很好的解决了这个问题。但是angular.js也有自身的一些不足。1：angular过重，不适用于对性能要求特别高的站点。2：ui组件封装比较复杂，不利于重用。而react解决了所有的这些问题。ReactJS官网地址：http://facebook.github.io/react/Github地址：https://github.com/facebook/react","text":"react.js介绍react.js的提出react.js的首次提出是在2014年Facebook的f8大会上。顺便科普一下f8大会，f8大会是由Facebook组织的年度的技术峰会，之所以叫f8，就是看大家在8小时以内能做出哪些有意思的东西。react.js称为颠覆式前端UI开发框架。目前基于html的前端开发变得越来越复杂，传统的开发方式基于来自服务器和来自用户输入的交互数据,动态反应到复杂界面的时候，代码量变得越来越大，难以维护。比如，前端开发框架jquey，每次数据更新，必须手动把数据更新渲染到ui界面上,代码量极大。基于此，google推出的angular.js的双向数据绑定很好的解决了这个问题。但是angular.js也有自身的一些不足。1：angular过重，不适用于对性能要求特别高的站点。2：ui组件封装比较复杂，不利于重用。而react解决了所有的这些问题。ReactJS官网地址：http://facebook.github.io/react/Github地址：https://github.com/facebook/react react.js的特点1、就是轻，数据渲染响应非常快。复杂或频繁的DOM操作通常是性能瓶颈产生的原因。React为此引入了虚拟DOM（Virtual DOM）的机制：在浏览器端用Javascript实现了一套DOM API。基于React进行开发时所有的DOM构造都是通过虚拟DOM进行，每当数据变化时，React都会重新构建整个DOM树，然后React将当前整个DOM树和上一次的DOM树进行对比，得到DOM结构的区别，然后仅仅将需要变化的部分进行实际的浏览器DOM更新。尽管每一次都需要构造完整的虚拟DOM树，但是因为虚拟DOM是内存数据，性能是极高的，而对实际DOM进行操作的仅仅是Diff部分，因而能达到提高性能的目的。 2：组件化开发思想。React推荐以组件的方式去重新思考UI构成，将UI上每一个功能相对独立的模块定义成组件，然后将小的组件通过组合或者嵌套的方式构成大的组件，最终完成整体UI的构建。 react试用场景react 这么厉害到底适用于哪些场景呢？1、复杂场景下的高性能要求。2、重用组件库，组件组合。 react html、css基础实践下面让我们来看看一组代码： 1234567891011121314151617181920212223242526272829&lt;html&gt;&lt;head&gt; &lt;script src=\"../build/react.js\"&gt;&lt;/script&gt; &lt;script src=\"../build/react-dom.js\"&gt;&lt;/script&gt; &lt;script src=\"../build/browser.min.js\"&gt;&lt;/script&gt; &lt;style type=\"text/css\"&gt; .redColor&#123; color: red; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"example\"&gt;&lt;/div&gt; &lt;script type=\"text/babel\"&gt; var Hello = React.createClass(&#123; render:function()&#123; var styleObj = &#123; textDecoration:'underline' &#125;; return &lt;div className=\"redColor\" style=&#123;&#123;fontSize:'18px'&#125;&#125;&gt;Hello &#123;this.props.name&#125;&lt;/div&gt; &#125; &#125;); ReactDOM.render( &lt;Hello name=\"World\"/&gt;, document.getElementById('example') ); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 现在来解释一下这段代码1.react用的是jsx，是facebook为react开发的一套语法糖。语法糖是计算机中添加的一种语法，对语言的功能没有影响，但是更方便程序员使用，增加可读性减少程序出错机会。类似的还有CoffeeScript、TypeScript等。最终都被解析库解析成js。这里引入的browser.js 就是jsx的解析库。作用是将 JSX 语法转为 JavaScript 语法。另外 &lt;script&gt; 标签的 type 属性为 text/babel 。表明这是jsx语法。 2.jsx为我们带来的便利就是，我们可以在js里写类dom的结构，比我们用原生js拼接字符串要简单方便许多。jsx语法允许我们生成原生的dom标签，还可以生成自定义标签。比如hello，这些统称为react components.通过调用ReactDOM将react components呈现在页面上。 3.ReactDOM.render是React的最基本方法，用于将模板转为 HTML 语言，并插入指定的 DOM 节点。第一个参数是要插入的components，第二个参数是要插入的容器。自定义的标签是通过React.createClass申明，参数是一个js的对象。return的内容就是渲染的结构。遇到 HTML 标签（以 &lt; 开头），就用 HTML 规则解析；遇到代码块（以 { 开头），就用 JavaScript 规则解析。 4.给标签添加css属性有两种： 一种：用外联样式，注意这里是className，因为这是jsx语法，class在js中已经是一个保留关键字。 二种：内联样式。在react中内联样式必须用样式对象来表示，在react中内联样式必须用样式对象来表示，必须用驼峰。且用｛｛｝｝包裹。这里为什么要用｛｛｝｝，让我们再看看另一种写法就一目了然了。 123456789101112131415&lt;div id=\"example\"&gt;&lt;/div&gt;&lt;script type=\"text/babel\"&gt; var Hello = React.createClass(&#123; render:function()&#123; var styleObj = &#123; fontSize:'18px' &#125;; return &lt;div className=\"redColor\" style=&#123;styleObj&#125;&gt;Hello &#123;this.props.name&#125;&lt;/div&gt; &#125; &#125;); ReactDOM.render( &lt;Hello name=\"World\"/&gt;, document.getElementById('example') );&lt;/script&gt; 这里申明一个样式对象，用｛｝包裹就能以js的方式来解析。和｛｛fontSize:&quot;18px&quot;｝｝异曲同工。可以隐约的看到，react的组件通过样式对象的申明可以，react组件是html、css、js的集合，成为真正意义上的独立组件。 这次我们简单介绍了react的由来、特点、应用场景。以及，jsx语法糖，如何生成自定义标签，插入节点，添加css样式，这些都是react的基础，接下来，我们继续react compenents的生命周期。","categories":[],"tags":[],"keywords":[]},{"title":"Python-Web并发重复数据防守策略","slug":"Python-Web并发重复数据防守策略","date":"2016-09-02T09:51:45.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/09/02/Python-Web并发重复数据防守策略/","link":"","permalink":"http://blog.pinbot.me/2016/09/02/Python-Web并发重复数据防守策略/","excerpt":"作者：jackie 1.重复数据提交原因 恶意用户脚本攻击 web页面按钮卡顿重复点击引起","text":"作者：jackie 1.重复数据提交原因 恶意用户脚本攻击 web页面按钮卡顿重复点击引起 2.服务器优化方向 web服务器层防御，如nginx可以限制单一IP每秒钟的访问次数 应用层防御，通过web应用程序进行控制 数据层防御 3.常规防冲击 nginx 配置 –访问速率控制12345server &#123; ... location /download/ &#123; limit_conn addr 100; #单一IP每秒钟最多访问100次 &#125; 在代理层防御主要应对于大规模高并发，例如有恶意用户高速率抓取本网站数据，导致网站服务性能下降时，就需要进行IP访问速率限制；但是考虑到国内网络环境，基本绝大用户都是共享公共IP进行上网，所以此限制也并不是一定会打开。 黑名单机制 –防恶意攻击 网络服务商控制，例如使用阿里云的可以通过阿里云的安全策略配置进行设置黑名单。 服务器 本地防火墙策略 web服务器 nginx配置黑名单 web应用中通过缓存黑名单进行控制 4.异常访问带来的数据重复如何规避 数据表多字段进行联合唯一索引，通过数据库的限制进行脏数据的排除。 (推荐) 数据库加锁，分悲观锁和乐观锁，具体概念不做讲述，一旦加了锁，也就给开发者自己加了锁，自己琢磨去吧。 具体业务进行单一服务化，单实例进行处理，可通过MQ与主业务服务进行交互。（推荐） 5.具体的某个应用服务如何进行访问速率限制 直接上代码了，通过redis的原子操作机制设定计数器，也可称为限速器。123456789101112131415161718192021def limit_api_call(key, limit, timeout): \"\"\" API限速器 :param key: :param limit:限制次数 :param timeout: 单位时间 :return: True or False \"\"\" lua_incr = \"\"\" local current current = redis.call(\"incr\",KEYS[1]) if tonumber(current) == 1 then redis.call(\"expire\",KEYS[1],ARGV[1]) end return current \"\"\" current = client.eval(lua_incr, 1, key, timeout) current = int(current) if current &gt; limit: return False return True Reids官方文档中也提供了其他几种实现方式，但是除了是用lua脚本原子操作进行辅助，其他都只能概率限制，无法准确限速。","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"http://blog.pinbot.me/tags/python/"},{"name":"HTTP","slug":"HTTP","permalink":"http://blog.pinbot.me/tags/HTTP/"},{"name":"并发","slug":"并发","permalink":"http://blog.pinbot.me/tags/并发/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"JavaScript之递归","slug":"JavaScript之递归","date":"2016-09-01T16:52:45.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/09/02/JavaScript之递归/","link":"","permalink":"http://blog.pinbot.me/2016/09/02/JavaScript之递归/","excerpt":"作者：纯利 那么什么叫递归呢？所谓递归函数就是在函数体内调用本函数。最简单的例子就是计算阶乘。0和1的阶乘都会被定义为1，更大的数的阶乘是通过计算11…来求得的，每次增加1，直至达到要计算阶乘的那个数。","text":"作者：纯利 那么什么叫递归呢？所谓递归函数就是在函数体内调用本函数。最简单的例子就是计算阶乘。0和1的阶乘都会被定义为1，更大的数的阶乘是通过计算11…来求得的，每次增加1，直至达到要计算阶乘的那个数。 递归的缺点：如果递归函数的终止条件不明确或者缺少终止条件会导致函数长时间运行，是用户界面处于假死状态。值得注意的是：浏览器对递归的支持熟练与JS调用栈大小直接相关，当使用太多递归甚至超过最大调用栈容量时，浏览器会报错误信息，各个浏览器对报错的提示信息也不一样。 下面我们先来看一下一个经典的递归阶乘函数：1234567function test(num)&#123; if(num &lt;= 1)&#123; return 1; &#125;else&#123; return num * test(num-1); &#125;&#125; 上面的的这个函数表面上没有什么问题，但是以下的代码却可能会导致问题：1234567891011121314function Car(model, year, names) &#123; this.model = model; this.year = year; this.names = names; this.output= function () &#123; return this.model + \"喜欢\" + this.names; &#125;;&#125;var tom= new Car(\"大叔\", 30, '萝莉');var dudu= new Car(\"欧巴\", 24, '御姐');console.log(tom.output());console.log(dudu.output()); 指向原始函数的引用就剩下一个，当调用f()函数时，而test已经不再是一个函数了，所以会导致错误，但是我们可以使用arguments.callee来解决这个问题。 大家都知道，arguments.callee是一个指向正在执行的函数的指针，因此可以用它来实现函数的递归调用,看如下代码：1234567function test(num)&#123; if(num &lt;= 1)&#123; return 1; &#125;else&#123; return num * arguments.callee(num-1); &#125;&#125; 这样即使函数赋值给了另外一个变量，f()函数依然是有效的，所以递归调用能正常完成。而且这种方式在严格模式和非严格模式下都可以使用哦。","categories":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://blog.pinbot.me/tags/javascript/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}]},{"title":"数据处理/分析/可视化飞艇(zeppelin)介绍","slug":"数据处理-分析-可视化飞艇-zeppelin-介绍","date":"2016-08-31T17:35:55.000Z","updated":"2016-08-31T17:35:55.000Z","comments":true,"path":"2016/09/01/数据处理-分析-可视化飞艇-zeppelin-介绍/","link":"","permalink":"http://blog.pinbot.me/2016/09/01/数据处理-分析-可视化飞艇-zeppelin-介绍/","excerpt":"author: likaiguo Zeppelin(飞艇)Zeppelin思维导图 推荐查看思维导图中的各个链接,尤其是官方文档和中文翻译.","text":"author: likaiguo Zeppelin(飞艇)Zeppelin思维导图 推荐查看思维导图中的各个链接,尤其是官方文档和中文翻译. 快速搭建Zeppelin环境安装过程 到官网下载二进制包（http://zeppelin.apache.org/download.html） 解压到本地(保证已经设置好Java环境) 运行Zeppelin服务bin/zeppelin-daemon.sh start|stop|restart 浏览器中打开：http://localhost:8080 即可进入Zeppelin首页。 Zeppelin是什么?A web-based notebook that enables interactive data analytics.You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. 一款基于web页面的笔记本(类似ipython中的notebook),其提供交互式数据分析功能.使用Zeppelin(飞艇)我们能使用如SQL,Scala等后端语言制作出数据驱动的,交互式的并且易于协作的文档. Zeppelin基本概念1.支持多种后端语言,Interpreter(解释器) 抽象出解释器概念,运行各种语言和数据处理后端工具.在Zeppelin中解释器被设计为可插拔的模块.目前支持各种各样的解释器,如上图所示包括Apache Spark, Python, JDBC, Markdown and Shell等等. 同时也可以写自己需要的解释器. 在现有的解释器的基础上配置对应的参数生成新的解释器 写相关的Java或者scala程序开发更加特定的解释器[参考文献2] 2.强大的数据可视化能力 Zeppelin具有较为常用的数据可视化的图表. 如上图所示,表格,柱状图,饼图,趋势图,散点图一应俱全. 数据可视化不仅限于Spark SQL,任意一种语言的表格输出都能被完美转译成对应的图表. 并且能够导出对应的CSV等类型数据. 3.数据透视表Apache Zeppelin aggregates values and displays them in pivot chart with simple drag and drop. You can easily create chart with multiple aggregated values including sum, count, average, min, max. 飞艇能够在页面上通过简单的拖拽进行各种聚合操作,并且展示出对应的数据透视表.同时也可以很容易通过求和,计数,平均,最小,最大创建各种聚合值的图表. 4.动态表格Zeppelin可以通过动态表格方式在notebook中添加诸如: 文本框,复选框,单选框等表单元素.通过这种方式,我们可以快速进行对应的动态操作. 典型应用: 这里${maxAge=30}的写法表示一个文本框元素,并且默认值为30。当修改对应的值是下方的图表会对应产生变化。 https://zeppelin.apache.org/docs/latest/manual/dynamicform.html 文档很重要. 遇到一个奇怪的问题:当使用下拉框时,对应的值可以实时变化. 其他如文本框,复选框都不实时变化,需要点击三角形run按钮才能生效. 5.将notebook共享给他人,更好的协作 可以直接将写好的notebook发送给其他人,放入工程的notebook目录下即可 可以将生成的图表共享给他人(复制对应的link,参见文献2和官方文档) Zeppelin使用场景(特点)?apache zeppelin应该会很吸引分布式计算、数据分析从业者,是个值得把玩的算比较前卫的项目。 代码量少， 模块很清楚， 可以尝试接入多种不同计算引擎， 实时任务运行、可视化效果 没有过多复杂的操作，只是区分了多个notebook， 每个notebook里做单独的分析处理工作，流程和结果会被保存下来。 此外，为spark做了更好的支持，比如默认是scala环境，默认sc已经创建好，即spark local可跑，默认spark sql有可视化效果。 一站式数据分析: 文档,不同工具集一应俱全 Zeppelin怎样服务于我们的业务? 应用于快速导入数据并且进行可视化 将多种数据处理技术和语言融合在一起 优美文档书写 快速给客户提供数据可视化服务 Zeppelin常见问题参考文献 Apache Zeppelin简介 Apache Zeppelin安装及介绍 让Spark如虎添翼的Zeppelin – 基础篇 Zeppelin 小试牛刀 – 使用Zeppelin展示MySQL的数据 Hadoop - Zeppelin 使用心得","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.pinbot.me/categories/大数据/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://blog.pinbot.me/tags/spark/"},{"name":"pyspark","slug":"pyspark","permalink":"http://blog.pinbot.me/tags/pyspark/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://blog.pinbot.me/categories/大数据/"}]},{"title":"认识 Apache Lucene","slug":"认识Apache Lucene","date":"2016-08-31T10:00:04.000Z","updated":"2017-06-30T02:36:04.000Z","comments":true,"path":"2016/08/31/认识Apache Lucene/","link":"","permalink":"http://blog.pinbot.me/2016/08/31/认识Apache Lucene/","excerpt":"&emsp;&emsp;&emsp;&emsp;为了更深入地理解ElasticSearch的工作原理，特别是索引和查询这两个过程，理解Lucene的工作原理至关重要。本质上，ElasticSearch是用Lucene来实现索引的查询功能的。如果读者没有用过Lucene，下面的几个部分将为您介绍Lucene的基本概念。","text":"&emsp;&emsp;&emsp;&emsp;为了更深入地理解ElasticSearch的工作原理，特别是索引和查询这两个过程，理解Lucene的工作原理至关重要。本质上，ElasticSearch是用Lucene来实现索引的查询功能的。如果读者没有用过Lucene，下面的几个部分将为您介绍Lucene的基本概念。 熟悉 Lucene&emsp;&emsp;&emsp;&emsp;读者也许会产生疑问，为什么ElasticSearch 的创造者最终采用Lucene而不是自己开发相应功能的组件。我们也不知道为什么，因为我们不是决策者。但是我们可以猜想可能是因为Lucene是一个成熟的、高性能的、可扩展的、轻量级的，而且功能强大的搜索引擎包。Lucene的核心jar包只有一个文件，而且不依赖任何第三方jar包。更重要的是，它提供的索引数据和检索数据的功能开箱即用。当然，Lucene也提供了多语言支持，具有拼写检查、高亮等功能；但是如果你不需要这些功能，你只需要下载Lucene的核心jar包，应用到你的项目 中就可以了。 总体架构介绍Lucene架构之前必须理解一些基本的概念,才能更好的理解Lucene的架构,这些概念是: Document:它是在索引和搜索过程中数据的主要表现形式，或者称“载体”，承载着我们索引和搜索的数据,它由一个或者多个域(Field)组成。 Field*:它是Document的组成部分，由两部分组成，名称(name)和值(value)。 Term:它是搜索的基本单位，其表现形式为文本中的一个词。 Token:它是单个Term在所属Field中文本的呈现形式，包含了Term内容、Term类型、Term在文本中的起始及偏移位置。 Apache Lucene把所有的信息都写入到一个称为倒排索引的数据结构中。这种数据结构把索引中的每个Term与相应的Document映射起来，这与关系型数据库存储数据的方式有很大的不同。读者可以把倒排索引想象成这样的一种数据结构：数据以Term为导向，而不是以Document为导向。 ElasticSearch Servier (document 1)Mastering ElasticSearch (document 2)Apache Solr 4 Cookbook (document 3)所以索引(以一种直观的形式)展现如下： Term count Docs 4 1 Apache 1 Cookbook 1 ElasticSearch 2 Mastering 1 Server 1 Solr 1 正如所看到的那样，每个词都指向它所在的文档号(Document Number/Document ID)。这样的存储方式使得高效的信息检索成为可能，比如基于词的检索(term-based query)。此外，每个词映射着一个数值(Count)，它代表着Term在文档集中出现的频繁程度。当然，Lucene创建的真实索引远比上文复杂和先进。这是因为在Lucene中，词向量(由单独的一个Field形成的小型倒排索引，通过它能够获取这个特殊Field的所有Token信息)可以存储；所有Field的原始信息可以存储；删除Document的标记信息可以存储……。核心在于了解数据的组织方式，而非存储细节。 每个索引被分成了多个段(Segment)，段具有一次写入，多次读取的特点。只要形成了，段就无法被修改。例如：被删除文档的信息被存储到一个单独的文件，但是其它的段文件并没有被修改。 需要注意的是，多个段是可以合并的，这个合并的过程称为segments merge。经过强制合并或者Lucene的合并策略触发的合并操作后，原来的多个段就会被Lucene创建的更大的一个段所代替了。很显然，段合并的过程是一个I/O密集型的任务。这个过程会清理一些信息，比如会删除.del文件。除了精减文件数量，段合并还能够提高搜索的效率，毕竟同样的信息，在一个段中读取会比在多个段中读取要快得多。但是，由于段合并是I/O密集型任务，建议不好强制合并，小心地配置好合并策略就可以了。 分析你的文本问题到这里就变得稍微复杂了一些。传入到Document中的数据是如何转变成倒排索引的？查询语句是如何转换成一个个Term使高效率文本搜索变得可行？这种转换数据的过程就称为文本分析(analysis) 文本分析工作由analyzer组件负责。analyzer由一个分词器(tokenizer)和0个或者多个过滤器(filter)组成,也可能会有0个或者多个字符映射器(character mappers)组成。 Lucene中的tokenizer用来把文本拆分成一个个的Token。Token包含了比较多的信息，比如Term在文本的中的位置及Term原始文本，以及Term的长度。文本经过tokenizer处理后的结果称为token stream。token stream其实就是一个个Token的顺序排列。token stream将等待着filter来处理。 除了tokenizer外，Lucene的另一个重要组成部分就是filter链，filter链将用来处理Token Stream中的每一个token。这些处理方式包括删除Token,改变Token，甚至添加新的Token。Lucene中内置了许多filter，读者也可以轻松地自己实现一个filter。有如下内置的filter： Lowercase filter：把所有token中的字符都变成小写 ASCII folding filter：去除tonken中非ASCII码的部分 Synonyms filter：根据同义词替换规则替换相应的token Multiple language-stemming filters：把Token(实际上是Token的文本内容)转化成词根或者词干的形式。 所以通过Filter可以让analyzer有几乎无限的处理能力：因为新的需求添加新的Filter就可以了。 索引和查询在我们用Lucene实现搜索功能时，也许会有读者不明觉历：上述的原理是如何对索引过程和搜索过程产生影响？ 索引过程：Lucene用用户指定好的analyzer解析用户添加的Document。当然Document中不同的Field可以指定不同的analyzer。如果用户的Document中有title和description两个Field，那么这两个Field可以指定不同的analyzer。 搜索过程：用户的输入查询语句将被选定的查询解析器(query parser)所解析,生成多个Query对象。当然用户也可以选择不解析查询语句，使查询语句保留原始的状态。在ElasticSearch中，有的Query对象会被解析(analyzed)，有的不会，比如：前缀查询(prefix query)就不会被解析，精确匹配查询(match query)就会被解析。对用户来说，理解这一点至关重要。 对于索引过程和搜索过程的数据解析这一环节，我们需要把握的重点在于：倒排索引中词应该和查询语句中的词正确匹配。如果无法匹配，那么Lucene也不会返回我们喜闻乐见的结果。举个例子：如果在索引阶段对文本进行了转小写(lowercasing)和转变成词根形式(stemming)处理，那么查询语句也必须进行相同的处理，不然就搜索结果就会是竹篮打水——一场空。 Lucence查询语言ElasticSearch提供的一些查询方式(query types)能够被Lucene的查询解析器(query parser)语法所支持。由于这个原因，我们来深入学习Lucene查询语言，了解其庐山真面目吧。 基础语法用户使用Lucene进行查询操作时，输入的查询语句会被分解成一个或者多个Term以及逻辑运算符号。一个Term，在Lucene中可以是一个词，也可以是一个短语(用双引号括引来的多个词)。如果事先设定规则：解析查询语句，那么指定的analyzer就会用来处理查询语句的每个term形成Query对象。 一个Query对象中会存在多个布尔运算符，这些布尔运算符将多个Term关联起来形成查询子句。布尔运算符号有如下类型： AND(与):给定两个Term(左运算对象和右运算对象)，形成一个查询表达式。只有两个Term都匹配成功，查询子句才匹配成功。比如：查询语句”apache AND lucene”的意思是匹配含apache且含lucene的文档。 OR(或):给定的多个Term，只要其中一个匹配成功，其形成的查询表达式就匹配成功。比如查询表达式”apache OR lucene”能够匹配包含“apache”的文档，也能匹配包含”lucene”的文档，还能匹配同时包含这两个Term的文档。 NOT(非): 这意味着对于与查询语句匹配的文档，NOT运算符后面的Term就不能在文档中出现的。例如：查询表达式“lucene NOT elasticsearch”就只能匹配包含lucene但是不含elasticsearch的文档。 此外，我们也许会用到如下的运算符： +这个符号表明：如果想要查询语句与文档匹配，那么给定的Term必须出现在文档中。例如：希望搜索到包含关键词lucene,最好能包含关键词apache的文档，可以用如下的查询表达式：”+lucene apache”。 -这个符号表明：如果想要查询语句与文档匹配，那么给定的Term不能出现在文档中。例如：希望搜索到包含关键词lucene,但是不含关键词elasticsearch的文档，可以用如下的查询表达式：”+lucene -elasticsearch”。 如果在Term前没有指定运算符，那么默认使用OR运算符。此外，也是最后一点：查询表达式可以用小括号组合起来，形成复杂的查询表达式。比如：elasticsearch AND (mastering OR book) 多域查询当然，跟ElasticSearch一样，Lucene中的所有数据都是存储在一个个的Field中，多个Field形成一个Document。如果希望查询指定的Field,就需要在查询表达式中指定Field Name(此域名非彼域名)，后面接一个冒号，紧接着一个查询表达式。例如：查询title域中包含关键词elasticsearch的文档，查询表达式如下： title:elasticsearch也可以把多个查询表达式用于一个域中。例如：查询title域中含关键词elasticsearch并且含短语“mastering book”的文档，查询表达式如下： title:(+elasticsearch +”mastering book”)当然，也可以换一种写法，作用是一样的： +title:elasticsearch +title:”mastering book”) 词语修饰符除了可以应用简单的关键词和查询表达式实现标准的域查询，Lucene还支持往查询表达式中传入修饰符使关键词具有变形能力。最常用的修饰符，也是大家都熟知的，就是通配符。Lucene支持?和*两种通配符。?可以匹配任意单个字符，而*能够匹配多个字符。 请注意出于性能考虑，默认的通配符不能是关键词的首字母。 此外，Lucene支持模糊查询(fuzzy query)和邻近查询(proximity query)。语法规则是查询表达式后面接一个~符号，后面紧跟一个整数。如果查询表达式是单独一个Term，这表示我们的搜索关键词可以由Term变形(替换一个字符，添加一个字符，删除一个字符)而来，即与Term是相似的。这种搜索方式称为模糊搜索(fuzzy search)。在~符号后面的整数表示最大编辑距离。例如：执行查询表达式 “writer~2”能够搜索到含writer和writers的文档。 当~符号用于一个短语时，~后面的整数表示短语中可接收的最大的词编辑距离(短语中替换一个词，添加一个词，删除一个词)。举个例子,查询表达式title:”mastering elasticsearch”只能匹配title域中含”mastering elasticsearch”的文档，而无法匹配含”mastering book elasticsearch”的文档。但是如果查询表达式变成title:”mastering elasticsearch”~2,那么两种文档就都能够成功匹配了。 此外，我们还可以使用加权(boosting)机制来改变关键词的重要程度。加权机制的语法是一个^符号后面接一个浮点数表示权重。如果权重小于1，就会降低关键词的重要程度。同理，如果权重大于1就会增加关键词的重要程度。默认的加权值为1。可以参考 第2章 活用用户查询语言 的 Lucene默认打分规则详解 章节部分的内容来了解更多关于加权(boosting)是如何影响打分排序的。 除了上述的功能外，Lucene还支持区间查询(range searching),其语法是用中括号或者}表示区间。例如：如果我们查询一个数值域(numeric field)，可以用如下查询表达式： price:[10.00 TO 15.00] 这条查询表达式能查询到price域的值在10.00到15.00之间的所有文档。对于string类型的field，区间查询也同样适用。例如： name:[Adam TO Adria] 这条查询表达式能查询到name域中含关键词Adam到关键词Adria之间关键词(字符串升序，且闭区间)的文档。如果希望区间的边界值不会被搜索到，那么就需要用大括号替换原来的中括号。例如，查询price域中价格在10.00(10.00要能够被搜索到)到15.00(15.00不能被搜索到)之间的文档，就需要用如下的查询表达式： price:[10.00 TO 15.00} 处理特殊字符 如果在搜索关键词中出现了如下字符集合中的任意一个字符，就需要用反斜杠(\\)进行转义。字符集合如下： +, -, &amp;&amp;, || , ! , (,) , { } , [ ] , ^, “ , ~, *, ?, : , \\, / 。例如，查询关键词 abc”efg 就需要转义成 abc\\”efg。","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/categories/搜索引擎/"}],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/tags/搜索引擎/"},{"name":"apache lucene","slug":"apache-lucene","permalink":"http://blog.pinbot.me/tags/apache-lucene/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/categories/搜索引擎/"}]},{"title":"Lucene学习总结之一：全文检索的基本原理","slug":"Lucene学习总结之一","date":"2016-08-31T10:00:04.000Z","updated":"2017-06-30T02:36:32.000Z","comments":true,"path":"2016/08/31/Lucene学习总结之一/","link":"","permalink":"http://blog.pinbot.me/2016/08/31/Lucene学习总结之一/","excerpt":"一、总论根据http://lucene.apache.org/java/docs/index.html定义： Lucene是一个高效的，基于Java的全文检索库。 所以在了解Lucene之前要费一番工夫了解一下全文检索。 那么什么叫做全文检索呢？这要从我们生活中的数据说起。","text":"一、总论根据http://lucene.apache.org/java/docs/index.html定义： Lucene是一个高效的，基于Java的全文检索库。 所以在了解Lucene之前要费一番工夫了解一下全文检索。 那么什么叫做全文检索呢？这要从我们生活中的数据说起。 我们生活中的数据总体分为两种：结构化数据和非结构化数据。 结构化数据：指具有固定格式或有限长度的数据，如数据库，元数据等。非结构化数据：指不定长或无固定格式的数据，如邮件，word文档等。当然有的地方还会提到第三种，半结构化数据，如XML，HTML等，当根据需要可按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。 非结构化数据又一种叫法叫全文数据。 按照数据的分类，搜索也分为两种：对结构化数据的搜索：如对数据库的搜索，用SQL语句。再如对元数据的搜索，如利用windows搜索对文件名，类型，修改时间进行搜索等。对非结构化数据的搜索：如利用windows的搜索也可以搜索文件内容，Linux下的grep命令，再如用Google和百度可以搜索大量内容数据。对非结构化数据也即对全文数据的搜索主要有两种方法： 一种是顺序扫描法(Serial Scanning)：所谓顺序扫描，比如要找内容包含某一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，如果此文档包含此字符串，则此文档为我们要找的文件，接着看下一个文件，直到扫描完所有的文件。如利用windows的搜索也可以搜索文件内容，只是相当的慢。如果你有一个80G硬盘，如果想在上面找到一个内容包含某字符串的文件，不花他几个小时，怕是做不到。Linux下的grep命令也是这一种方式。大家可能觉得这种方法比较原始，但对于小数据量的文件，这种方法还是最直接，最方便的。但是对于大量的文件，这种方法就很慢了。 有人可能会说，对非结构化数据顺序扫描很慢，对结构化数据的搜索却相对较快（由于结构化数据有一定的结构可以采取一定的搜索算法加快速度），那么把我们的非结构化数据想办法弄得有一定结构不就行了吗？ 这种想法很天然，却构成了全文检索的基本思路，也即将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。 这部分从非结构化数据中提取出的然后重新组织的信息，我们称之索引。 这种说法比较抽象，举几个例子就很容易明白，比如字典，字典的拼音表和部首检字表就相当于字典的索引，对每一个字的解释是非结构化的，如果字典没有音节表和部首检字表，在茫茫辞海中找一个字只能顺序扫描。然而字的某些信息可以提取出来进行结构化处理，比如读音，就比较结构化，分声母和韵母，分别只有几种可以一一列举，于是将读音拿出来按一定的顺序排列，每一项读音都指向此字的详细解释的页数。我们搜索时按结构化的拼音搜到读音，然后按其指向的页数，便可找到我们的非结构化数据——也即对字的解释。 这种先建立索引，再对索引进行搜索的过程就叫全文检索(Full-text Search)。 下面这幅图来自《Lucene in action》，但却不仅仅描述了Lucene的检索过程，而是描述了全文检索的一般过程。 全文检索大体分两个过程，索引创建(Indexing)和搜索索引(Search)。 索引创建：将现实世界中所有的结构化和非结构化数据提取信息，创建索引的过程。搜索索引：就是得到用户的查询请求，搜索创建的索引，然后返回结果的过程。于是全文检索就存在三个重要问题： 索引里面究竟存些什么？(Index) 如何创建索引？(Indexing) 如何对索引进行搜索？(Search) 下面我们顺序对每个个问题进行研究。 二、索引里面究竟存些什么索引里面究竟需要存些什么呢？ 首先我们来看为什么顺序扫描的速度慢： 其实是由于我们想要搜索的信息和非结构化数据中所存储的信息不一致造成的。 非结构化数据中所存储的信息是每个文件包含哪些字符串，也即已知文件，欲求字符串相对容易，也即是从文件到字符串的映射。而我们想搜索的信息是哪些文件包含此字符串，也即已知字符串，欲求文件，也即从字符串到文件的映射。两者恰恰相反。于是如果索引总能够保存从字符串到文件的映射，则会大大提高搜索速度。 由于从字符串到文件的映射是文件到字符串映射的反向过程，于是保存这种信息的索引称为反向索引。 反向索引的所保存的信息一般如下： 假设我的文档集合里面有100篇文档，为了方便表示，我们为文档编号从1到100，得到下面的结构 左边保存的是一系列字符串，称为词典。 每个字符串都指向包含此字符串的文档(Document)链表，此文档链表称为倒排表(Posting List)。 有了索引，便使保存的信息和要搜索的信息一致，可以大大加快搜索的速度。 比如说，我们要寻找既包含字符串“lucene”又包含字符串“solr”的文档，我们只需要以下几步： 取出包含字符串“lucene”的文档链表。 取出包含字符串“solr”的文档链表。 通过合并链表，找出既包含“lucene”又包含“solr”的文件。 看到这个地方，有人可能会说，全文检索的确加快了搜索的速度，但是多了索引的过程，两者加起来不一定比顺序扫描快多少。的确，加上索引的过程，全文检索不一定比顺序扫描快，尤其是在数据量小的时候更是如此。而对一个很大量的数据创建索引也是一个很慢的过程。 然而两者还是有区别的，顺序扫描是每次都要扫描，而创建索引的过程仅仅需要一次，以后便是一劳永逸的了，每次搜索，创建索引的过程不必经过，仅仅搜索创建好的索引就可以了。 这也是全文搜索相对于顺序扫描的优势之一：一次索引，多次使用。 三、如何创建索引全文检索的索引创建过程一般有以下几步： 第一步：一些要索引的原文档(Document)。为了方便说明索引创建过程，这里特意用两个文件为例： 文件一：Students should be allowed to go out with their friends, but not allowed to drink beer. 文件二：My friend Jerry went to school to see his students but found them drunk which is not allowed. 第二步：将原文档传给分次组件(Tokenizer)。分词组件(Tokenizer)会做以下几件事情(此过程称为Tokenize)： 将文档分成一个一个单独的单词。 去除标点符号。 去除停词(Stop word)。 所谓停词(Stop word)就是一种语言中最普通的一些单词，由于没有特别的意义，因而大多数情况下不能成为搜索的关键词，因而创建索引时，这种词会被去掉而减少索引的大小。 英语中挺词(Stop word)如：“the”,“a”，“this”等。 对于每一种语言的分词组件(Tokenizer)，都有一个停词(stop word)集合。 经过分词(Tokenizer)后得到的结果称为词元(Token)。 在我们的例子中，便得到以下词元(Token)： “Students”，“allowed”，“go”，“their”，“friends”，“allowed”，“drink”，“beer”，“My”，“friend”，“Jerry”，“went”，“school”，“see”，“his”，“students”，“found”，“them”，“drunk”，“allowed”。 第三步：将得到的词元(Token)传给语言处理组件(Linguistic Processor)。语言处理组件(linguistic processor)主要是对得到的词元(Token)做一些同语言相关的处理。 对于英语，语言处理组件(Linguistic Processor)一般做以下几点： 变为小写(Lowercase)。 将单词缩减为词根形式，如“cars”到“car”等。这种操作称为：stemming。 将单词转变为词根形式，如“drove”到“drive”等。这种操作称为：lemmatization。 Stemming 和 lemmatization的异同： 相同之处：Stemming和lemmatization都要使词汇成为词根形式。两者的方式不同：Stemming采用的是“缩减”的方式：“cars”到“car”，“driving”到“drive”。Lemmatization采用的是“转变”的方式：“drove”到“drove”，“driving”到“drive”。两者的算法不同：Stemming主要是采取某种固定的算法来做这种缩减，如去除“s”，去除“ing”加“e”，将“ational”变为“ate”，将“tional”变为“tion”。Lemmatization主要是采用保存某种字典的方式做这种转变。比如字典中有“driving”到“drive”，“drove”到“drive”，“am, is, are”到“be”的映射，做转变时，只要查字典就可以了。Stemming和lemmatization不是互斥关系，是有交集的，有的词利用这两种方式都能达到相同的转换。 语言处理组件(linguistic processor)的结果称为词(Term)。 在我们的例子中，经过语言处理，得到的词(Term)如下： “student”，“allow”，“go”，“their”，“friend”，“allow”，“drink”，“beer”，“my”，“friend”，“jerry”，“go”，“school”，“see”，“his”，“student”，“find”，“them”，“drink”，“allow”。 也正是因为有语言处理的步骤，才能使搜索drove，而drive也能被搜索出来。 第四步：将得到的词(Term)传给索引组件(Indexer)。索引组件(Indexer)主要做以下几件事情： 利用得到的词(Term)创建一个字典。 在我们的例子中字典如下： 对字典按字母顺序进行排序。 合并相同的词(Term)成为文档倒排(Posting List)链表。 在此表中，有几个定义： Document Frequency 即文档频次，表示总共有多少文件包含此词(Term)。Frequency 即词频率，表示此文件中包含了几个此词(Term)。所以对词(Term) “allow”来讲，总共有两篇文档包含此词(Term)，从而词(Term)后面的文档链表总共有两项，第一项表示包含“allow”的第一篇文档，即1号文档，此文档中，“allow”出现了2次，第二项表示包含“allow”的第二个文档，是2号文档，此文档中，“allow”出现了1次。 到此为止，索引已经创建好了，我们可以通过它很快的找到我们想要的文档。 而且在此过程中，我们惊喜地发现，搜索“drive”，“driving”，“drove”，“driven”也能够被搜到。因为在我们的索引中，“driving”，“drove”，“driven”都会经过语言处理而变成“drive”，在搜索时，如果您输入“driving”，输入的查询语句同样经过我们这里的一到三步，从而变为查询“drive”，从而可以搜索到想要的文档。 三、如何对索引进行搜索？到这里似乎我们可以宣布“我们找到想要的文档了”。 然而事情并没有结束，找到了仅仅是全文检索的一个方面。不是吗？如果仅仅只有一个或十个文档包含我们查询的字符串，我们的确找到了。然而如果结果有一千个，甚至成千上万个呢？那个又是您最想要的文件呢？ 打开Google吧，比如说您想在微软找份工作，于是您输入“Microsoft job”，您却发现总共有22600000个结果返回。好大的数字呀，突然发现找不到是一个问题，找到的太多也是一个问题。在如此多的结果中，如何将最相关的放在最前面呢？ 当然Google做的很不错，您一下就找到了jobs at Microsoft。想象一下，如果前几个全部是“Microsoft does a good job at software industry…”将是多么可怕的事情呀。 如何像Google一样，在成千上万的搜索结果中，找到和查询语句最相关的呢？ 如何判断搜索出的文档和查询语句的相关性呢？ 这要回到我们第三个问题：如何对索引进行搜索？ 搜索主要分为以下几步： 第一步：用户输入查询语句。查询语句同我们普通的语言一样，也是有一定语法的。 不同的查询语句有不同的语法，如SQL语句就有一定的语法。 查询语句的语法根据全文检索系统的实现而不同。最基本的有比如：AND, OR, NOT等。 举个例子，用户输入语句：lucene AND learned NOT hadoop。 说明用户想找一个包含lucene和learned然而不包括hadoop的文档。 第二步：对查询语句进行词法分析，语法分析，及语言处理。由于查询语句有语法，因而也要进行语法分析，语法分析及语言处理。 词法分析主要用来识别单词和关键字。 如上述例子中，经过词法分析，得到单词有lucene，learned，hadoop, 关键字有AND, NOT。 如果在词法分析中发现不合法的关键字，则会出现错误。如lucene AMD learned，其中由于AND拼错，导致AMD作为一个普通的单词参与查询。 语法分析主要是根据查询语句的语法规则来形成一棵语法树。 如果发现查询语句不满足语法规则，则会报错。如lucene NOT AND learned，则会出错。 如上述例子，lucene AND learned NOT hadoop形成的语法树如下： 语言处理同索引过程中的语言处理几乎相同。 如learned变成learn等。 经过第二步，我们得到一棵经过语言处理的语法树 第三步：搜索索引，得到符合语法树的文档。此步骤有分几小步： 首先，在反向索引表中，分别找出包含lucene，learn，hadoop的文档链表。其次，对包含lucene，learn的链表进行合并操作，得到既包含lucene又包含learn的文档链表。然后，将此链表与hadoop的文档链表进行差操作，去除包含hadoop的文档，从而得到既包含lucene又包含learn而且不包含hadoop的文档链表。此文档链表就是我们要找的文档。 第四步：根据得到的文档和查询语句的相关性，对结果进行排序。虽然在上一步，我们得到了想要的文档，然而对于查询结果应该按照与查询语句的相关性进行排序，越相关者越靠前。 如何计算文档和查询语句的相关性呢？ 不如我们把查询语句看作一片短小的文档，对文档与文档之间的相关性(relevance)进行打分(scoring)，分数高的相关性好，就应该排在前面。 那么又怎么对文档之间的关系进行打分呢？ 这可不是一件容易的事情，首先我们看一看判断人之间的关系吧。 首先看一个人，往往有很多要素，如性格，信仰，爱好，衣着，高矮，胖瘦等等。 其次对于人与人之间的关系，不同的要素重要性不同，性格，信仰，爱好可能重要些，衣着，高矮，胖瘦可能就不那么重要了，所以具有相同或相似性格，信仰，爱好的人比较容易成为好的朋友，然而衣着，高矮，胖瘦不同的人，也可以成为好的朋友。 因而判断人与人之间的关系，首先要找出哪些要素对人与人之间的关系最重要，比如性格，信仰，爱好。其次要判断两个人的这些要素之间的关系，比如一个人性格开朗，另一个人性格外向，一个人信仰佛教，另一个信仰上帝，一个人爱好打篮球，另一个爱好踢足球。我们发现，两个人在性格方面都很积极，信仰方面都很善良，爱好方面都爱运动，因而两个人关系应该会很好。 我们再来看看公司之间的关系吧。 首先看一个公司，有很多人组成，如总经理，经理，首席技术官，普通员工，保安，门卫等。 其次对于公司与公司之间的关系，不同的人重要性不同，总经理，经理，首席技术官可能更重要一些，普通员工，保安，门卫可能较不重要一点。所以如果两个公司总经理，经理，首席技术官之间关系比较好，两个公司容易有比较好的关系。然而一位普通员工就算与另一家公司的一位普通员工有血海深仇，怕也难影响两个公司之间的关系。 因而判断公司与公司之间的关系，首先要找出哪些人对公司与公司之间的关系最重要，比如总经理，经理，首席技术官。其次要判断这些人之间的关系，不如两家公司的总经理曾经是同学，经理是老乡，首席技术官曾是创业伙伴。我们发现，两家公司无论总经理，经理，首席技术官，关系都很好，因而两家公司关系应该会很好。 分析了两种关系，下面看一下如何判断文档之间的关系了。 首先，一个文档有很多词(Term)组成，如search, lucene, full-text, this, a, what等。 其次对于文档之间的关系，不同的Term重要性不同，比如对于本篇文档，search, Lucene, full-text就相对重要一些，this, a , what可能相对不重要一些。所以如果两篇文档都包含search, Lucene，fulltext，这两篇文档的相关性好一些，然而就算一篇文档包含this, a, what，另一篇文档不包含this, a, what，也不能影响两篇文档的相关性。 因而判断文档之间的关系，首先找出哪些词(Term)对文档之间的关系最重要，如search, Lucene, fulltext。然后判断这些词(Term)之间的关系。 找出词(Term)对文档的重要性的过程称为计算词的权重(Term weight)的过程。 计算词的权重(term weight)有两个参数，第一个是词(Term)，第二个是文档(Document)。 词的权重(Term weight)表示此词(Term)在此文档中的重要程度，越重要的词(Term)有越大的权重(Term weight)，因而在计算文档之间的相关性中将发挥更大的作用。 判断词(Term)之间的关系从而得到文档相关性的过程应用一种叫做向量空间模型的算法(Vector Space Model)。 下面仔细分析一下这两个过程： 计算权重(Term weight)的过程。影响一个词(Term)在一篇文档中的重要性主要有两个因素： Term Frequency (tf)：即此Term在此文档中出现了多少次。tf 越大说明越重要。Document Frequency (df)：即有多少文档包含次Term。df 越大说明越不重要。容易理解吗？词(Term)在文档中出现的次数越多，说明此词(Term)对该文档越重要，如“搜索”这个词，在本文档中出现的次数很多，说明本文档主要就是讲这方面的事的。然而在一篇英语文档中，this出现的次数更多，就说明越重要吗？不是的，这是由第二个因素进行调整，第二个因素说明，有越多的文档包含此词(Term), 说明此词(Term)太普通，不足以区分这些文档，因而重要性越低。 这也如我们程序员所学的技术，对于程序员本身来说，这项技术掌握越深越好（掌握越深说明花时间看的越多，tf越大），找工作时越有竞争力。然而对于所有程序员来说，这项技术懂得的人越少越好（懂得的人少df小），找工作越有竞争力。人的价值在于不可替代性就是这个道理。 道理明白了，我们来看看公式： 这仅仅只term weight计算公式的简单典型实现。实现全文检索系统的人会有自己的实现，Lucene就与此稍有不同。 判断Term之间的关系从而得到文档相关性的过程，也即向量空间模型的算法(VSM)。我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。 于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。 Document = {term1, term2, …… ,term N} Document Vector = {weight1, weight2, …… ,weight N} 同样我们把查询语句看作一个简单的文档，也用向量来表示。 Query = {term1, term 2, …… , term N} Query Vector = {weight1, weight2, …… , weight N} 我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。 如图： 我们认为两个向量之间的夹角越小，相关性越大。 所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。 有人可能会问，查询语句一般是很短的，包含的词(Term)是很少的，因而查询向量的维数很小，而文档很长，包含词(Term)很多，文档向量维数很大。你的图中两者维数怎么都是N呢？ 在这里，既然要放到相同的向量空间，自然维数是相同的，不同时，取二者的并集，如果不含某个词(Term)时，则权重(Term Weight)为0。 相关性打分公式如下： 举个例子，查询语句有11个Term，共有三篇文档搜索出来。其中各自的权重(Term weight)，如下表格。 于是计算，三篇文档同查询语句的相关性打分分别为： 于是文档二相关性最高，先返回，其次是文档一，最后是文档三。 到此为止，我们可以找到我们最想要的文档了。 说了这么多，其实还没有进入到Lucene，而仅仅是信息检索技术(Information retrieval)中的基本理论，然而当我们看过Lucene后我们会发现，Lucene是对这种基本理论的一种基本的的实践。所以在以后分析Lucene的文章中，会常常看到以上理论在Lucene中的应用。 在进入Lucene之前，对上述索引创建和搜索过程所一个总结，如图： 索引过程： 1) 有一系列被索引文件 2) 被索引文件经过语法分析和语言处理形成一系列词(Term)。 3) 经过索引创建形成词典和反向索引表。 4) 通过索引存储将索引写入硬盘。 搜索过程： a) 用户输入查询语句。 b) 对查询语句经过语法分析和语言分析得到一系列词(Term)。 c) 通过语法分析得到一个查询树。 d) 通过索引存储将索引读入到内存。 e) 利用查询树搜索索引，从而得到每个词(Term)的文档链表，对文档链表进行交，差，并得到结果文档。 f) 将搜索到的结果文档对查询的相关性进行排序。 g) 返回查询结果给用户。 下面我们可以进入Lucene的世界了。","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/categories/搜索引擎/"}],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/tags/搜索引擎/"},{"name":"apache lucene","slug":"apache-lucene","permalink":"http://blog.pinbot.me/tags/apache-lucene/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://blog.pinbot.me/categories/搜索引擎/"}]},{"title":"基于机器学习的高价值用户简历自动分类","slug":"简历自动分类","date":"2016-08-31T10:00:04.000Z","updated":"2017-06-30T02:39:32.000Z","comments":true,"path":"2016/08/31/简历自动分类/","link":"","permalink":"http://blog.pinbot.me/2016/08/31/简历自动分类/","excerpt":"一、业务介绍对于任何互联网招聘企业来说，求职者的简历库都是核心资产。因为这是他们变现的基础。只有拥有足够多的简历，让企业可以在该网站上获取需要的人才，才能持续从企业客户获得订单。以猎聘网为例。猎聘网的市场定位为满足企业中高端人才的需求。这使得猎聘向企业客户提供的简历是具有一定质量的中高层企业管理者、专业技术人才等。出售这样的简历资源，也是猎聘变现的主要来源。那么在此类简历资源定价方面，企业需要付较高的费用来购买此类简历;而对于其余的简历，企业仅需要付出非常低廉的成本即可获得。因此，猎聘内部根据简历的信息，将简历进行等级分类。目标清晰后，那么问题随之而来。猎聘网获取简历的最主要方式是在线注册。网站为吸引用户注册，在注册时一般只是填写一些简单的名片信息。待注册完成后选择进行简历完善，填写复杂麻烦一些个人信息，如教育经历和工作经历等。最终形成一份完善的简历。但猎聘每天有数万新用户注册。有些用户如果找工作的意愿并不强烈等原因，只是填写了少量的职业信息，即名片信息，而并未完成整个简历的填写。而一般情况下的手工分类在分级的过程中，使用了简历中多方面的信息，如果简历不完整，将无法准确对简历进行评价。为获得完整的简历，猎聘职业顾问团队(GCDC)需要电话联系该部分用户引导完善简历。当然这种方式也是成本最为昂贵的。历史数据表明，未填写简历的用户中有相当数量的高价值的用户，而猎聘职业顾问需要能够优先拨打这批高级用户并提升其转化率。这就要求将评分较高、更有可能是高级的用户推荐给GCDC进行优先电话拨打，提高了高级用户简历转化率。","text":"一、业务介绍对于任何互联网招聘企业来说，求职者的简历库都是核心资产。因为这是他们变现的基础。只有拥有足够多的简历，让企业可以在该网站上获取需要的人才，才能持续从企业客户获得订单。以猎聘网为例。猎聘网的市场定位为满足企业中高端人才的需求。这使得猎聘向企业客户提供的简历是具有一定质量的中高层企业管理者、专业技术人才等。出售这样的简历资源，也是猎聘变现的主要来源。那么在此类简历资源定价方面，企业需要付较高的费用来购买此类简历;而对于其余的简历，企业仅需要付出非常低廉的成本即可获得。因此，猎聘内部根据简历的信息，将简历进行等级分类。目标清晰后，那么问题随之而来。猎聘网获取简历的最主要方式是在线注册。网站为吸引用户注册，在注册时一般只是填写一些简单的名片信息。待注册完成后选择进行简历完善，填写复杂麻烦一些个人信息，如教育经历和工作经历等。最终形成一份完善的简历。但猎聘每天有数万新用户注册。有些用户如果找工作的意愿并不强烈等原因，只是填写了少量的职业信息，即名片信息，而并未完成整个简历的填写。而一般情况下的手工分类在分级的过程中，使用了简历中多方面的信息，如果简历不完整，将无法准确对简历进行评价。为获得完整的简历，猎聘职业顾问团队(GCDC)需要电话联系该部分用户引导完善简历。当然这种方式也是成本最为昂贵的。历史数据表明，未填写简历的用户中有相当数量的高价值的用户，而猎聘职业顾问需要能够优先拨打这批高级用户并提升其转化率。这就要求将评分较高、更有可能是高级的用户推荐给GCDC进行优先电话拨打，提高了高级用户简历转化率。二、数据处理与模型部分1.数据采集训练数据主要来自三张表——user_c、user_register和res_user。表user_c主要存储用户的名片信息;user_register存储用户的注册信息;res_user存储用户的简历信息。从这三个表中我们抽取用户的名片信息及评级信息。(字段及对应含义见表-1)2、数据预处理如数据去重；剔除无效数据：年龄未满18岁或超过退休年龄、工龄小但职位高等明显不合常识的数据。 3.选取特征属性确定了如下8个特征：性别(男、女)、出生年份、开始工作年份、最高学历、职能、当前公司、当前工作的城市。为用户等级的评判依据。 4.特征属性的预处理： 1)非度量属性的二值化处理：如性别属性。男女设定为二值属性(男→1;女→0); 2）可度量离散特征的有序化处理。如城市分一、二、三线城市。学历分博士、硕士、本科、专科及以下等。 5.数据建模 k近邻(k-Nearest Neighbors，简称kNN)是一种常用的监督式学习方法。其基本思想是 ：相似的对象具有相同或者相近的类别(物以类聚、人以群分)。如果一个对象在特征空间中的k个距离最近、最相似的训练样本大多数属于某个类别，则该对象可以被判定属于该类别。 在建立模型时，我们已经建立好的分类作为训练实例。对于每一个新的实例，计算其与每个训练实例的欧式距离，选取距离最近的k个实例(k值的选取依赖于交叉验证)，采用“多数表决”的策略，计算新样例属于高级用户的可能性(打分在1~100分)，并根据分数高低对用户进行排序，推荐给用户获取部门作为召回策略的重要参考。 从如上的计算过程我们知道，对于每一个新的样例，kNN需要计算其与每个训练样本的距离。如果训练样例的数量较大的话，该计算步骤比较耗时，且耗费内存。为提高打分效率，我们引入了基于规则的“低端职位过滤”机制。先构建了一个“常用低端职位词典”(如“摆地摊”、“司机”、“服务员”等)。如果一个新的样例的职位名称存在于该职位词典中，则被直接判为0分，而无需进入kNN算法的相关计算环节，这样就显著提高了打分效率。同时，也可以提高准确率。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/tags/机器学习/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.pinbot.me/categories/机器学习/"}]},{"title":"从__str__说开去","slug":"从__str__说开去","date":"2016-08-31T08:00:00.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/08/31/从__str__说开去/","link":"","permalink":"http://blog.pinbot.me/2016/08/31/从__str__说开去/","excerpt":"__str__ 和 __repr__介绍object.__str__是python中一个常见的特殊方法, 会被内置函数被 str 和 print 调用。常常与它一起出现的还有object.__repr__, 类似地, 它会被内置函数 repr 调用。","text":"__str__ 和 __repr__介绍object.__str__是python中一个常见的特殊方法, 会被内置函数被 str 和 print 调用。常常与它一起出现的还有object.__repr__, 类似地, 它会被内置函数 repr 调用。 区别那么 str 和 repr 同样作为”informal string representation of instances”, 有何区别? 用一句话概括就是: repr is for developers, str is for customers. 这点在IDE中调试时得以展现: 123456789101112131415class Student(object): def __init__(self, name, grade): self.name = name self.grade = grade def __str__(self): return '&#123;0&#125;(&#123;1&#125;)'.format(self.name, self.grade) def __repr__(self): return '&lt;Student&gt;'if __name__ == '__main__': student = Student(name='Roy', grade=11) 在debug模式下, pycharm将 student 展示为: 简洁明了。 __unicode__出场率同样高的还有object.__unicode__, 和object.__str__作用类似, 但不同的是, object.__unicode__ 返回的是一个unicode object, 而 object.__str__ 返回的是string object。这会引起一些问题, 特别是当你在使用python2中的unicode_literals时。 unicode_literalsUnicodeEncodeError我们对上面的代码做一些修改: 1234567891011121314151617# -*- coding: utf-8 -*-from __future__ import unicode_literalsclass Student(object): def __init__(self, name, grade): self.name = name self.grade = grade def __str__(self): return '&#123;0&#125;(&#123;1&#125;)'.format(self.name, self.grade) def __repr__(self): return '&lt;Student&gt;' def __unicode__(self): return '&lt;Student&gt;' 改动在于import了unicode_literals, 并为 Student 添加了一个 __unicode__ 方法, 看起来好像没有什么问题。 但当你实例化一个 Student , 并将 name 指定为中文时: 12student = Student(name='罗伊', grade=11)print student 报错了, UnicodeEncodeError 。 你或许精通python2的中文编码问题, 但也许并没有注意到这个问题。在使用django时遇到过 [Bad Unicode data] 这个东西, 问题是一样的, django在项目中也使用了 unicode_literals 。 问题在哪问题在于 object.__str__ 返回的必须为string object, 而使用 unicode_literals 之后返回的为unicode object, python2解释器会尝试用默认的编码(ascii)对其进行encode, 所以报错。 解决问题unicode_literals 在python2中是个利器, 不能不用。接下来我们用两种方法来解决上面这个问题。 patch这是一种经典的方法: 123import sysreload(sys)sys.setdefaultencoding('utf8') 重载 sys 并将 defaultencoding 从 ascii 修改为 utf-8 , 对含中文的unicode object使用utf-8进行encode是可行的。 装饰器123456789def force_encoded_string_output(func): if sys.version_info.major &lt; 3: def _func(*args, **kwargs): return func(*args, **kwargs).encode(&apos;utf-8&apos;) return _func else: return func 使用 force_encoded_string_output 装饰 object.__str__ 即可, 解决的思路和上面类似。 最佳实践当你在python2中同时使用中文, unicode_literals, __str__, __unicode__ 可以考虑下面的方式: 123456789101112from __future__ import unicode_literalsclass Best(object): def __str__(self): return unicode(self).encode('utf-8') def __unicode__(self): s = 'Put your data here.' assert isinstance(s, unicode) return s","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"http://blog.pinbot.me/tags/python/"},{"name":"str","slug":"str","permalink":"http://blog.pinbot.me/tags/str/"},{"name":"unicode_literals","slug":"unicode-literals","permalink":"http://blog.pinbot.me/tags/unicode-literals/"},{"name":"Kxrr","slug":"Kxrr","permalink":"http://blog.pinbot.me/tags/Kxrr/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"前端入门之我见","slug":"前端入门之我见","date":"2016-08-30T14:48:37.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2016/08/30/前端入门之我见/","link":"","permalink":"http://blog.pinbot.me/2016/08/30/前端入门之我见/","excerpt":"作者：Adam at 2016-08-30 22:48:37 这两周陆陆续续进行了两个独立的前端项目，一个是前后端分离的Angular项目，一个是ES6+Nodejs的全栈项目，前者先做，后者差不多刚完成，最明显的感觉就是写了Angular不想写JQuery，写了ES6不想写ES5。","text":"作者：Adam at 2016-08-30 22:48:37 这两周陆陆续续进行了两个独立的前端项目，一个是前后端分离的Angular项目，一个是ES6+Nodejs的全栈项目，前者先做，后者差不多刚完成，最明显的感觉就是写了Angular不想写JQuery，写了ES6不想写ES5。 我就在想，为什么会有这么强烈感觉？是什么导致的？ 首先，我们来看看前端主要做什么？ 一是页面：HTML+CSS样式布局；二是Javascript脚本：根据页面事件响应、控制页面逻辑。 就这么简单。 然后，CSS样式多了，Javascript函数多了，我们就希望代码好维护，方便调用，少写代码，于是就出现了各种CSS／Javascript框架。 随之而来的问题也出现了，逻辑变复杂，这就需要我们把注释要写清楚，甚至完全文档化。 Javascript写法太自由了，写出来效果往往容易有bug，加上不同浏览器的、不同终端的折腾，好吧，我们把单元测试、界面测试补上，这下总可以了吧。 但是，问题并没有结束。 我们开始思考Javascript是否真的适合写前端页面，为什么Javascript写大型项目这么痛苦？为什么要不断重复写写登录注册？为什么要离不开for循环？为什么不能尽可能高的重用代码？ 我们很早就在说OOP、MVC，也有现在的MVVM、SAM，也出了不少经典框架，但Javascript始终还是Javascript，没有class，没有isArray，只有说不清道不明的 prototype 和 __proto__ 。既然我们知道什么样的语法简洁高效，为什么不让Javascript也能这么做呢？所以，Type Script出来了，Webpack/Babel出来了，ES6出来了。 所以，如果现在你想学前端，直接写ES6吧，有了webpack和babel，以前能做的现在都能做，现在能做的，以前不一定能做。 如今前端也再也不是写写页面、做做脚本，不再是网页三剑客的时代。你还需要精通Sublime/Vim这些编辑器，会架构前端开发环境、熟悉Nodejs/NPM，掌握Phantomjs/Jasmine等测试手段，会用JsDoc写文档。当然，最重要的还是要学好ES6。 最后，我总结一下我学习ES6后，发现的一些好处，希望和大家多交流、沟通～ OOP的原配Class，写起来的酸爽倍儿棒只有自己知道 模块化导入，让我可以前后端共享代码 函数参数的扩展是我的最爱，直接让我轻松20% 代码密集度明显好于过去，这是密集恐惧症的福音 Webpack无疑是前端开发自动化的必备神器，你值得拥有～ 你不再需要模版语言，ES6就是最好的模版语言 一切皆Js，HTML是，CSS也是","categories":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.pinbot.me/tags/ES6/"},{"name":"Webpack","slug":"Webpack","permalink":"http://blog.pinbot.me/tags/Webpack/"},{"name":"经验总结","slug":"经验总结","permalink":"http://blog.pinbot.me/tags/经验总结/"}],"keywords":[{"name":"blog","slug":"blog","permalink":"http://blog.pinbot.me/categories/blog/"}]},{"title":"聘宝招聘Python实习生","slug":"聘宝招聘Python实习生","date":"2015-09-29T02:51:04.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2015/09/29/聘宝招聘Python实习生/","link":"","permalink":"http://blog.pinbot.me/2015/09/29/聘宝招聘Python实习生/","excerpt":"公司情况现在主要做的产品是 http://www.pinbot.me/ ,已经拿到A轮 坐标：四川成都高新区府城大道399天府新谷9号楼二单元1505成都浩泊云动科技有限公司","text":"公司情况现在主要做的产品是 http://www.pinbot.me/ ,已经拿到A轮 坐标：四川成都高新区府城大道399天府新谷9号楼二单元1505成都浩泊云动科技有限公司 我们的研发团队喜欢什么 K.I.S.S 敏捷开发，快速原型和必要的单元测试 使用*nix 有创造性思维，喜欢创造的人 函数式编程和各种高并发的编程语言（Scheme、Clojure、Golang、Elixir） 为什么招实习生成都Python的圈子本来就小，看来看去就那么些人，大牛实在难搞定，所以希望找到一些有兴趣往Python方向发展的人，一起成长。 在这里你能做什么 了解到Python Web开发常用的工具和流程规范 做一些真正有人使用的产品 一起建设团队，给团队带来更高效的流程和工具 做自己想做的产品，如果你有好的创意都可以跟我们产品经理沟通，将创意实现到产品中 我们能提供什么 Mac和双显是我们日常的开发工具 每周免费的零食 技术分享 每月一次的Hack Day 妹子都是女神级别的，养眼提高工作效率 工作描述 负责http://www.pinbot.me/ 网站新功能的开发和日常维护 负责内部CRM管理系统的开发维护 技能要求# coding: utf-8 &quot;&quot;&quot; 对以下技术熟悉或者有强烈兴趣 &quot;&quot;&quot; # 基本技能 BASIC_SKILL = [ &apos;*nix&apos;, &apos;Vim or Emacs&apos;, &apos;Git&apos;, &apos;Unix哲学&apos;, &apos;基础算法和数据结构&apos;, &apos;计算机组成原理&apos;, &apos;网路协议&apos;, ] # 后端技能 BACKEND_SKILL = [ &apos;Python&apos;, &apos;Django&apos;, &apos;Python常用库&apos;, &apos;了解Http 协议&apos;, &apos;NodeJS&apos;, ] # 前端技能 FRONTEND_SKILL = [ &apos;HTML&apos;, &apos;CSS&apos;, &apos;JS&apos;, &apos;AngularJS&apos;, &apos;React&apos;, &apos;JQuery&apos;, ] # 运维 MAINTAIN_SKILL = [ &apos;bash&apos;, &apos;Docker&apos;, &apos;Fabric&apos;, &apos;Ansible&apos;, &apos;SaltStack&apos;, ] # 数据库 DATABASE = [ &apos;MySQL&apos;, &apos;Mongo&apos;, ] ALL_SKILL = set(i.lower() for i in (BASIC_SKILL + BACKEND_SKILL + FRONTEND_SKILL + MAINTAIN_SKILL + DATABASE)) def i_want_you(your_skill): &quot;&quot;&quot; 符合以上任意关键词就可以了 后续可以让我们的算法工程师来做一个测试程序 &gt;&gt;&gt; i_want_you(&apos;Python HTML Docker MySQL&apos;) I want you &gt;&gt;&gt; i_want_you(&apos;java&apos;) hehe &quot;&quot;&quot; your_skill = [i.lower() for i in your_skill.split()] print &apos;I want you&apos; if set(your_skill).intersection(ALL_SKILL) else &apos;hehe&apos; if __name__ == &apos;__main__&apos;: your_skill = raw_input(&apos;Input your skill: &apos;) i_want_you(your_skill) 补充 有github或者bitbucket等开源社区账号优先 有自己博客的优先 联系我简历请投至：dengyu@hopperclouds.com","categories":[{"name":"招聘","slug":"招聘","permalink":"http://blog.pinbot.me/categories/招聘/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.pinbot.me/tags/Python/"},{"name":"招聘","slug":"招聘","permalink":"http://blog.pinbot.me/tags/招聘/"}],"keywords":[{"name":"招聘","slug":"招聘","permalink":"http://blog.pinbot.me/categories/招聘/"}]},{"title":"聘宝研发团队必备技能","slug":"聘宝研发团队必备技能","date":"2015-09-24T08:09:24.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2015/09/24/聘宝研发团队必备技能/","link":"","permalink":"http://blog.pinbot.me/2015/09/24/聘宝研发团队必备技能/","excerpt":"简介聘宝(www.pinbot.me) 立志做新一代的智能猎头，让企业招聘变得更简单，我们崇尚敏捷，崇尚开源，崇尚K.I.S.S","text":"简介聘宝(www.pinbot.me) 立志做新一代的智能猎头，让企业招聘变得更简单，我们崇尚敏捷，崇尚开源，崇尚K.I.S.S Web研发团队必备技能后端技术Python (我们主要使用的后端语言) Django (我们正在使用的框架) Python常用库(celery, requests, json, bs4...) HTTP协议 (这个必须会) Unit Test (知道什么地方要有测试) 前端技术HTML Javascript (Node是一个趋势，所以必须会) CSS Angular.js React.js 数据库Mysql Mongo Redis 明确知道不同类型数据库的应用场景，了解数据的设计范式和调优 消息队列RabbitMQ 运维相关Docker (很多东西正在尝试用docker去完成，确实很方便) Ansible (服务器配置管理) 开发环境*nix (必须会，不喜欢windows) VIM (运维要用，必须会) Emacs (看个人爱好) Git及Git开发流程 (必须会) 文档markdown rst 职业素养1. 独立思考，好学 2. 沟通能力强 3. 了解Python哲学 4. 读过程序员修炼之道和代码整洁之道等必读书籍 5. 对程序设计有自己的追求 6. 了解软件工程的思想 推荐技术Golang、Clojure(并发是以后的趋势) 函数式编程语言（可能也是以后的趋势） elixir (聘宝会考虑用它做东西) 总结这个是我们团队的后端必备技能，欢迎大家在评论中补充","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.pinbot.me/tags/Python/"},{"name":"运维","slug":"运维","permalink":"http://blog.pinbot.me/tags/运维/"},{"name":"javascript","slug":"javascript","permalink":"http://blog.pinbot.me/tags/javascript/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://blog.pinbot.me/categories/技术/"}]},{"title":"使用Hexo作为博客","slug":"使用Hexo作为博客","date":"2015-09-24T07:29:25.000Z","updated":"2017-06-29T09:27:58.000Z","comments":true,"path":"2015/09/24/使用Hexo作为博客/","link":"","permalink":"http://blog.pinbot.me/2015/09/24/使用Hexo作为博客/","excerpt":"安装和部署请参考下面或自行google 官网 http://wsgzao.github.io/post/hexo-guide/","text":"安装和部署请参考下面或自行google 官网 http://wsgzao.github.io/post/hexo-guide/ 基本使用准备# 安装node 和 cnpm brew install node npm install cnpm -g # 将项目clone 下来 git clone git@github.com:HopperClouds/hopperclouds.github.io.git # 安装hexo 依赖的node库 cnpm install # 遇到问题 # { [Error: Cannot find module &apos;./build/Release/DTraceProviderBindings&apos; ] code: &apos;MODULE_NOT_FOUND&apos; } # { [Error: Cannot find module &apos;./build/default/DTraceProviderBindings&apos; ] code: &apos;MODULE_NOT_FOUND&apos; } # { [Error: Cannot find module &apos;./build/Debug/DTraceProviderBindings&apos; ] code: &apos;MODULE_NOT_FOUND&apos; } # 使用 cnpm install --no-optional 开始写文章hexo new &quot;your title&quot; # 在source/_posts/your\\ title.md 文件 # 在里面使用markdown编辑博客 # 生成文件格式 title: 使用Hexo作为博客 date: 2015-09-24 15:29:25 # 类别 categories: - 其他 # 标签 tags: - 其他 - 开始 --- markdown格式正文内容 生成文章hexo generate # 使用--watch 参数检测文件更新 hexo generate --watch 预览hexo server 发布hexo deploy 将markdown源码push到source分支git push origin master:source 总结静态博客才是写博客的正确姿势 初次使用觉得不像octopress 那样完善，至于为什么不用octopress, 是因为我们是使用Python和JS的团队，Node对我们来说更友好一些。 对于使用Emacs的用户还没有org mode支持，可以hack一下了。","categories":[{"name":"其他","slug":"其他","permalink":"http://blog.pinbot.me/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://blog.pinbot.me/tags/其他/"},{"name":"开始","slug":"开始","permalink":"http://blog.pinbot.me/tags/开始/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://blog.pinbot.me/categories/其他/"}]}]}